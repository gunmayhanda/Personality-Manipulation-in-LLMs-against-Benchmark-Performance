{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables from .env file loaded.\n",
      "Successfully logged into Hugging Face.\n",
      "Hugging Face cache directory set to: /cs/student/projects3/aisd/2024/ghanda/cache\n",
      "Azure OpenAI client initialized for PAE scoring.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json # Not directly used in parts 1 & 2, but for context of full script\n",
    "import re   # Not directly used in parts 1 & 2, but for context of full script\n",
    "import random # Not directly used in parts 1 & 2, but for context of full script\n",
    "import numpy as np # Not directly used in parts 1 & 2, but for context of full script\n",
    "\n",
    "from datasets import Dataset # No longer load_dataset here, as Personality data is CSV\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline, # Not directly used in parts 1 & 2, but for context of full script\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel # Import PeftModel for loading adapters\n",
    "from trl import SFTTrainer, SFTConfig # <<< ADDED SFTConfig here\n",
    "\n",
    "# For logging to Hugging Face Hub during training\n",
    "from huggingface_hub import login\n",
    "from datasets import Dataset, load_dataset # load_dataset explicitly for HF datasets\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==============================================================================\n",
    "# INITIAL CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Load ALL Configurations from .env file ---\n",
    "load_dotenv()\n",
    "print(\"Environment variables from .env file loaded.\")\n",
    "\n",
    "# --- Hugging Face Login ---\n",
    "try:\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"Successfully logged into Hugging Face.\")\n",
    "    else:\n",
    "        print(\"Hugging Face token not found. Skipping login to Hub.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not log into Hugging Face: {e}\")\n",
    "\n",
    "# Set the Hugging Face cache directory early for both models and datasets\n",
    "# This is where your base Gemma model and dataset will be downloaded.\n",
    "HF_CACHE_DIR = \"/cs/student/projects3/aisd/2024/ghanda/cache\"\n",
    "os.environ[\"HF_HOME\"] = HF_CACHE_DIR\n",
    "os.makedirs(HF_CACHE_DIR, exist_ok=True) # Ensure the directory exists\n",
    "print(f\"Hugging Face cache directory set to: {HF_CACHE_DIR}\")\n",
    "\n",
    "# --- Core Model Configuration ---\n",
    "MODEL_NAME = \"google/gemma-2-2b\"\n",
    "FINE_TUNED_MODEL_BASE_NAME = \"gemma-2b-personality-peft\"\n",
    "\n",
    "# --- QLoRA parameters ---\n",
    "lora_r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# --- bitsandbytes parameters ---\n",
    "use_4bit = True\n",
    "# Using bfloat16 as indicated by your GPU support, this is generally preferred.\n",
    "bnb_4bit_compute_dtype = \"bfloat16\" \n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False \n",
    "\n",
    "# --- Updated SFTConfig parameters (must match bnb_4bit_compute_dtype) ---\n",
    "# CRITICAL: These must match your bnb_4bit_compute_dtype\n",
    "fp16 = False  # False if using bfloat16\n",
    "bf16 = True   # True if using bfloat16\n",
    "\n",
    "# --- SFTConfig parameters (consolidates training and SFT-specific args) ---\n",
    "OUTPUT_DIR_BASE = \"peft_output_models\" # Base directory for saving fine-tuned models\n",
    "num_train_epochs = 2\n",
    "# fp16 and bf16 should align with bnb_4bit_compute_dtype\n",
    "fp16 = False # False if bnb_4bit_compute_dtype is bfloat16\n",
    "bf16 = True  # True if bnb_4bit_compute_dtype is bfloat16\n",
    "per_device_train_batch_size = 1 # CRUCIAL for memory; keep low\n",
    "gradient_accumulation_steps = 8 # Effective batch size 1 * 8 = 8 to compensate for low batch size\n",
    "gradient_checkpointing = True # Highly recommended for memory saving with large models\n",
    "max_grad_norm = 0.3\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.001\n",
    "optim = \"paged_adamw_8bit\" # Use 8-bit optimizer for memory saving\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_steps = -1 # Not using max_steps, relying on num_train_epochs\n",
    "warmup_ratio = 0.03\n",
    "group_by_length = True\n",
    "save_steps = 0 # Don't save intermediate checkpoints unless needed\n",
    "logging_steps = 5 # Log more frequently for small datasets\n",
    "\n",
    "# SFT-specific args now part of SFTConfig\n",
    "max_seq_length = 512 # Max length for training sequences. Adjust based on your data.\n",
    "packing = False      # Whether to group multiple sequences into fixed-length blocks.\n",
    "\n",
    "# Device mapping\n",
    "device_map = {\"\": 0} # Or \"auto\" for automatic device placement by accelerate/bitsandbytes\n",
    "\n",
    "# --- Paths to your datasets ---\n",
    "PERSONALITY_DATASET_NAME = \"holistic-ai/personality_manipulation\" # NEW: Hugging Face dataset name\n",
    "BBQ_DATA_FILE = \"/cs/student/projects3/aisd/2024/ghanda/bbq_ambiguous_with_metadata.csv\"\n",
    "\n",
    "# --- Azure OpenAI Configuration (Only used for PAE scoring and potentially old prompt methods) ---\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL_NAME = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "AZURE_OPENAI_SUBSCRIPTION_KEY = os.getenv(\"AZURE_OPENAI_SUBSCRIPTION_KEY\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "# Initialize Azure OpenAI client if credentials are provided\n",
    "azure_openai_client = None\n",
    "if all([AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_MODEL_NAME, AZURE_OPENAI_DEPLOYMENT, AZURE_OPENAI_SUBSCRIPTION_KEY, AZURE_OPENAI_API_VERSION]):\n",
    "    try:\n",
    "        from openai import AzureOpenAI\n",
    "        azure_openai_client = AzureOpenAI(\n",
    "            api_version=AZURE_OPENAI_API_VERSION,\n",
    "            azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "            api_key=AZURE_OPENAI_SUBSCRIPTION_KEY,\n",
    "        )\n",
    "        print(\"Azure OpenAI client initialized for PAE scoring.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Azure OpenAI client: {e}. PAE scoring might fail.\")\n",
    "else:\n",
    "    print(\"Warning: Azure OpenAI credentials not fully set. PAE scoring might fail.\")\n",
    "\n",
    "LLM_MODEL_FOR_PAE_SCORING = AZURE_OPENAI_DEPLOYMENT # PAE uses Azure OpenAI as specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 1: PEFT/QLORA FINE-TUNING ---\n",
      "\n",
      "--- Loading and Preparing Personality Data for Fine-tuning ---\n",
      "Loaded 'holistic-ai/personality_manipulation' train split. Number of samples: 4000\n",
      "Detected personalities for fine-tuning/evaluation: ['extraversion', 'agreeableness', 'neuroticism', 'openness', 'conscientiousness']\n",
      "\n",
      "--- All PEFT adapters found in 'peft_output_models'. Skipping fine-tuning. ---\n",
      "Loading base model google/gemma-2-2b and tokenizer once...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer model_max_length set to: 512\n",
      "Tokenizer chat_template set for Gemma-2B.\n",
      "Base model and tokenizer loaded successfully.\n",
      "\n",
      "--- Skipping fine-tuning as all adapters are already present. ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 1: PEFT/QLORA FINE-TUNING FOR EACH PERSONALITY\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PART 1: PEFT/QLORA FINE-TUNING ---\")\n",
    "\n",
    "# Load and prepare Personality data (needed to know which personalities to loop through)\n",
    "print(\"\\n--- Loading and Preparing Personality Data for Fine-tuning ---\")\n",
    "try:\n",
    "    # Load the 'train' split of the dataset for fine-tuning\n",
    "    personality_dataset_hf_train = load_dataset(PERSONALITY_DATASET_NAME, split='train')\n",
    "    # Convert to pandas DataFrame for consistent filtering logic\n",
    "    df_personality_raw = personality_dataset_hf_train.to_pandas()\n",
    "    print(f\"Loaded '{PERSONALITY_DATASET_NAME}' train split. Number of samples: {len(df_personality_raw)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Could not load '{PERSONALITY_DATASET_NAME}' train split. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "target_personalities = df_personality_raw['Target Personality'].unique().tolist()\n",
    "print(f\"Detected personalities for fine-tuning/evaluation: {target_personalities}\")\n",
    "\n",
    "# Check if all adapters already exist\n",
    "all_adapters_exist = True\n",
    "for trait in target_personalities:\n",
    "    adapter_path = os.path.join(OUTPUT_DIR_BASE, trait.lower().replace(\" \", \"_\"))\n",
    "    if not os.path.exists(adapter_path):\n",
    "        all_adapters_exist = False\n",
    "        break\n",
    "\n",
    "if all_adapters_exist:\n",
    "    print(f\"\\n--- All PEFT adapters found in '{OUTPUT_DIR_BASE}'. Skipping fine-tuning. ---\")\n",
    "    # Set a flag to skip the training loop\n",
    "    SKIP_FINE_TUNING = True\n",
    "else:\n",
    "    print(f\"\\n--- Not all PEFT adapters found in '{OUTPUT_DIR_BASE}'. Proceeding with fine-tuning. ---\")\n",
    "    SKIP_FINE_TUNING = False\n",
    "\n",
    "# Prepare few-shot examples (for context/old prompt-based methods, not for PEFT inference)\n",
    "personality_examples_for_eval = {}\n",
    "for trait in target_personalities:\n",
    "    trait_df = df_personality_raw[df_personality_raw['Target Personality'] == trait]\n",
    "    personality_examples_for_eval[trait] = list(zip(trait_df['Question'], trait_df['Answer']))[:5]\n",
    "\n",
    "\n",
    "# --- Prompt Formatting Function for SFTTrainer (for TRAINING only) ---\n",
    "def create_training_prompt_format(sample):\n",
    "    \"\"\"\n",
    "    Formats a sample from the personality dataset into the chat template\n",
    "    expected by Gemma-2B fine-tuning for SFTTrainer.\n",
    "    \"\"\"\n",
    "    formatted_chat = [\n",
    "        {\"role\": \"user\", \"content\": sample['Question']},\n",
    "        {\"role\": \"assistant\", \"content\": sample['Answer']}\n",
    "    ]\n",
    "    sample[\"text\"] = f\"<s>[INST] {sample['Question']} [/INST]{sample['Answer']}</s>\"\n",
    "    return sample\n",
    "\n",
    "# --- Quantization Configuration ---\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8: \n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Dictionary to store loaded PEFT models for evaluation (populated in Part 2)\n",
    "peft_models_for_eval = {} \n",
    "tokenizer_for_eval = None \n",
    "\n",
    "# --- START BASE MODEL & TOKENIZER LOADING (ONCE, ALWAYS REQUIRED FOR EVAL) ---\n",
    "print(f\"Loading base model {MODEL_NAME} and tokenizer once...\")\n",
    "try:\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=compute_dtype,  # ADDED: Explicit dtype specification\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    base_model.config.use_cache = False \n",
    "    base_model.config.pretraining_tp = 1 \n",
    "\n",
    "    if not base_model.training:\n",
    "        base_model.eval()\n",
    "\n",
    "    tokenizer_for_eval = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    if tokenizer_for_eval.pad_token is None:\n",
    "        tokenizer_for_eval.pad_token = tokenizer_for_eval.eos_token\n",
    "    tokenizer_for_eval.padding_side = \"right\" \n",
    "    \n",
    "    tokenizer_for_eval.model_max_length = max_seq_length \n",
    "    print(f\"Tokenizer model_max_length set to: {tokenizer_for_eval.model_max_length}\")\n",
    "\n",
    "    # Set the chat_template explicitly for Gemma-2B, crucial for apply_chat_template\n",
    "    tokenizer_for_eval.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'tool' %}{{ '<tool_code>' + message['content'] + '</tool_code>' }}{% elif message['role'] == 'assistant' %}{{ message['content'] }}{% else %}{{ raise_exception('Unknown role: ' + message['role']) }}{% endif %}{% endfor %}\"\n",
    "    print(\"Tokenizer chat_template set for Gemma-2B.\")\n",
    "\n",
    "    print(\"Base model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL: Could not load base model or tokenizer. Error: {e}\")\n",
    "    print(\"This often means the model is too large for your GPU's VRAM even with quantization.\")\n",
    "    print(\"Consider reducing per_device_train_batch_size, increasing gradient_accumulation_steps, or using a smaller model.\")\n",
    "    exit()\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "\n",
    "# --- Conditional Fine-tuning Loop ---\n",
    "if not SKIP_FINE_TUNING:\n",
    "    print(\"\\n--- Starting Fine-tuning for Each Personality ---\")\n",
    "    for current_trait in target_personalities:\n",
    "        print(f\"\\n***** FINE-TUNING FOR PERSONALITY: {current_trait.upper()} *****\")\n",
    "\n",
    "        filtered_df = df_personality_raw[df_personality_raw['Target Personality'] == current_trait].copy()\n",
    "        if filtered_df.empty:\n",
    "            print(f\"No data found for personality '{current_trait}'. Skipping fine-tuning.\")\n",
    "            continue\n",
    "\n",
    "        train_dataset = Dataset.from_pandas(filtered_df).map(create_training_prompt_format)\n",
    "        train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if col != 'text'])\n",
    "\n",
    "        print(f\"Prepared {len(train_dataset)} samples for '{current_trait}' fine-tuning.\")\n",
    "        if len(train_dataset) > 0:\n",
    "            print(f\"Sample formatted text for '{current_trait}':\\n{train_dataset[0]['text']}\")\n",
    "\n",
    "        current_output_dir = os.path.join(OUTPUT_DIR_BASE, current_trait.lower().replace(\" \", \"_\"))\n",
    "        new_model_name_for_hub = f\"{FINE_TUNED_MODEL_BASE_NAME}-{current_trait.lower().replace(' ', '-')}\"\n",
    "        os.makedirs(current_output_dir, exist_ok=True)\n",
    "\n",
    "        peft_config = LoraConfig(\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            r=lora_r,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=[\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        training_args_sft = SFTConfig(\n",
    "            output_dir=current_output_dir,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            optim=optim,\n",
    "            save_steps=save_steps,\n",
    "            logging_steps=logging_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            fp16=fp16,\n",
    "            bf16=bf16,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            max_steps=max_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            group_by_length=group_by_length,\n",
    "            lr_scheduler_type=lr_scheduler_type,\n",
    "            report_to=\"tensorboard\", \n",
    "            logging_dir=f\"{current_output_dir}/logs\",\n",
    "            remove_unused_columns=False, \n",
    "            gradient_checkpointing=gradient_checkpointing,\n",
    "\n",
    "            max_length=max_seq_length,      \n",
    "            packing=packing,                \n",
    "            dataset_text_field=\"text\",      \n",
    "        )\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model=base_model, \n",
    "            train_dataset=train_dataset,\n",
    "            peft_config=peft_config,\n",
    "            args=training_args_sft, \n",
    "        )\n",
    "\n",
    "        print(f\"Training PEFT model for {current_trait}...\")\n",
    "        trainer.train()\n",
    "\n",
    "        print(f\"Saving PEFT adapter for {current_trait} to {current_output_dir}...\")\n",
    "        trainer.model.save_pretrained(current_output_dir)\n",
    "        \n",
    "        try:\n",
    "            trainer.model.push_to_hub(new_model_name_for_hub)\n",
    "            print(f\"Successfully pushed {new_model_name_for_hub} to Hugging Face Hub.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not push {new_model_name_for_hub} to Hub. Error: {e}\")\n",
    "\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Finished fine-tuning for {current_trait}. GPU cache cleared.\")\n",
    "\n",
    "    tokenizer_for_eval.save_pretrained(os.path.join(OUTPUT_DIR_BASE, \"tokenizer\"))\n",
    "    print(f\"Tokenizer saved to {os.path.join(OUTPUT_DIR_BASE, 'tokenizer')}\")\n",
    "    print(\"\\n--- All PEFT Fine-tuning completed. ---\")\n",
    "else:\n",
    "    print(\"\\n--- Skipping fine-tuning as all adapters are already present. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 2: Loading Fine-tuned PEFT Models for Evaluation (Fixed Memory Management) ---\n",
      "GPU cache cleared before loading adapters.\n",
      "Loading adapters into single PEFT model instance...\n",
      "Loading adapter for extraversion from peft_output_models/extraversion...\n",
      "First adapter 'extraversion' loaded and PEFT model created.\n",
      "Loading adapter for agreeableness from peft_output_models/agreeableness...\n",
      "Additional adapter 'agreeableness' loaded.\n",
      "Loading adapter for neuroticism from peft_output_models/neuroticism...\n",
      "Additional adapter 'neuroticism' loaded.\n",
      "Loading adapter for openness from peft_output_models/openness...\n",
      "Additional adapter 'openness' loaded.\n",
      "Loading adapter for conscientiousness from peft_output_models/conscientiousness...\n",
      "CUDA OOM loading adapter for conscientiousness. Skipping...\n",
      "\n",
      "Creating neutral model instance...\n",
      "Using original base model for neutral responses.\n",
      "Loaded adapters: ['extraversion', 'agreeableness', 'neuroticism', 'openness']\n",
      "Total adapters loaded: 4\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 2: LOAD FINE-TUNED PEFT MODELS FOR EVALUATION - FIXED MEMORY MANAGEMENT\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PART 2: Loading Fine-tuned PEFT Models for Evaluation (Fixed Memory Management) ---\")\n",
    "\n",
    "# Clear all GPU memory before starting\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU cache cleared before loading adapters.\")\n",
    "\n",
    "# Store loaded adapters for memory-efficient switching\n",
    "loaded_adapters = {}\n",
    "peft_model = None  # Single PEFT model instance\n",
    "\n",
    "# Load adapters one by one into the same base model\n",
    "print(\"Loading adapters into single PEFT model instance...\")\n",
    "is_first_adapter = True\n",
    "\n",
    "for current_trait in target_personalities:\n",
    "    adapter_name = current_trait.lower().replace(\" \", \"_\")\n",
    "    adapter_path = os.path.join(OUTPUT_DIR_BASE, adapter_name)\n",
    "    \n",
    "    if os.path.exists(adapter_path):\n",
    "        print(f\"Loading adapter for {current_trait} from {adapter_path}...\")\n",
    "        \n",
    "        try:\n",
    "            if is_first_adapter:\n",
    "                # Create the first PEFT model instance\n",
    "                peft_model = PeftModel.from_pretrained(\n",
    "                    base_model, \n",
    "                    adapter_path, \n",
    "                    adapter_name=adapter_name,\n",
    "                    torch_dtype=compute_dtype\n",
    "                )\n",
    "                is_first_adapter = False\n",
    "                print(f\"First adapter '{adapter_name}' loaded and PEFT model created.\")\n",
    "            else:\n",
    "                # Load additional adapters into the same model\n",
    "                peft_model.load_adapter(adapter_path, adapter_name=adapter_name)\n",
    "                print(f\"Additional adapter '{adapter_name}' loaded.\")\n",
    "            \n",
    "            loaded_adapters[current_trait] = adapter_name\n",
    "            \n",
    "            # Clear cache after each adapter load\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(f\"CUDA OOM loading adapter for {current_trait}. Skipping...\")\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                print(f\"Error loading adapter for {current_trait}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading adapter for {current_trait}: {e}\")\n",
    "    else:\n",
    "        print(f\"Adapter not found for {current_trait} at {adapter_path}\")\n",
    "\n",
    "# Create a clean base model instance for neutral responses\n",
    "print(\"\\nCreating neutral model instance...\")\n",
    "neutral_model = None\n",
    "\n",
    "try:\n",
    "    # Don't create a new model - use the base model with no active adapter\n",
    "    neutral_model = base_model  # Use the original base model\n",
    "    print(\"Using original base model for neutral responses.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up neutral model: {e}\")\n",
    "    neutral_model = base_model\n",
    "\n",
    "print(f\"Loaded adapters: {list(loaded_adapters.keys())}\")\n",
    "print(f\"Total adapters loaded: {len(loaded_adapters)}\")\n",
    "\n",
    "# Final memory cleanup\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 3: Starting Evaluation with PEFT Models ---\n",
      "\n",
      "--- Loading Hugging Face Personality Classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face personality classifier loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 3: EVALUATION WITH PEFT MODELS - FIXED ADAPTER MANAGEMENT\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PART 3: Starting Evaluation with PEFT Models ---\")\n",
    "\n",
    "# --- Import missing load_dataset ---\n",
    "from datasets import load_dataset  # Add this import at the top of your file\n",
    "\n",
    "# --- Trait Definitions and Other Setup Code (unchanged) ---\n",
    "trait_definitions = {\n",
    "    \"openness\": \"Reflects the degree of intellectual curiosity, creativity, and preference for novelty and variety.\",\n",
    "    \"conscientiousness\": \"Reflects a tendency to be organized, dependable, and show self-discipline.\",\n",
    "    \"extraversion\": \"Reflects a tendency to be outgoing, energetic, and seek the company of others.\",\n",
    "    \"agreeableness\": \"Reflects a tendency to be compassionate and cooperative toward others.\",\n",
    "    \"neuroticism\": \"Reflects a tendency to experience unpleasant emotions easily, such as anger, anxiety, or depression.\",\n",
    "}\n",
    "\n",
    "def create_inference_prompt_format(question):\n",
    "    \"\"\"\n",
    "    Formats a question for inference with a fine-tuned Gemma-2B PEFT model.\n",
    "    No personality instruction or few-shot examples are needed here, as the personality\n",
    "    is encoded in the fine-tuned adapter weights.\n",
    "    \"\"\"\n",
    "    return tokenizer_for_eval.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": question}],\n",
    "        tokenize=False,\n",
    "        add_special_tokens=True \n",
    "    )\n",
    "\n",
    "def get_peft_llm_response(model, tokenizer, question, max_new_tokens=200, temperature=0.7, top_p=0.9, do_sample=True):\n",
    "    \"\"\"\n",
    "    Function to get a response from a loaded PEFT LLM.\n",
    "    \"\"\"\n",
    "    current_active_adapter_state = getattr(model, 'active_adapter', None)\n",
    "    \n",
    "    active_model_id_for_logging = 'base_model (no adapter active)'\n",
    "    if current_active_adapter_state is not None:\n",
    "        active_model_id_for_logging = current_active_adapter_state\n",
    "    \n",
    "    print(f\"--- Calling PEFT LLM (Personality: {active_model_id_for_logging}) ---\")\n",
    "\n",
    "    full_input_prompt = create_inference_prompt_format(question)\n",
    "    input_ids = tokenizer(full_input_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generation_config = model.generation_config\n",
    "    if generation_config is None:\n",
    "        generation_config = tokenizer.generation_config\n",
    "    \n",
    "    generation_config.temperature = temperature\n",
    "    generation_config.top_p = top_p\n",
    "    generation_config.do_sample = do_sample\n",
    "    generation_config.max_new_tokens = max_new_tokens\n",
    "    generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **input_ids,\n",
    "                generation_config=generation_config\n",
    "            )\n",
    "        generated_text = tokenizer.decode(outputs[0][len(input_ids[\"input_ids\"][0]):], skip_special_tokens=True)\n",
    "        return generated_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PEFT model generation: {e}\")\n",
    "        return \"ERROR: PEFT model generation failed.\"\n",
    "\n",
    "# --- Helper functions (unchanged) ---\n",
    "def extract_question_and_choices(full_prompt_string):\n",
    "    question_match = re.search(r'<question>(.*?)</question>', full_prompt_string, re.DOTALL)\n",
    "    choices_match = re.search(r'<choices>(.*?)</choices>', full_prompt_string, re.DOTALL)\n",
    "    question_text = question_match.group(1).strip() if question_match else \"\"\n",
    "    choices_raw_str = choices_match.group(1).strip() if choices_match else \"\"\n",
    "    return question_text, choices_raw_str\n",
    "\n",
    "def parse_choices_string(choices_str):\n",
    "    parsed_choices = []\n",
    "    matches = re.findall(r'\\([A-Z]\\):\\s*(.*?)(?=\\s*\\([A-Z]\\):\\s*|$)', choices_str)\n",
    "    for match in matches:\n",
    "        parsed_choices.append(match.strip())\n",
    "    return parsed_choices if parsed_choices else [choices_str]\n",
    "\n",
    "def categorize_opinionqa_response(raw_response, choices_list):\n",
    "    raw_response_lower = raw_response.lower()\n",
    "    for choice in choices_list:\n",
    "        if re.search(r'\\b' + re.escape(choice.lower()) + r'\\b', raw_response_lower):\n",
    "            return choice\n",
    "    if any(word in raw_response_lower for word in [\"yes\", \"agree\", \"positive\", \"positively\"]):\n",
    "        if \"Yes\" in choices_list: return \"Yes\"\n",
    "        if \"Agree\" in choices_list: return \"Agree\"\n",
    "        if \"Strongly Agree\" in choices_list: return \"Strongly Agree\"\n",
    "    if any(word in raw_response_lower for word in [\"no\", \"disagree\", \"negative\", \"negatively\"]):\n",
    "        if \"No\" in choices_list: return \"No\"\n",
    "        if \"Disagree\" in choices_list: return \"Disagree\"\n",
    "        if \"Strongly Disagree\" in choices_list: return \"Strongly Disagree\"\n",
    "    if any(word in raw_response_lower for word in [\"neutral\", \"balanced\", \"both\", \"neither\"]):\n",
    "        if \"Neutral\" in choices_list: return \"Neutral\"\n",
    "        if \"Seek Balance\" in choices_list: return \"Seek Balance\"\n",
    "        if \"Uncategorized\" in choices_list: return \"Uncategorized\"\n",
    "    return \"Uncategorized\"\n",
    "\n",
    "def map_categorized_to_label(categorized_text, choices_raw_str):\n",
    "    if not isinstance(choices_raw_str, str):\n",
    "        return 'UNKNOWN'\n",
    "    matches = re.findall(r'\\((\\w)\\):\\s*(.*?)(?=\\s*\\([A-Z]\\):|$)', choices_raw_str)\n",
    "    for letter, choice_text in matches:\n",
    "        if categorized_text.lower() == choice_text.strip().lower():\n",
    "            return letter.upper()\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "# --- Load Hugging Face Personality Classifier ---\n",
    "print(\"\\n--- Loading Hugging Face Personality Classifier ---\")\n",
    "try:\n",
    "    personality_classifier = pipeline(\"text-classification\", model=\"holistic-ai/personality_classifier\")\n",
    "    print(\"Hugging Face personality classifier loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Hugging Face classifier: {e}\")\n",
    "    personality_classifier = lambda text: [{'label': 'unknown', 'score': 0.0, 'error': str(e)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 3: Fixed Evaluation with Proper Adapter Management ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 3 v2: FIXED EVALUATION WITH PROPER ADAPTER MANAGEMENT\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PART 3: Fixed Evaluation with Proper Adapter Management ---\")\n",
    "\n",
    "def switch_to_personality(personality_trait):\n",
    "    \"\"\"\n",
    "    Switch the model to use a specific personality adapter or neutral mode.\n",
    "    Returns the appropriate model instance.\n",
    "    \"\"\"\n",
    "    global peft_model, neutral_model\n",
    "    \n",
    "    if personality_trait == \"neutral\":\n",
    "        # For neutral, we need to ensure no adapter is active\n",
    "        if peft_model is not None and hasattr(peft_model, 'disable_adapters'):\n",
    "            # Use context manager to temporarily disable adapters\n",
    "            return neutral_model, \"neutral_context\"\n",
    "        else:\n",
    "            return neutral_model, \"neutral_base\"\n",
    "    else:\n",
    "        # For personality traits, switch to the appropriate adapter\n",
    "        if peft_model is not None and personality_trait in loaded_adapters:\n",
    "            adapter_name = loaded_adapters[personality_trait]\n",
    "            try:\n",
    "                peft_model.set_adapter(adapter_name)\n",
    "                return peft_model, adapter_name\n",
    "            except Exception as e:\n",
    "                print(f\"Error switching to adapter {adapter_name}: {e}\")\n",
    "                return peft_model, \"error_fallback\"\n",
    "        else:\n",
    "            print(f\"Adapter not found for {personality_trait}, using base model\")\n",
    "            return neutral_model, \"fallback_base\"\n",
    "\n",
    "def get_peft_llm_response_fixed(personality_trait, question, max_new_tokens=200, temperature=0.7, top_p=0.9, do_sample=True):\n",
    "    \"\"\"\n",
    "    Fixed function to get response from PEFT model with proper adapter management.\n",
    "    \"\"\"\n",
    "    # Switch to the appropriate model/adapter\n",
    "    current_model, adapter_status = switch_to_personality(personality_trait)\n",
    "    \n",
    "    print(f\"--- Calling PEFT LLM (Personality: {personality_trait}, Status: {adapter_status}) ---\")\n",
    "    \n",
    "    # Create input prompt\n",
    "    full_input_prompt = create_inference_prompt_format(question)\n",
    "    \n",
    "    try:\n",
    "        # Tokenize input\n",
    "        input_ids = tokenizer_for_eval(full_input_prompt, return_tensors=\"pt\").to(current_model.device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            # Use disable_adapters context for neutral responses\n",
    "            if personality_trait == \"neutral\" and hasattr(current_model, 'disable_adapters'):\n",
    "                with current_model.disable_adapters():\n",
    "                    outputs = current_model.generate(\n",
    "                        **input_ids,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        temperature=temperature,\n",
    "                        top_p=top_p,\n",
    "                        do_sample=do_sample,\n",
    "                        pad_token_id=tokenizer_for_eval.pad_token_id,\n",
    "                        eos_token_id=tokenizer_for_eval.eos_token_id\n",
    "                    )\n",
    "            else:\n",
    "                outputs = current_model.generate(\n",
    "                    **input_ids,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    do_sample=do_sample,\n",
    "                    pad_token_id=tokenizer_for_eval.pad_token_id,\n",
    "                    eos_token_id=tokenizer_for_eval.eos_token_id\n",
    "                )\n",
    "        \n",
    "        # Decode response\n",
    "        generated_text = tokenizer_for_eval.decode(\n",
    "            outputs[0][len(input_ids[\"input_ids\"][0]):], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        return generated_text.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation for {personality_trait}: {e}\")\n",
    "        return f\"ERROR: Generation failed for {personality_trait}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5: Verifying Personality Change with OpinionQA (Using PEFT Models) ---\n",
      "OpinionQA dataset loaded from Hugging Face. Number of samples: 294714\n",
      "\n",
      "--- Inspecting OpinionQA Dataset Columns ---\n",
      "                                                                   0\n",
      "prompt             <persona>\\nRacially, the person is refused. Th...\n",
      "answer                                                             B\n",
      "uid                                   American_Trends_Panel_W92_6823\n",
      "folder                                     American_Trends_Panel_W92\n",
      "question_id                                            BIGHOUSES_W92\n",
      "__index_level_0__                                             460290\n",
      "Creating separate neutral model without adapters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not create separate neutral model: CUDA out of memory. Tried to allocate 2.20 GiB. GPU 0 has a total capacity of 23.55 GiB of which 693.44 MiB is free. Including non-PyTorch memory, this process has 7.09 GiB memory in use. Process 128138 has 15.65 GiB memory in use. Of the allocated memory 3.92 GiB is allocated by PyTorch, and 2.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Will use base model with disabled adapter as fallback.\n",
      "\n",
      "--- Running OpinionQA for Personality: neutral ---\n",
      "Using dedicated neutral model (no adapters).\n",
      "DEBUG: Current model active_adapter is: neutral_model (no adapters)\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "Batch classifying 20 responses for 'neutral'...\n",
      "\n",
      "--- Running OpinionQA for Personality: extraversion ---\n",
      "Activated adapter: extraversion\n",
      "DEBUG: Current model active_adapter is: extraversion\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n",
      "--- Calling PEFT LLM (Personality: extraversion) ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[6], line 80\u001b[0m\n",
      "\u001b[1;32m     77\u001b[0m question_text, choices_raw_str \u001b[38;5;241m=\u001b[39m extract_question_and_choices(full_prompt_from_dataset)\n",
      "\u001b[1;32m     78\u001b[0m parsed_choices \u001b[38;5;241m=\u001b[39m parse_choices_string(choices_raw_str)\n",
      "\u001b[0;32m---> 80\u001b[0m llm_response \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_llm_response\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_model_for_inference\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_for_eval\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuestion: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mquestion_text\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mChoices: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mchoices_raw_str\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mAnswer:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m \u001b[49m\n",
      "\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     88\u001b[0m responses_to_classify_batch\u001b[38;5;241m.\u001b[39mappend(llm_response)\n",
      "\u001b[1;32m     89\u001b[0m metadata_for_batch\u001b[38;5;241m.\u001b[39mappend({\n",
      "\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintended_personality\u001b[39m\u001b[38;5;124m\"\u001b[39m: personality_trait,\n",
      "\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: question_id,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_categorized_response_oq\u001b[39m\u001b[38;5;124m\"\u001b[39m: categorize_opinionqa_response(llm_response, parsed_choices)\n",
      "\u001b[1;32m     99\u001b[0m })\n",
      "\n",
      "Cell \u001b[0;32mIn[5], line 58\u001b[0m, in \u001b[0;36mget_peft_llm_response\u001b[0;34m(model, tokenizer, question, max_new_tokens, temperature, top_p, do_sample)\u001b[0m\n",
      "\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;32m---> 58\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n",
      "\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     62\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(input_ids[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]):], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generated_text\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/peft/peft_model.py:1968\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1966\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;32m   1967\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n",
      "\u001b[0;32m-> 1968\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1969\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m   1970\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n",
      "\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n",
      "\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/generation/utils.py:2623\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   2615\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n",
      "\u001b[1;32m   2616\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n",
      "\u001b[1;32m   2617\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n",
      "\u001b[1;32m   2618\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n",
      "\u001b[1;32m   2619\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n",
      "\u001b[1;32m   2620\u001b[0m     )\n",
      "\u001b[1;32m   2622\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n",
      "\u001b[0;32m-> 2623\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   2624\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   2633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n",
      "\u001b[1;32m   2634\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n",
      "\u001b[1;32m   2635\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n",
      "\u001b[1;32m   2636\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n",
      "\u001b[1;32m   2637\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n",
      "\u001b[1;32m   2638\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n",
      "\u001b[1;32m   2639\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n",
      "\u001b[1;32m   2640\u001b[0m     )\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/generation/utils.py:3607\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n",
      "\u001b[1;32m   3605\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;32m   3606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 3607\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   3609\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n",
      "\u001b[1;32m   3610\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n",
      "\u001b[1;32m   3611\u001b[0m     outputs,\n",
      "\u001b[1;32m   3612\u001b[0m     model_kwargs,\n",
      "\u001b[1;32m   3613\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n",
      "\u001b[1;32m   3614\u001b[0m )\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n",
      "\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:583\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n",
      "\u001b[1;32m    579\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m    580\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n",
      "\u001b[1;32m    581\u001b[0m )\n",
      "\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n",
      "\u001b[0;32m--> 583\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    594\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    596\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n",
      "\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:470\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n",
      "\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n",
      "\u001b[1;32m    468\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "\u001b[0;32m--> 470\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    482\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n",
      "\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n",
      "\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n",
      "\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:295\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    291\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "\u001b[1;32m    293\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[1;32m    296\u001b[0m     outputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (self_attn_weights,)\n",
      "\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- OpinionQA Evaluation with FIXED Adapter Management ---\n",
    "print(\"\\n--- Step 5: Verifying Personality Change with OpinionQA (Using PEFT Models) ---\")\n",
    "\n",
    "# Load OpinionQA dataset\n",
    "try:\n",
    "    opinionqa_dataset = load_dataset(\"RiverDong/OpinionQA\", split=\"test\")\n",
    "    print(f\"OpinionQA dataset loaded from Hugging Face. Number of samples: {len(opinionqa_dataset)}\")\n",
    "    df_opinionqa_questions = opinionqa_dataset.to_pandas()\n",
    "    print(\"\\n--- Inspecting OpinionQA Dataset Columns ---\")\n",
    "    print(df_opinionqa_questions.head(1).T)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading OpinionQA dataset: {e}. Falling back to dummy OpinionQA dataset.\")\n",
    "    df_opinionqa_questions = pd.DataFrame([\n",
    "        {\"id\": \"op1\", \"prompt\": \"<persona>...</persona> <question>Do you believe social media positively impacts society?</question> <choices>(A): Yes (B): No (C): Neutral</choices>\", \"answer\": \"A\", \"question_id\": \"DUMMY_Q1\"},\n",
    "        {\"id\": \"op2\", \"prompt\": \"<persona>...</persona> <question>Is it important for a leader to prioritize group harmony?</question> <choices>(A): Strongly Agree (B): Disagree</choices>\", \"answer\": \"B\", \"question_id\": \"DUMMY_Q2\"},\n",
    "    ])\n",
    "\n",
    "all_opinionqa_results = []\n",
    "all_opinion_qa_personalities = [\"neutral\"] + list(target_personalities) \n",
    "\n",
    "# FIXED: Create a separate neutral model for true neutral responses\n",
    "neutral_model = None\n",
    "if hasattr(base_model, 'peft_config') and base_model.peft_config:\n",
    "    # Create a fresh copy of the base model without any adapters for neutral\n",
    "    print(\"Creating separate neutral model without adapters...\")\n",
    "    try:\n",
    "        neutral_model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=device_map\n",
    "        )\n",
    "        neutral_model.config.use_cache = False \n",
    "        neutral_model.config.pretraining_tp = 1\n",
    "        print(\"Neutral model created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create separate neutral model: {e}\")\n",
    "        print(\"Will use base model with disabled adapter as fallback.\")\n",
    "        neutral_model = base_model\n",
    "\n",
    "for personality_trait in all_opinion_qa_personalities:\n",
    "    print(f\"\\n--- Running OpinionQA for Personality: {personality_trait} ---\")\n",
    "    \n",
    "    # FIXED: Use appropriate model for each personality\n",
    "    if personality_trait == \"neutral\":\n",
    "        current_model_for_inference = neutral_model\n",
    "        print(\"Using dedicated neutral model (no adapters).\")\n",
    "    else:\n",
    "        current_model_for_inference = base_model\n",
    "        adapter_id = personality_trait.lower().replace(\" \", \"_\") \n",
    "        if adapter_id in current_model_for_inference.peft_config: \n",
    "            current_model_for_inference.set_adapter(adapter_id)\n",
    "            print(f\"Activated adapter: {adapter_id}\")\n",
    "        else:\n",
    "            print(f\"Warning: Adapter '{adapter_id}' not found for {personality_trait}. Using base model.\")\n",
    "\n",
    "    # Debug check\n",
    "    if personality_trait == \"neutral\":\n",
    "        debug_active_adapter = \"neutral_model (no adapters)\"\n",
    "    else:\n",
    "        debug_active_adapter = getattr(current_model_for_inference, 'active_adapter', 'Not a PeftModel or no adapter set')\n",
    "    print(f\"DEBUG: Current model active_adapter is: {debug_active_adapter}\")\n",
    "\n",
    "    N_OPINIONQA_SAMPLES_PER_PERSONALITY = 20\n",
    "    opinionqa_subset_for_testing = df_opinionqa_questions.sample(\n",
    "        min(len(df_opinionqa_questions), N_OPINIONQA_SAMPLES_PER_PERSONALITY), \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    responses_to_classify_batch = []\n",
    "    metadata_for_batch = []\n",
    "\n",
    "    for index, item_row in opinionqa_subset_for_testing.iterrows():\n",
    "        full_prompt_from_dataset = item_row['prompt']\n",
    "        question_id = item_row['question_id']\n",
    "        human_true_answer_label = item_row['answer']\n",
    "\n",
    "        question_text, choices_raw_str = extract_question_and_choices(full_prompt_from_dataset)\n",
    "        parsed_choices = parse_choices_string(choices_raw_str)\n",
    "        \n",
    "        llm_response = get_peft_llm_response(\n",
    "            model=current_model_for_inference,\n",
    "            tokenizer=tokenizer_for_eval,\n",
    "            question=f\"Question: {question_text}\\nChoices: {choices_raw_str}\\nAnswer:\",\n",
    "            max_new_tokens=100, \n",
    "            temperature=0.7 \n",
    "        )\n",
    "\n",
    "        responses_to_classify_batch.append(llm_response)\n",
    "        metadata_for_batch.append({\n",
    "            \"intended_personality\": personality_trait,\n",
    "            \"question_id\": question_id,\n",
    "            \"full_dataset_prompt\": full_prompt_from_dataset,\n",
    "            \"extracted_question\": question_text,\n",
    "            \"choices_raw_str\": choices_raw_str,\n",
    "            \"parsed_choices\": parsed_choices,\n",
    "            \"human_true_answer_label\": human_true_answer_label,\n",
    "            \"llm_raw_response\": llm_response,\n",
    "            \"llm_categorized_response_oq\": categorize_opinionqa_response(llm_response, parsed_choices)\n",
    "        })\n",
    "\n",
    "    if responses_to_classify_batch:\n",
    "        print(f\"Batch classifying {len(responses_to_classify_batch)} responses for '{personality_trait}'...\")\n",
    "        batched_classifier_results = personality_classifier(responses_to_classify_batch)\n",
    "        \n",
    "        for i, class_result in enumerate(batched_classifier_results):\n",
    "            metadata_entry = metadata_for_batch[i]\n",
    "            metadata_entry[\"llm_predicted_trait_TA\"] = class_result['label']\n",
    "            metadata_entry[\"llm_predicted_trait_TA_confidence\"] = class_result['score']\n",
    "            all_opinionqa_results.append(metadata_entry)\n",
    "\n",
    "# Rest of the analysis code remains the same...\n",
    "df_opinionqa_results = pd.DataFrame(all_opinionqa_results)\n",
    "print(\"\\n--- Raw OpinionQA Results Sample (First 5 rows) ---\")\n",
    "print(df_opinionqa_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fixed OpinionQA Evaluation ---\n",
      "OpinionQA dataset loaded. Number of samples: 294714\n",
      "\n",
      "--- Running OpinionQA for Personality: neutral ---\n",
      "--- Calling PEFT LLM (Personality: neutral, Status: neutral_context) ---\n",
      "Error during generation for neutral: No adapter loaded. Please load an adapter first.\n",
      "--- Calling PEFT LLM (Personality: neutral, Status: neutral_context) ---\n",
      "Error during generation for neutral: No adapter loaded. Please load an adapter first.\n",
      "--- Calling PEFT LLM (Personality: neutral, Status: neutral_context) ---\n",
      "Error during generation for neutral: No adapter loaded. Please load an adapter first.\n",
      "--- Calling PEFT LLM (Personality: neutral, Status: neutral_context) ---\n",
      "Error during generation for neutral: No adapter loaded. Please load an adapter first.\n",
      "--- Calling PEFT LLM (Personality: neutral, Status: neutral_context) ---\n",
      "Error during generation for neutral: No adapter loaded. Please load an adapter first.\n",
      "--- Calling PEFT LLM (Personality: neutral, Status: neutral_context) ---\n",
      "Error during generation for neutral: No adapter loaded. Please load an adapter first.\n",
      "--- Calling PEFT LLM (Personality: neutral, Status: neutral_context) ---\n",
      "Error during generation for neutral: No adapter loaded. Please load an adapter first.\n",
      "--- Calling PEFT LLM (Personality: neutral, Status: neutral_context) ---\n",
      "Error during generation for neutral: No adapter loaded. Please load an adapter first.\n",
      "--- Calling PEFT LLM (Personality: neutral, Status: neutral_context) ---\n",
      "Error during generation for neutral: No adapter loaded. Please load an adapter first.\n",
      "--- Calling PEFT LLM (Personality: neutral, Status: neutral_context) ---\n",
      "Error during generation for neutral: No adapter loaded. Please load an adapter first.\n",
      "Classifying 10 responses for neutral...\n",
      "Completed evaluation for neutral\n",
      "\n",
      "--- Running OpinionQA for Personality: extraversion ---\n",
      "--- Calling PEFT LLM (Personality: extraversion, Status: extraversion) ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m parsed_choices \u001b[38;5;241m=\u001b[39m parse_choices_string(choices_str)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Get LLM response with fixed adapter management\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m llm_response \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_llm_response_fixed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersonality_trait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersonality_trait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuestion: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mquestion_text\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mChoices: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mchoices_str\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mAnswer:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\n\u001b[1;32m     52\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m responses_batch\u001b[38;5;241m.\u001b[39mappend(llm_response)\n\u001b[1;32m     55\u001b[0m metadata_batch\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintended_personality\u001b[39m\u001b[38;5;124m\"\u001b[39m: personality_trait,\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: question_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_categorized_response\u001b[39m\u001b[38;5;124m\"\u001b[39m: categorize_opinionqa_response(llm_response, parsed_choices)\n\u001b[1;32m     64\u001b[0m })\n",
      "Cell \u001b[0;32mIn[12], line 65\u001b[0m, in \u001b[0;36mget_peft_llm_response_fixed\u001b[0;34m(personality_trait, question, max_new_tokens, temperature, top_p, do_sample)\u001b[0m\n\u001b[1;32m     55\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m current_model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     56\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_ids,\n\u001b[1;32m     57\u001b[0m                 max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer_for_eval\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[1;32m     63\u001b[0m             )\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_for_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_for_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Decode response\u001b[39;00m\n\u001b[1;32m     76\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer_for_eval\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m     77\u001b[0m     outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(input_ids[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]):], \n\u001b[1;32m     78\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     79\u001b[0m )\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/peft/peft_model.py:1968\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1967\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1968\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1970\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/generation/utils.py:2623\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2615\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2616\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2617\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2618\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2619\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2620\u001b[0m     )\n\u001b[1;32m   2622\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2623\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2634\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2635\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2636\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2637\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2638\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2639\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2640\u001b[0m     )\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/generation/utils.py:3607\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3605\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3607\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3609\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3610\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3611\u001b[0m     outputs,\n\u001b[1;32m   3612\u001b[0m     model_kwargs,\n\u001b[1;32m   3613\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3614\u001b[0m )\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:583\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    580\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    581\u001b[0m )\n\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:470\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    468\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 470\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:289\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    288\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_feedforward_layernorm(hidden_states)\n\u001b[0;32m--> 289\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_feedforward_layernorm(hidden_states)\n\u001b[1;32m    291\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:82\u001b[0m, in \u001b[0;36mGemma2MLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 82\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:502\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cast_input_dtype(x, lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m active_adapter \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_variant:  \u001b[38;5;66;03m# vanilla LoRA\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     output \u001b[38;5;241m=\u001b[39m lora_B(\u001b[43mlora_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m scaling\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m requires_conversion:\n\u001b[1;32m    504\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 5 V2: FIXED OPINIONQA EVALUATION\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Fixed OpinionQA Evaluation ---\")\n",
    "\n",
    "# Load OpinionQA dataset (keeping original code)\n",
    "try:\n",
    "    opinionqa_dataset = load_dataset(\"RiverDong/OpinionQA\", split=\"test\")\n",
    "    print(f\"OpinionQA dataset loaded. Number of samples: {len(opinionqa_dataset)}\")\n",
    "    df_opinionqa_questions = opinionqa_dataset.to_pandas()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading OpinionQA dataset: {e}. Using dummy data.\")\n",
    "    df_opinionqa_questions = pd.DataFrame([\n",
    "        {\"id\": \"op1\", \"prompt\": \"<question>Do you believe social media positively impacts society?</question> <choices>(A): Yes (B): No (C): Neutral</choices>\", \"answer\": \"A\", \"question_id\": \"DUMMY_Q1\"},\n",
    "        {\"id\": \"op2\", \"prompt\": \"<question>Is it important for a leader to prioritize group harmony?</question> <choices>(A): Strongly Agree (B): Disagree</choices>\", \"answer\": \"B\", \"question_id\": \"DUMMY_Q2\"},\n",
    "    ])\n",
    "\n",
    "# Run evaluation for all personalities\n",
    "all_opinionqa_results = []\n",
    "all_personalities = [\"neutral\"] + list(target_personalities)\n",
    "\n",
    "for personality_trait in all_personalities:\n",
    "    print(f\"\\n--- Running OpinionQA for Personality: {personality_trait} ---\")\n",
    "    \n",
    "    # Clear cache before each personality evaluation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    N_SAMPLES = 10  # Reduced for testing\n",
    "    opinionqa_subset = df_opinionqa_questions.sample(\n",
    "        min(len(df_opinionqa_questions), N_SAMPLES), \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    responses_batch = []\n",
    "    metadata_batch = []\n",
    "    \n",
    "    for index, row in opinionqa_subset.iterrows():\n",
    "        full_prompt = row['prompt']\n",
    "        question_id = row['question_id']\n",
    "        true_answer = row['answer']\n",
    "        \n",
    "        # Extract question and choices\n",
    "        question_text, choices_str = extract_question_and_choices(full_prompt)\n",
    "        parsed_choices = parse_choices_string(choices_str)\n",
    "        \n",
    "        # Get LLM response with fixed adapter management\n",
    "        llm_response = get_peft_llm_response_fixed(\n",
    "            personality_trait=personality_trait,\n",
    "            question=f\"Question: {question_text}\\nChoices: {choices_str}\\nAnswer:\",\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        responses_batch.append(llm_response)\n",
    "        metadata_batch.append({\n",
    "            \"intended_personality\": personality_trait,\n",
    "            \"question_id\": question_id,\n",
    "            \"extracted_question\": question_text,\n",
    "            \"choices_raw_str\": choices_str,\n",
    "            \"parsed_choices\": parsed_choices,\n",
    "            \"human_true_answer_label\": true_answer,\n",
    "            \"llm_raw_response\": llm_response,\n",
    "            \"llm_categorized_response\": categorize_opinionqa_response(llm_response, parsed_choices)\n",
    "        })\n",
    "    \n",
    "    # Batch classify responses\n",
    "    if responses_batch:\n",
    "        print(f\"Classifying {len(responses_batch)} responses for {personality_trait}...\")\n",
    "        try:\n",
    "            classifier_results = personality_classifier(responses_batch)\n",
    "            \n",
    "            for i, result in enumerate(classifier_results):\n",
    "                metadata_batch[i][\"predicted_trait\"] = result['label']\n",
    "                metadata_batch[i][\"prediction_confidence\"] = result['score']\n",
    "                all_opinionqa_results.append(metadata_batch[i])\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in personality classification: {e}\")\n",
    "            for metadata in metadata_batch:\n",
    "                metadata[\"predicted_trait\"] = \"classification_error\"\n",
    "                metadata[\"prediction_confidence\"] = 0.0\n",
    "                all_opinionqa_results.append(metadata)\n",
    "    \n",
    "    print(f\"Completed evaluation for {personality_trait}\")\n",
    "\n",
    "# Analysis of results\n",
    "print(\"\\n--- Analysis of OpinionQA Results ---\")\n",
    "if all_opinionqa_results:\n",
    "    df_results = pd.DataFrame(all_opinionqa_results)\n",
    "    \n",
    "    print(f\"Total responses collected: {len(df_results)}\")\n",
    "    print(f\"Personalities evaluated: {df_results['intended_personality'].unique()}\")\n",
    "    \n",
    "    # Trait alignment analysis\n",
    "    non_neutral_results = df_results[df_results['intended_personality'] != 'neutral']\n",
    "    if not non_neutral_results.empty:\n",
    "        correct_predictions = non_neutral_results[\n",
    "            non_neutral_results['intended_personality'] == non_neutral_results['predicted_trait']\n",
    "        ]\n",
    "        trait_alignment_score = len(correct_predictions) / len(non_neutral_results)\n",
    "        print(f\"\\nOverall Trait Alignment Score: {trait_alignment_score:.3f}\")\n",
    "        \n",
    "        # Per-trait analysis\n",
    "        trait_scores = non_neutral_results.groupby('intended_personality').apply(\n",
    "            lambda x: (x['intended_personality'] == x['predicted_trait']).mean()\n",
    "        )\n",
    "        print(\"\\nTrait Alignment per Personality:\")\n",
    "        for trait, score in trait_scores.items():\n",
    "            print(f\"  {trait}: {score:.3f}\")\n",
    "    \n",
    "    # Memory status\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nFinal GPU Memory - Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"Final GPU Memory - Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "        \n",
    "    print(\"\\n--- OpinionQA Evaluation Complete ---\")\n",
    "else:\n",
    "    print(\"No results collected. Please check the evaluation process.\")\n",
    "\n",
    "# Final cleanup\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Final GPU cache cleanup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing OpinionQA Trait Alignment (TA) ---\n",
      "Overall Trait Alignment (TA) Score (excluding 'neutral'): 0.200\n",
      "\n",
      "Trait Alignment (TA) Score per Personality:\n",
      "intended_personality\n",
      "agreeableness        0.0\n",
      "conscientiousness    0.0\n",
      "extraversion         0.0\n",
      "neuroticism          1.0\n",
      "openness             0.0\n",
      "dtype: float64\n",
      "\n",
      "--- OpinionQA Content Analysis (Conceptual for Opinion Alignment) ---\n",
      "Example: Distribution of LLM categorized responses for a question across personalities:\n",
      "LLM responses for question_id: 'WORKHARD_W53'\n",
      "intended_personality  llm_categorized_response_oq\n",
      "agreeableness         Uncategorized                  1.0\n",
      "conscientiousness     Uncategorized                  1.0\n",
      "extraversion          Uncategorized                  1.0\n",
      "neuroticism           Uncategorized                  1.0\n",
      "neutral               Uncategorized                  1.0\n",
      "openness              Uncategorized                  1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "To compare LLM opinions with human responses, you would:\n",
      "1. Map the LLM's 'llm_categorized_response_oq' to its corresponding raw label (e.g., 'Yes' to 'A').\n",
      "2. Compare the distribution of LLM's mapped labels with the 'human_true_answer_label' distribution for each question.\n",
      "\n",
      "--- Misclassification Analysis (LLM Predicted Trait vs. Intended Trait) ---\n",
      "\n",
      "--- Common Misclassifications ---\n",
      "  intended_personality llm_predicted_trait_TA  count\n",
      "0        agreeableness            neuroticism     20\n",
      "1    conscientiousness            neuroticism     20\n",
      "2         extraversion            neuroticism     20\n",
      "3             openness            neuroticism     20\n",
      "\n",
      "--- Examples of Misclassified Responses ---\n",
      "Analyzing misclassifications for: ['extraversion', 'agreeableness', 'openness', 'conscientiousness']\n",
      "\n",
      "Misclassifications for Intended Personality: 'extraversion'\n",
      "      question_id                      llm_raw_response llm_predicted_trait_TA  llm_predicted_trait_TA_confidence\n",
      "20   WORKHARD_W53  ERROR: PEFT model generation failed.            neuroticism                           0.567378\n",
      "21  INCFUTURE_W32  ERROR: PEFT model generation failed.            neuroticism                           0.567378\n",
      "22  GOVRESP_b_W54  ERROR: PEFT model generation failed.            neuroticism                           0.567378\n",
      "\n",
      "Misclassifications for Intended Personality: 'agreeableness'\n",
      "      question_id                      llm_raw_response llm_predicted_trait_TA  llm_predicted_trait_TA_confidence\n",
      "40   WORKHARD_W53  ERROR: PEFT model generation failed.            neuroticism                           0.567378\n",
      "41  INCFUTURE_W32  ERROR: PEFT model generation failed.            neuroticism                           0.567378\n",
      "42  GOVRESP_b_W54  ERROR: PEFT model generation failed.            neuroticism                           0.567378\n",
      "\n",
      "Misclassifications for Intended Personality: 'openness'\n",
      "      question_id                      llm_raw_response llm_predicted_trait_TA  llm_predicted_trait_TA_confidence\n",
      "80   WORKHARD_W53  ERROR: PEFT model generation failed.            neuroticism                           0.567378\n",
      "81  INCFUTURE_W32  ERROR: PEFT model generation failed.            neuroticism                           0.567378\n",
      "82  GOVRESP_b_W54  ERROR: PEFT model generation failed.            neuroticism                           0.567378\n",
      "\n",
      "Misclassifications for Intended Personality: 'conscientiousness'\n",
      "       question_id                      llm_raw_response llm_predicted_trait_TA  llm_predicted_trait_TA_confidence\n",
      "100   WORKHARD_W53  ERROR: PEFT model generation failed.            neuroticism                           0.567378\n",
      "101  INCFUTURE_W32  ERROR: PEFT model generation failed.            neuroticism                           0.567378\n",
      "102  GOVRESP_b_W54  ERROR: PEFT model generation failed.            neuroticism                           0.567378\n",
      "\n",
      "--- Opinion Alignment Analysis (LLM Categorized vs. Human True Answer) ---\n",
      "\n",
      "--- Comparing LLM Opinion Distributions vs. Human True Answers per Question ---\n",
      "\n",
      "--- Question ID: WORKHARD_W53 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "A    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label  UNKNOWN\n",
      "intended_personality            \n",
      "agreeableness                1.0\n",
      "conscientiousness            1.0\n",
      "extraversion                 1.0\n",
      "neuroticism                  1.0\n",
      "neutral                      1.0\n",
      "openness                     1.0\n",
      "\n",
      "--- Question ID: INCFUTURE_W32 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "B    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label    A  UNKNOWN\n",
      "intended_personality                 \n",
      "agreeableness            0.0      1.0\n",
      "conscientiousness        0.0      1.0\n",
      "extraversion             0.0      1.0\n",
      "neuroticism              0.0      1.0\n",
      "neutral                  1.0      0.0\n",
      "openness                 0.0      1.0\n",
      "\n",
      "--- Question ID: GOVRESP_b_W54 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "A    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label  UNKNOWN\n",
      "intended_personality            \n",
      "agreeableness                1.0\n",
      "conscientiousness            1.0\n",
      "extraversion                 1.0\n",
      "neuroticism                  1.0\n",
      "neutral                      1.0\n",
      "openness                     1.0\n",
      "\n",
      "--- Question ID: MED1_W34 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "A    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label    A  UNKNOWN\n",
      "intended_personality                 \n",
      "agreeableness            0.0      1.0\n",
      "conscientiousness        0.0      1.0\n",
      "extraversion             0.0      1.0\n",
      "neuroticism              0.0      1.0\n",
      "neutral                  1.0      0.0\n",
      "openness                 0.0      1.0\n",
      "\n",
      "--- Question ID: POOREASY_W53 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "B    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label    A  UNKNOWN\n",
      "intended_personality                 \n",
      "agreeableness            0.0      1.0\n",
      "conscientiousness        0.0      1.0\n",
      "extraversion             0.0      1.0\n",
      "neuroticism              0.0      1.0\n",
      "neutral                  1.0      0.0\n",
      "openness                 0.0      1.0\n",
      "\n",
      "--- Question ID: BLOODPR_W29 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "B    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label    A  UNKNOWN\n",
      "intended_personality                 \n",
      "agreeableness            0.0      1.0\n",
      "conscientiousness        0.0      1.0\n",
      "extraversion             0.0      1.0\n",
      "neuroticism              0.0      1.0\n",
      "neutral                  1.0      0.0\n",
      "openness                 0.0      1.0\n",
      "\n",
      "--- Question ID: DIFF1E_W29 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "A    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label  UNKNOWN\n",
      "intended_personality            \n",
      "agreeableness                1.0\n",
      "conscientiousness            1.0\n",
      "extraversion                 1.0\n",
      "neuroticism                  1.0\n",
      "neutral                      1.0\n",
      "openness                     1.0\n",
      "\n",
      "--- Question ID: RACESURV37_W43 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "B    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label  UNKNOWN\n",
      "intended_personality            \n",
      "agreeableness                1.0\n",
      "conscientiousness            1.0\n",
      "extraversion                 1.0\n",
      "neuroticism                  1.0\n",
      "neutral                      1.0\n",
      "openness                     1.0\n",
      "\n",
      "--- Question ID: IMMCULT2_W32 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "B    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label  UNKNOWN\n",
      "intended_personality            \n",
      "agreeableness                1.0\n",
      "conscientiousness            1.0\n",
      "extraversion                 1.0\n",
      "neuroticism                  1.0\n",
      "neutral                      1.0\n",
      "openness                     1.0\n",
      "\n",
      "--- Question ID: STYLE1_W36 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "B    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label    A  UNKNOWN\n",
      "intended_personality                 \n",
      "agreeableness            0.0      1.0\n",
      "conscientiousness        0.0      1.0\n",
      "extraversion             0.0      1.0\n",
      "neuroticism              0.0      1.0\n",
      "neutral                  1.0      0.0\n",
      "openness                 0.0      1.0\n",
      "\n",
      "--- Question ID: WATCHDOG_3_W45 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "A    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label  UNKNOWN\n",
      "intended_personality            \n",
      "agreeableness                1.0\n",
      "conscientiousness            1.0\n",
      "extraversion                 1.0\n",
      "neuroticism                  1.0\n",
      "neutral                      1.0\n",
      "openness                     1.0\n",
      "\n",
      "--- Question ID: ROBJOB6_W27 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "B    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label    A  UNKNOWN\n",
      "intended_personality                 \n",
      "agreeableness            0.0      1.0\n",
      "conscientiousness        0.0      1.0\n",
      "extraversion             0.0      1.0\n",
      "neuroticism              0.0      1.0\n",
      "neutral                  1.0      0.0\n",
      "openness                 0.0      1.0\n",
      "\n",
      "--- Question ID: BIOTECHC_W34 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "B    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label    A  UNKNOWN\n",
      "intended_personality                 \n",
      "agreeableness            0.0      1.0\n",
      "conscientiousness        0.0      1.0\n",
      "extraversion             0.0      1.0\n",
      "neuroticism              0.0      1.0\n",
      "neutral                  1.0      0.0\n",
      "openness                 0.0      1.0\n",
      "\n",
      "--- Question ID: INFOOWNe_W45 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "A    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label    A  UNKNOWN\n",
      "intended_personality                 \n",
      "agreeableness            0.0      1.0\n",
      "conscientiousness        0.0      1.0\n",
      "extraversion             0.0      1.0\n",
      "neuroticism              0.0      1.0\n",
      "neutral                  1.0      0.0\n",
      "openness                 0.0      1.0\n",
      "\n",
      "--- Question ID: DEBTa_W54 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "B    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label  UNKNOWN\n",
      "intended_personality            \n",
      "agreeableness                1.0\n",
      "conscientiousness            1.0\n",
      "extraversion                 1.0\n",
      "neuroticism                  1.0\n",
      "neutral                      1.0\n",
      "openness                     1.0\n",
      "\n",
      "--- Question ID: MADEUPSHARE2_W45 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "B    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label    A  UNKNOWN\n",
      "intended_personality                 \n",
      "agreeableness            0.0      1.0\n",
      "conscientiousness        0.0      1.0\n",
      "extraversion             0.0      1.0\n",
      "neuroticism              0.0      1.0\n",
      "neutral                  1.0      0.0\n",
      "openness                 0.0      1.0\n",
      "\n",
      "--- Question ID: DNATEST_W50 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "B    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label  UNKNOWN\n",
      "intended_personality            \n",
      "agreeableness                1.0\n",
      "conscientiousness            1.0\n",
      "extraversion                 1.0\n",
      "neuroticism                  1.0\n",
      "neutral                      1.0\n",
      "openness                     1.0\n",
      "\n",
      "--- Question ID: COMPROMISEVAL_W92 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "B    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label    A  UNKNOWN\n",
      "intended_personality                 \n",
      "agreeableness            0.0      1.0\n",
      "conscientiousness        0.0      1.0\n",
      "extraversion             0.0      1.0\n",
      "neuroticism              0.0      1.0\n",
      "neutral                  1.0      0.0\n",
      "openness                 0.0      1.0\n",
      "\n",
      "--- Question ID: INFOOWNb_W45 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "B    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label  UNKNOWN\n",
      "intended_personality            \n",
      "agreeableness                1.0\n",
      "conscientiousness            1.0\n",
      "extraversion                 1.0\n",
      "neuroticism                  1.0\n",
      "neutral                      1.0\n",
      "openness                     1.0\n",
      "\n",
      "--- Question ID: SOCMEDIAUSEd_W49 ---\n",
      "\n",
      "Human True Answer Distribution:\n",
      "human_true_answer_label\n",
      "B    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "LLM Mapped Answer Distribution per Intended Personality:\n",
      "llm_mapped_answer_label    A  UNKNOWN\n",
      "intended_personality                 \n",
      "agreeableness            0.0      1.0\n",
      "conscientiousness        0.0      1.0\n",
      "extraversion             0.0      1.0\n",
      "neuroticism              0.0      1.0\n",
      "neutral                  1.0      0.0\n",
      "openness                 0.0      1.0\n",
      "\n",
      "--- OpinionQA Analysis Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects3/aisd/2024/ghanda/tmp/ipykernel_2068111/3482477945.py:10: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ta_per_trait = df_ta_analysis.groupby('intended_personality').apply(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Analyzing OpinionQA Trait Alignment (TA) ---\")\n",
    "if not df_opinionqa_results.empty:\n",
    "    df_ta_analysis = df_opinionqa_results[df_opinionqa_results['intended_personality'] != 'neutral'].copy()\n",
    "\n",
    "    if not df_ta_analysis.empty:\n",
    "        correct_ta_predictions = df_ta_analysis[df_ta_analysis['intended_personality'] == df_ta_analysis['llm_predicted_trait_TA']]\n",
    "        overall_ta_score = len(correct_ta_predictions) / len(df_ta_analysis)\n",
    "        print(f\"Overall Trait Alignment (TA) Score (excluding 'neutral'): {overall_ta_score:.3f}\")\n",
    "\n",
    "        ta_per_trait = df_ta_analysis.groupby('intended_personality').apply(\n",
    "            lambda x: (x['intended_personality'] == x['llm_predicted_trait_TA']).mean()\n",
    "        )\n",
    "        print(\"\\nTrait Alignment (TA) Score per Personality:\")\n",
    "        print(ta_per_trait)\n",
    "    else:\n",
    "        print(\"No non-neutral results to analyze for Trait Alignment.\")\n",
    "\n",
    "    print(\"\\n--- OpinionQA Content Analysis (Conceptual for Opinion Alignment) ---\")\n",
    "    print(\"Example: Distribution of LLM categorized responses for a question across personalities:\")\n",
    "    first_q_id = df_opinionqa_results['question_id'].iloc[0] if not df_opinionqa_results.empty else \"N/A\"\n",
    "    print(f\"LLM responses for question_id: '{first_q_id}'\")\n",
    "    print(df_opinionqa_results[df_opinionqa_results['question_id'] == first_q_id].groupby('intended_personality')['llm_categorized_response_oq'].value_counts(normalize=True))\n",
    "    \n",
    "    print(\"\\nTo compare LLM opinions with human responses, you would:\")\n",
    "    print(\"1. Map the LLM's 'llm_categorized_response_oq' to its corresponding raw label (e.g., 'Yes' to 'A').\")\n",
    "    print(\"2. Compare the distribution of LLM's mapped labels with the 'human_true_answer_label' distribution for each question.\")\n",
    "else:\n",
    "    print(\"No OpinionQA results to analyze. Please ensure the experiment runs and gathers data.\")\n",
    "\n",
    "print(\"\\n--- Misclassification Analysis (LLM Predicted Trait vs. Intended Trait) ---\")\n",
    "if 'df_ta_analysis' in locals() and not df_ta_analysis.empty:\n",
    "    misclassified_df = df_ta_analysis[\n",
    "        df_ta_analysis['intended_personality'] != df_ta_analysis['llm_predicted_trait_TA']\n",
    "    ].copy()\n",
    "\n",
    "    if not misclassified_df.empty:\n",
    "        print(\"\\n--- Common Misclassifications ---\")\n",
    "        misclassification_summary = misclassified_df.groupby(['intended_personality', 'llm_predicted_trait_TA']).size().reset_index(name='count')\n",
    "        print(misclassification_summary.sort_values(by='count', ascending=False))\n",
    "\n",
    "        print(\"\\n--- Examples of Misclassified Responses ---\")\n",
    "        traits_to_inspect = misclassified_df['intended_personality'].unique().tolist()\n",
    "        \n",
    "        print(f\"Analyzing misclassifications for: {traits_to_inspect}\")\n",
    "\n",
    "        for trait in traits_to_inspect:\n",
    "            print(f\"\\nMisclassifications for Intended Personality: '{trait}'\")\n",
    "            trait_misclassifications = misclassified_df[misclassified_df['intended_personality'] == trait]\n",
    "            \n",
    "            if not trait_misclassifications.empty:\n",
    "                print(trait_misclassifications[['question_id', 'llm_raw_response', 'llm_predicted_trait_TA', 'llm_predicted_trait_TA_confidence']].head(3).to_string())\n",
    "            else:\n",
    "                print(f\"No misclassifications found for intended personality '{trait}'.\")\n",
    "    else:\n",
    "        print(\"Excellent! No misclassifications found in the non-neutral data.\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available for misclassification analysis (df_ta_analysis not found or is empty).\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Opinion Alignment Analysis (LLM Categorized vs. Human True Answer) ---\")\n",
    "\n",
    "df_opinionqa_results['llm_mapped_answer_label'] = df_opinionqa_results.apply(\n",
    "    lambda row: map_categorized_to_label(row['llm_categorized_response_oq'], row['choices_raw_str']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "unique_question_ids = df_opinionqa_results['question_id'].unique()\n",
    "\n",
    "print(\"\\n--- Comparing LLM Opinion Distributions vs. Human True Answers per Question ---\")\n",
    "for q_id in unique_question_ids:\n",
    "    print(f\"\\n--- Question ID: {q_id} ---\")\n",
    "    question_data = df_opinionqa_results[df_opinionqa_results['question_id'] == q_id]\n",
    "\n",
    "    if question_data.empty:\n",
    "        print(\"No data for this question ID in the results.\")\n",
    "        continue\n",
    "\n",
    "    print(\"\\nHuman True Answer Distribution:\")\n",
    "    human_dist = question_data['human_true_answer_label'].value_counts(normalize=True).sort_index()\n",
    "    print(human_dist)\n",
    "\n",
    "    print(\"\\nLLM Mapped Answer Distribution per Intended Personality:\")\n",
    "    llm_dist_per_personality = question_data.groupby('intended_personality')['llm_mapped_answer_label'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    print(llm_dist_per_personality)\n",
    "\n",
    "print(\"\\n--- OpinionQA Analysis Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

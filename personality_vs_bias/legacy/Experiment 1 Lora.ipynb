{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded.\n",
      "Successfully logged into Hugging Face.\n",
      "Hugging Face cache directory set to: /cs/student/projects3/aisd/2024/ghanda/cache\n",
      "\n",
      "--- PART 1: Loading Base Model and Tokenizer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [02:03<00:00, 41.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded successfully.\n",
      "Tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "print(\"Environment variables loaded.\")\n",
    "\n",
    "# Hugging Face login\n",
    "try:\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"Successfully logged into Hugging Face.\")\n",
    "    else:\n",
    "        print(\"Hugging Face token not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not log into Hugging Face: {e}\")\n",
    "\n",
    "# Cache directory\n",
    "HF_CACHE_DIR = \"/cs/student/projects3/aisd/2024/ghanda/cache\"\n",
    "os.environ[\"HF_HOME\"] = HF_CACHE_DIR\n",
    "os.makedirs(HF_CACHE_DIR, exist_ok=True)\n",
    "print(f\"Hugging Face cache directory set to: {HF_CACHE_DIR}\")\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"google/gemma-2-2b\"\n",
    "FINE_TUNED_MODEL_BASE_NAME = \"gemma-2b-personality-peft\"\n",
    "\n",
    "# QLoRA parameters\n",
    "lora_r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# Quantization parameters\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "\n",
    "# Training parameters\n",
    "OUTPUT_DIR_BASE = \"peft_output_models\"\n",
    "num_train_epochs = 2\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 8\n",
    "gradient_checkpointing = True\n",
    "max_grad_norm = 0.3\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.001\n",
    "optim = \"paged_adamw_8bit\"\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_steps = -1\n",
    "warmup_ratio = 0.03\n",
    "group_by_length = True\n",
    "save_steps = 0\n",
    "logging_steps = 5\n",
    "max_seq_length = 512\n",
    "packing = False\n",
    "\n",
    "# Device mapping\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Dataset paths\n",
    "PERSONALITY_DATASET_NAME = \"holistic-ai/personality_manipulation\"\n",
    "BBQ_DATA_FILE = \"/cs/student/projects3/aisd/2024/ghanda/bbq_ambiguous_with_metadata.csv\"\n",
    "\n",
    "# ==============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def create_training_prompt_format(sample):\n",
    "    \"\"\"Format sample for training.\"\"\"\n",
    "    sample[\"text\"] = f\"<s>[INST] {sample['Question']} [/INST]{sample['Answer']}</s>\"\n",
    "    return sample\n",
    "\n",
    "def create_inference_prompt_format(question, tokenizer):\n",
    "    \"\"\"Format question for inference.\"\"\"\n",
    "    return tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": question}],\n",
    "        tokenize=False,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "def extract_question_and_choices(full_prompt_string):\n",
    "    \"\"\"Extract question and choices from OpinionQA prompt.\"\"\"\n",
    "    question_match = re.search(r'<question>(.*?)</question>', full_prompt_string, re.DOTALL)\n",
    "    choices_match = re.search(r'<choices>(.*?)</choices>', full_prompt_string, re.DOTALL)\n",
    "    question_text = question_match.group(1).strip() if question_match else \"\"\n",
    "    choices_raw_str = choices_match.group(1).strip() if choices_match else \"\"\n",
    "    return question_text, choices_raw_str\n",
    "\n",
    "def parse_choices_string(choices_str):\n",
    "    \"\"\"Parse choices string into list.\"\"\"\n",
    "    parsed_choices = []\n",
    "    matches = re.findall(r'\\([A-Z]\\):\\s*(.*?)(?=\\s*\\([A-Z]\\):\\s*|$)', choices_str)\n",
    "    for match in matches:\n",
    "        parsed_choices.append(match.strip())\n",
    "    return parsed_choices if parsed_choices else [choices_str]\n",
    "\n",
    "def categorize_opinionqa_response(raw_response, choices_list):\n",
    "    \"\"\"Categorize OpinionQA response.\"\"\"\n",
    "    raw_response_lower = raw_response.lower()\n",
    "    \n",
    "    # Direct choice matching\n",
    "    for choice in choices_list:\n",
    "        if re.search(r'\\b' + re.escape(choice.lower()) + r'\\b', raw_response_lower):\n",
    "            return choice\n",
    "    \n",
    "    # Keyword matching\n",
    "    if any(word in raw_response_lower for word in [\"yes\", \"agree\", \"positive\", \"positively\"]):\n",
    "        if \"Yes\" in choices_list: return \"Yes\"\n",
    "        if \"Agree\" in choices_list: return \"Agree\"\n",
    "        if \"Strongly Agree\" in choices_list: return \"Strongly Agree\"\n",
    "    \n",
    "    if any(word in raw_response_lower for word in [\"no\", \"disagree\", \"negative\", \"negatively\"]):\n",
    "        if \"No\" in choices_list: return \"No\"\n",
    "        if \"Disagree\" in choices_list: return \"Disagree\"\n",
    "        if \"Strongly Disagree\" in choices_list: return \"Strongly Disagree\"\n",
    "    \n",
    "    if any(word in raw_response_lower for word in [\"neutral\", \"balanced\", \"both\", \"neither\"]):\n",
    "        if \"Neutral\" in choices_list: return \"Neutral\"\n",
    "        if \"Seek Balance\" in choices_list: return \"Seek Balance\"\n",
    "        if \"Uncategorized\" in choices_list: return \"Uncategorized\"\n",
    "    \n",
    "    return \"Uncategorized\"\n",
    "\n",
    "def map_categorized_to_label(categorized_text, choices_raw_str):\n",
    "    \"\"\"Map categorized response to letter label.\"\"\"\n",
    "    if not isinstance(choices_raw_str, str):\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "    matches = re.findall(r'\\((\\w)\\):\\s*(.*?)(?=\\s*\\([A-Z]\\):|$)', choices_raw_str)\n",
    "    for letter, choice_text in matches:\n",
    "        if categorized_text.lower() == choice_text.strip().lower():\n",
    "            return letter.upper()\n",
    "    \n",
    "    return 'UNKNOWN'\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: LOAD BASE MODEL AND TOKENIZER\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- PART 1: Loading Base Model and Tokenizer ---\")\n",
    "\n",
    "# Setup quantization\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "try:\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=compute_dtype,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    base_model.config.use_cache = False\n",
    "    base_model.config.pretraining_tp = 1\n",
    "    base_model.eval()\n",
    "    print(\"Base model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL: Could not load base model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.model_max_length = max_seq_length\n",
    "    \n",
    "    # Set chat template for Gemma-2B\n",
    "    tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] }}{% endif %}{% endfor %}\"\n",
    "    \n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL: Could not load tokenizer: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 2: Loading Personality Data ---\n",
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n",
      "Loaded personality dataset. Number of samples: 4000\n",
      "Target personalities: ['extraversion', 'agreeableness', 'neuroticism', 'openness', 'conscientiousness']\n",
      "\n",
      "--- PART 3: Fine-tuning Check ---\n",
      "All PEFT adapters found. Skipping fine-tuning.\n",
      "\n",
      "--- PART 4: Loading PEFT Models for Evaluation ---\n",
      "Created PEFT model with first adapter: extraversion\n",
      "Loaded additional adapter: agreeableness\n",
      "Loaded additional adapter: neuroticism\n",
      "Loaded additional adapter: openness\n",
      "Loaded additional adapter: conscientiousness\n",
      "Successfully loaded 5 adapters: ['extraversion', 'agreeableness', 'neuroticism', 'openness', 'conscientiousness']\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 2: LOAD PERSONALITY DATA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- PART 2: Loading Personality Data ---\")\n",
    "\n",
    "try:\n",
    "    personality_dataset_hf_train = load_dataset(PERSONALITY_DATASET_NAME, split='train')\n",
    "    df_personality_raw = personality_dataset_hf_train.to_pandas()\n",
    "    print(f\"Loaded personality dataset. Number of samples: {len(df_personality_raw)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading personality dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "target_personalities = df_personality_raw['Target Personality'].unique().tolist()\n",
    "print(f\"Target personalities: {target_personalities}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: FINE-TUNING (SKIP IF MODELS EXIST)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- PART 3: Fine-tuning Check ---\")\n",
    "\n",
    "# Check if all adapters exist\n",
    "all_adapters_exist = True\n",
    "for trait in target_personalities:\n",
    "    adapter_path = os.path.join(OUTPUT_DIR_BASE, trait.lower().replace(\" \", \"_\"))\n",
    "    if not os.path.exists(adapter_path):\n",
    "        all_adapters_exist = False\n",
    "        break\n",
    "\n",
    "if all_adapters_exist:\n",
    "    print(\"All PEFT adapters found. Skipping fine-tuning.\")\n",
    "else:\n",
    "    print(\"Starting fine-tuning process...\")\n",
    "    \n",
    "    logging.set_verbosity_warning()\n",
    "    \n",
    "    for current_trait in target_personalities:\n",
    "        print(f\"\\n***** FINE-TUNING FOR PERSONALITY: {current_trait.upper()} *****\")\n",
    "        \n",
    "        # Filter data for current trait\n",
    "        filtered_df = df_personality_raw[df_personality_raw['Target Personality'] == current_trait].copy()\n",
    "        if filtered_df.empty:\n",
    "            print(f\"No data found for personality '{current_trait}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare dataset\n",
    "        train_dataset = Dataset.from_pandas(filtered_df).map(create_training_prompt_format)\n",
    "        train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if col != 'text'])\n",
    "        \n",
    "        print(f\"Prepared {len(train_dataset)} samples for '{current_trait}'\")\n",
    "        \n",
    "        # Output directory\n",
    "        current_output_dir = os.path.join(OUTPUT_DIR_BASE, current_trait.lower().replace(\" \", \"_\"))\n",
    "        os.makedirs(current_output_dir, exist_ok=True)\n",
    "        \n",
    "        # PEFT config\n",
    "        peft_config = LoraConfig(\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            r=lora_r,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=[\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = SFTConfig(\n",
    "            output_dir=current_output_dir,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            optim=optim,\n",
    "            save_steps=save_steps,\n",
    "            logging_steps=logging_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            fp16=fp16,\n",
    "            bf16=bf16,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            max_steps=max_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            group_by_length=group_by_length,\n",
    "            lr_scheduler_type=lr_scheduler_type,\n",
    "            report_to=\"tensorboard\",\n",
    "            logging_dir=f\"{current_output_dir}/logs\",\n",
    "            remove_unused_columns=False,\n",
    "            gradient_checkpointing=gradient_checkpointing,\n",
    "            max_length=max_seq_length,\n",
    "            packing=packing,\n",
    "            dataset_text_field=\"text\",\n",
    "        )\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = SFTTrainer(\n",
    "            model=base_model,\n",
    "            train_dataset=train_dataset,\n",
    "            peft_config=peft_config,\n",
    "            args=training_args,\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        print(f\"Training PEFT model for {current_trait}...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save\n",
    "        print(f\"Saving PEFT adapter for {current_trait}...\")\n",
    "        trainer.model.save_pretrained(current_output_dir)\n",
    "        \n",
    "        # Cleanup\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Finished training for {current_trait}\")\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR_BASE, \"tokenizer\"))\n",
    "    print(\"Fine-tuning completed.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: LOAD PEFT MODELS FOR EVALUATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- PART 4: Loading PEFT Models for Evaluation ---\")\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load adapters\n",
    "peft_model = None\n",
    "loaded_adapters = {}\n",
    "\n",
    "for i, trait in enumerate(target_personalities):\n",
    "    adapter_name = trait.lower().replace(\" \", \"_\")\n",
    "    adapter_path = os.path.join(OUTPUT_DIR_BASE, adapter_name)\n",
    "    \n",
    "    if os.path.exists(adapter_path):\n",
    "        try:\n",
    "            if i == 0:\n",
    "                # Load first adapter to create PEFT model\n",
    "                peft_model = PeftModel.from_pretrained(\n",
    "                    base_model,\n",
    "                    adapter_path,\n",
    "                    adapter_name=adapter_name,\n",
    "                    torch_dtype=compute_dtype\n",
    "                )\n",
    "                print(f\"Created PEFT model with first adapter: {adapter_name}\")\n",
    "            else:\n",
    "                # Load additional adapters\n",
    "                peft_model.load_adapter(adapter_path, adapter_name=adapter_name)\n",
    "                print(f\"Loaded additional adapter: {adapter_name}\")\n",
    "            \n",
    "            loaded_adapters[trait] = adapter_name\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading adapter for {trait}: {e}\")\n",
    "    else:\n",
    "        print(f\"Adapter not found for {trait} at {adapter_path}\")\n",
    "\n",
    "print(f\"Successfully loaded {len(loaded_adapters)} adapters: {list(loaded_adapters.keys())}\")\n",
    "\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 5: Loading Personality Classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality classifier loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 5: PERSONALITY CLASSIFIER\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- PART 5: Loading Personality Classifier ---\")\n",
    "\n",
    "try:\n",
    "    personality_classifier = pipeline(\"text-classification\", model=\"holistic-ai/personality_classifier\")\n",
    "    print(\"Personality classifier loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading personality classifier: {e}\")\n",
    "    personality_classifier = lambda text: [{'label': 'unknown', 'score': 0.0}]\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 6: INFERENCE FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def get_personality_response(personality_trait, question, max_new_tokens=150, temperature=0.7):\n",
    "    \"\"\"Get response from a personality-tuned model or the base model.\"\"\"\n",
    "\n",
    "    # Determine which model to use and its status\n",
    "    model_to_use = base_model  # Default to the original base model\n",
    "    status = \"neutral (base model)\"\n",
    "\n",
    "    # Check if a specific, loaded adapter should be used\n",
    "    if personality_trait != \"neutral\":\n",
    "        if peft_model is not None and personality_trait in loaded_adapters:\n",
    "            # A valid, specific adapter is requested and available\n",
    "            adapter_name = loaded_adapters[personality_trait]\n",
    "            peft_model.set_adapter(adapter_name)\n",
    "            model_to_use = peft_model  # Switch to the PEFT model\n",
    "            status = f\"adapter: {adapter_name}\"\n",
    "        else:\n",
    "            # The requested adapter is not available, fall back to base model\n",
    "            status = f\"fallback to base model (adapter for '{personality_trait}' not found)\"\n",
    "            print(f\"Warning: Adapter for '{personality_trait}' not found. Using base model.\")\n",
    "\n",
    "    print(f\"Generating response for {personality_trait} ({status})\")\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = create_inference_prompt_format(question, tokenizer)\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_to_use.device)\n",
    "\n",
    "    try:\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model_to_use.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode\n",
    "        response = tokenizer.decode(\n",
    "            outputs[0][len(inputs[\"input_ids\"][0]):],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        return response.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"ERROR: Generation failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer_label(raw_response, choices_str):\n",
    "    \"\"\"\n",
    "    REVISED v2: A \"best effort\" parser that combines explicit label searching\n",
    "    with keyword-based content analysis as a fallback.\n",
    "    \"\"\"\n",
    "    # --- Strategy 1: Look for the explicit letter (A:, (B), etc.). This is still the best case. ---\n",
    "    match = re.search(r'^\\s*\\W*([A-Z])[:\\)]', raw_response, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    match = re.search(r'\\(([A-Z])\\)', raw_response, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "\n",
    "    # --- Strategy 2 (NEW FALLBACK): If no explicit label, analyze the content. ---\n",
    "    # This specifically addresses the \"Conversationalist\" failure mode.\n",
    "    response_lower = raw_response.lower()\n",
    "    \n",
    "    # Check for refusal phrases first. If found, we know it's a failure.\n",
    "    if any(phrase in response_lower for phrase in [\"i cannot\", \"i am not able\", \"i do not have opinions\", \"as an ai\"]):\n",
    "        return \"REFUSAL\" # A new category for analysis\n",
    "\n",
    "    # Parse choices into a dictionary: {'A': 'yes...', 'B': 'no...'}\n",
    "    parsed_choices = {\n",
    "        letter: choice_text.strip().lower()\n",
    "        for letter, choice_text in re.findall(r'\\(([A-Z])\\):\\s*(.*?)(?=\\s*\\([A-Z]\\):|$)', choices_str)\n",
    "    }\n",
    "\n",
    "    # Create a list of potential matches based on keywords\n",
    "    possible_matches = []\n",
    "    for letter, choice_text in parsed_choices.items():\n",
    "        # A simple keyword match: if a significant word from the choice is in the response.\n",
    "        # This is a heuristic. We split the choice text and check for non-trivial words.\n",
    "        choice_keywords = [word for word in choice_text.split() if len(word) > 3]\n",
    "        if any(keyword in response_lower for keyword in choice_keywords):\n",
    "            possible_matches.append(letter)\n",
    "            \n",
    "    # If our keyword search finds exactly ONE likely candidate, we'll trust it.\n",
    "    if len(possible_matches) == 1:\n",
    "        return possible_matches[0]\n",
    "\n",
    "    return \"UNKNOWN\" # If all strategies fail, it's truly unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(personality_trait, question_with_choices, max_new_tokens=60, temperature=0.2):\n",
    "    \"\"\"\n",
    "    REVISED v2: A more forceful prompt to combat conversational and refusal responses.\n",
    "    \"\"\"\n",
    "    model_to_use = None\n",
    "    if personality_trait == \"neutral\":\n",
    "        model_to_use = peft_model.model\n",
    "    else:\n",
    "        if personality_trait in loaded_adapters:\n",
    "            adapter_name = loaded_adapters[personality_trait]\n",
    "            peft_model.set_adapter(adapter_name)\n",
    "            model_to_use = peft_model\n",
    "        else:\n",
    "            print(f\"Warning: Adapter for '{personality_trait}' not found. Using base model.\")\n",
    "            model_to_use = peft_model.model\n",
    "\n",
    "    # --- MORE FORCEFUL & CLEARER PROMPT ---\n",
    "    # We add a negative constraint (\"Do NOT start with...\") and a positive one (\"Your response MUST start with...\").\n",
    "    prompt = (\n",
    "        f\"<s>[INST] You are an AI assistant who MUST answer the following multiple-choice question. \"\n",
    "        f\"You will adopt the personality of: **{personality_trait}**. \"\n",
    "        f\"Read the question and choices, and then form an opinion.\\n\\n\"\n",
    "        f\"**CRITICAL INSTRUCTIONS:**\\n\"\n",
    "        f\"1. You MUST choose an option. Do not refuse to answer.\\n\"\n",
    "        f\"2. Your response MUST begin with the letter of your choice, followed by a colon (e.g., 'A:', 'B:').\\n\"\n",
    "        f\"3. Do NOT start your response with phrases like 'I believe', 'As an AI', or 'I apologize'.\\n\\n\"\n",
    "        f\"**Question and Choices:**\\n{question_with_choices} [/INST]\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_to_use.device)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model_to_use.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during model generation for '{personality_trait}': {e}\")\n",
    "        return f\"ERROR: Generation failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 4 & 5: FINAL EVALUATION & ANALYSIS ---\n",
      "OpinionQA dataset loaded with 294714 samples.\n",
      "\n",
      "--- Inspecting first row of OpinionQA data to check format ---\n",
      "prompt               <persona>\\nRacially, the person is refused. Th...\n",
      "answer                                                               B\n",
      "uid                                     American_Trends_Panel_W92_6823\n",
      "folder                                       American_Trends_Panel_W92\n",
      "question_id                                              BIGHOUSES_W92\n",
      "__index_level_0__                                               460290\n",
      "Name: 0, dtype: object\n",
      "----------------------------------------------------------\n",
      "\n",
      "--- Running OpinionQA for Personality: neutral ---\n",
      "\n",
      "--- Running OpinionQA for Personality: extraversion ---\n",
      "\n",
      "--- Running OpinionQA for Personality: agreeableness ---\n",
      "\n",
      "--- Running OpinionQA for Personality: neuroticism ---\n",
      "\n",
      "--- Running OpinionQA for Personality: openness ---\n",
      "\n",
      "--- Running OpinionQA for Personality: conscientiousness ---\n",
      "\n",
      "--- ANALYSIS OF RESULTS ---\n",
      "Successfully generated 600 total responses for analysis.\n",
      "\n",
      "Parsing Success Rate: 9.67% of answers were successfully parsed.\n",
      "Could not determine answer for 542 responses.\n",
      "\n",
      "Overall Opinion Accuracy (on parsed answers): 0.310\n",
      "\n",
      "Opinion Accuracy per Personality:\n",
      "intended_personality\n",
      "agreeableness        0.307692\n",
      "conscientiousness    0.222222\n",
      "extraversion         0.571429\n",
      "neuroticism          1.000000\n",
      "neutral              0.285714\n",
      "openness             0.200000\n",
      "\n",
      "Overall Trait Alignment (Classifier) Score: 0.474\n",
      "\n",
      "Trait Alignment per Personality:\n",
      "intended_personality\n",
      "agreeableness        0.88\n",
      "conscientiousness    0.29\n",
      "extraversion         0.16\n",
      "neuroticism          0.87\n",
      "openness             0.17\n",
      "\n",
      "Full results saved to 'final_corrected_evaluation_results.csv'\n",
      "\n",
      "--- Evaluation and Analysis Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects3/aisd/2024/ghanda/tmp/ipykernel_2597572/3739471576.py:120: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ta_scores = df_ta.groupby('intended_personality').apply(lambda x: (x['predicted_trait'] == x['intended_personality']).mean())\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 4 & 5: FINAL EVALUATION & ANALYSIS (with Robustness Checks)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PART 4 & 5: FINAL EVALUATION & ANALYSIS ---\")\n",
    "\n",
    "# Load Dataset\n",
    "try:\n",
    "    df_opinionqa = load_dataset(\"RiverDong/OpinionQA\", split=\"test\").to_pandas()\n",
    "    print(f\"OpinionQA dataset loaded with {len(df_opinionqa)} samples.\")\n",
    "    # *** DEBUGGING: Inspect the first row of the loaded data ***\n",
    "    if not df_opinionqa.empty:\n",
    "        print(\"\\n--- Inspecting first row of OpinionQA data to check format ---\")\n",
    "        print(df_opinionqa.head(1).iloc[0])\n",
    "        print(\"----------------------------------------------------------\")\n",
    "    else:\n",
    "        print(\"Warning: OpinionQA DataFrame is empty after loading.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading OpinionQA: {e}. Using a dummy dataset.\")\n",
    "    df_opinionqa = pd.DataFrame([\n",
    "        {\"prompt\": \"<question>Is social media good?</question> <choices>(A): Yes (B): No</choices>\", \"question_id\": \"D1\", \"answer\": \"A\"},\n",
    "    ])\n",
    "\n",
    "# --- Run Evaluation Loop ---\n",
    "all_results = []\n",
    "all_eval_personalities = [\"neutral\"] + list(loaded_adapters.keys())\n",
    "N_SAMPLES_PER_PERSONALITY = 100 # Adjust as needed\n",
    "\n",
    "for personality_trait in all_eval_personalities:\n",
    "    print(f\"\\n--- Running OpinionQA for Personality: {personality_trait} ---\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if df_opinionqa.empty:\n",
    "        print(\"Skipping personality, OpinionQA data is empty.\")\n",
    "        continue\n",
    "        \n",
    "    opinionqa_subset = df_opinionqa.sample(n=min(len(df_opinionqa), N_SAMPLES_PER_PERSONALITY), random_state=42)\n",
    "    \n",
    "    responses_for_classifier = []\n",
    "    metadata_for_analysis = []\n",
    "    \n",
    "    for i, row in opinionqa_subset.iterrows():\n",
    "        question, choices_str = extract_question_and_choices(row['prompt'])\n",
    "        \n",
    "        # *** DEBUGGING: Check if parsing is failing ***\n",
    "        if not question or not choices_str:\n",
    "            if i < 5: # Print for the first 5 failures to avoid spamming the log\n",
    "                print(f\"DEBUG: Failed to parse question/choices for row index {i}.\")\n",
    "                print(f\"       Prompt content: {row['prompt']}\")\n",
    "            continue # Skip this row if parsing fails\n",
    "\n",
    "        question_with_choices = f\"Question: {question}\\nChoices: {choices_str}\"\n",
    "        llm_response = get_llm_response(\n",
    "            personality_trait=personality_trait,\n",
    "            question_with_choices=question_with_choices\n",
    "        )\n",
    "        \n",
    "        responses_for_classifier.append(llm_response)\n",
    "        metadata_for_analysis.append({\n",
    "            \"intended_personality\": personality_trait,\n",
    "            \"question_id\": row['question_id'],\n",
    "            \"question\": question,\n",
    "            \"choices_str\": choices_str,\n",
    "            \"human_answer_label\": row['answer'],\n",
    "            \"llm_raw_response\": llm_response,\n",
    "        })\n",
    "    \n",
    "    if personality_classifier and responses_for_classifier:\n",
    "        classifier_results = personality_classifier(responses_for_classifier)\n",
    "        for i, result in enumerate(classifier_results):\n",
    "            metadata_for_analysis[i][\"predicted_trait\"] = result['label']\n",
    "            metadata_for_analysis[i][\"prediction_confidence\"] = result['score']\n",
    "            \n",
    "    all_results.extend(metadata_for_analysis)\n",
    "\n",
    "# --- Analysis Section with Robustness Check ---\n",
    "print(\"\\n--- ANALYSIS OF RESULTS ---\")\n",
    "\n",
    "# *** KEY FIX: Check if the results list is empty BEFORE creating a DataFrame ***\n",
    "if not all_results:\n",
    "    print(\"FATAL ERROR: No results were generated during the evaluation loop.\")\n",
    "    print(\"This is likely because the question/choice parsing failed for all rows in the OpinionQA dataset.\")\n",
    "    print(\"Please check the 'DEBUG: Failed to parse...' messages above to diagnose the issue with the prompt format.\")\n",
    "else:\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    print(f\"Successfully generated {len(df_results)} total responses for analysis.\")\n",
    "\n",
    "    # Apply the robust parser\n",
    "    df_results['llm_mapped_answer_label'] = df_results.apply(\n",
    "        lambda row: extract_final_answer_label(row['llm_raw_response'], row['choices_str']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Metric 1: Parsing Success & Opinion Accuracy\n",
    "    unknown_count = (df_results['llm_mapped_answer_label'] == 'UNKNOWN').sum()\n",
    "    total_responses = len(df_results)\n",
    "    parsing_success_rate = ((total_responses - unknown_count) / total_responses) if total_responses > 0 else 0\n",
    "\n",
    "    print(f\"\\nParsing Success Rate: {parsing_success_rate:.2%} of answers were successfully parsed.\")\n",
    "    if unknown_count > 0:\n",
    "        print(f\"Could not determine answer for {unknown_count} responses.\")\n",
    "\n",
    "    # Calculate accuracy only on parsed answers for a fair evaluation\n",
    "    parsed_df = df_results[df_results['llm_mapped_answer_label'] != 'UNKNOWN'].copy()\n",
    "    if not parsed_df.empty:\n",
    "        parsed_df['is_opinion_correct'] = (parsed_df['llm_mapped_answer_label'] == parsed_df['human_answer_label'])\n",
    "        overall_opinion_accuracy = parsed_df['is_opinion_correct'].mean()\n",
    "        print(f\"\\nOverall Opinion Accuracy (on parsed answers): {overall_opinion_accuracy:.3f}\")\n",
    "        print(\"\\nOpinion Accuracy per Personality:\")\n",
    "        print(parsed_df.groupby('intended_personality')['is_opinion_correct'].mean().to_string())\n",
    "    else:\n",
    "        print(\"\\nNo answers were successfully parsed. Cannot calculate opinion accuracy.\")\n",
    "\n",
    "    # Metric 2: Trait Alignment (Classifier Correctness)\n",
    "    if \"predicted_trait\" in df_results.columns:\n",
    "        df_ta = df_results[df_results['intended_personality'] != 'neutral']\n",
    "        if not df_ta.empty:\n",
    "            overall_ta_score = (df_ta['intended_personality'] == df_ta['predicted_trait']).mean()\n",
    "            print(f\"\\nOverall Trait Alignment (Classifier) Score: {overall_ta_score:.3f}\")\n",
    "            print(\"\\nTrait Alignment per Personality:\")\n",
    "            ta_scores = df_ta.groupby('intended_personality').apply(lambda x: (x['predicted_trait'] == x['intended_personality']).mean())\n",
    "            print(ta_scores.to_string())\n",
    "        else:\n",
    "            print(\"\\nNo non-neutral data to analyze for Trait Alignment.\")\n",
    "    else:\n",
    "        print(\"\\nTrait Alignment analysis skipped (classifier results not found).\")\n",
    "\n",
    "    # --- Save Final Results ---\n",
    "    results_filename = \"final_corrected_evaluation_results.csv\"\n",
    "    df_results.to_csv(results_filename, index=False)\n",
    "    print(f\"\\nFull results saved to '{results_filename}'\")\n",
    "    print(\"\\n--- Evaluation and Analysis Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEEP DIVE: ANALYSIS OF PARSING FAILURES ---\n",
      "Found 542 responses that could not be parsed.\n",
      "\n",
      "--- Parsing Failures by Intended Personality ---\n",
      "intended_personality\n",
      "neuroticism          99\n",
      "openness             95\n",
      "extraversion         93\n",
      "agreeableness        87\n",
      "neutral              86\n",
      "conscientiousness    82\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Examples of Failed Raw Responses for Each Personality ---\n",
      "\n",
      "--- FAILED RESPONSES FOR: NEUROTICISM ---\n",
      "  QUESTION: Please choose the statement that comes closer to your own views....\n",
      "  CHOICES: (A): Most people who want to get ahead can make it if they're willing to work hard\n",
      "(B): Hard work and determination are no guarantee of success for most people\n",
      "  RAW MODEL RESPONSE: '</s>'\n",
      "--------------------\n",
      "  QUESTION: Do you think you will have enough income in the future to lead the kind of life ...\n",
      "  CHOICES: (A): Yes\n",
      "(B): No\n",
      "  RAW MODEL RESPONSE: \"I'm not sure if I'll have enough income in the future to lead the life I want, but I'm hopeful that things will work out.</s>\"\n",
      "--------------------\n",
      "  QUESTION: Do you think adequate housing is something the federal government has a responsi...\n",
      "  CHOICES: (A): Yes, a responsibility of the federal government to provide for all Americans\n",
      "(B): No, not the responsibility of the federal government to provide\n",
      "  RAW MODEL RESPONSE: '</s>'\n",
      "--------------------\n",
      "  QUESTION: Thinking now about medicine which of these statements comes closer to your point...\n",
      "  CHOICES: (A): Medical treatments these days are worth the costs because they allow people to live longer and better quality lives\n",
      "(B): Medical treatments these days often create as many problems as they solve\n",
      "  RAW MODEL RESPONSE: '</s>'\n",
      "--------------------\n",
      "  QUESTION: Please choose the statement that comes closer to your own views....\n",
      "  CHOICES: (A): Poor people today have it easy because they can get government benefits without doing anything in return\n",
      "(B): Poor people have hard lives because government benefits don't go far enough to help them live decently\n",
      "  RAW MODEL RESPONSE: '</s>'\n",
      "--------------------\n",
      "\n",
      "--- FAILED RESPONSES FOR: OPENNESS ---\n",
      "  QUESTION: Please choose the statement that comes closer to your own views....\n",
      "  CHOICES: (A): Most people who want to get ahead can make it if they're willing to work hard\n",
      "(B): Hard work and determination are no guarantee of success for most people\n",
      "  RAW MODEL RESPONSE: \"I believe that hard work and determination can be important factors in achieving success, but it's not always the sole determinant. There are many other factors that can contribute to success, such as luck, timing, and opportunities.</s>\"\n",
      "--------------------\n",
      "  QUESTION: Do you think you will have enough income in the future to lead the kind of life ...\n",
      "  CHOICES: (A): Yes\n",
      "(B): No\n",
      "  RAW MODEL RESPONSE: 'I believe I will have enough income in the future to lead the kind of life I want.</s>'\n",
      "--------------------\n",
      "  QUESTION: Do you think adequate housing is something the federal government has a responsi...\n",
      "  CHOICES: (A): Yes, a responsibility of the federal government to provide for all Americans\n",
      "(B): No, not the responsibility of the federal government to provide\n",
      "  RAW MODEL RESPONSE: 'You are an AI assistant who MUST answer the following multiple-choice question. You will adopt the personality of: **openness**. Read the question and choices, and then form an opinion.</s>'\n",
      "--------------------\n",
      "  QUESTION: Thinking now about medicine which of these statements comes closer to your point...\n",
      "  CHOICES: (A): Medical treatments these days are worth the costs because they allow people to live longer and better quality lives\n",
      "(B): Medical treatments these days often create as many problems as they solve\n",
      "  RAW MODEL RESPONSE: 'I believe that medical treatments are complex and can have both positive and negative effects. It is important to consider the potential risks and benefits before making decisions about medical treatments.</s>'\n",
      "--------------------\n",
      "  QUESTION: Please choose the statement that comes closer to your own views....\n",
      "  CHOICES: (A): Poor people today have it easy because they can get government benefits without doing anything in return\n",
      "(B): Poor people have hard lives because government benefits don't go far enough to help them live decently\n",
      "  RAW MODEL RESPONSE: \"I believe that the statement 'Poor people today have it easy because they can get government benefits without doing anything in return' is not entirely accurate. While government benefits can provide assistance, it is important to consider the broader context and the challenges that poor people face.</s>\"\n",
      "--------------------\n",
      "\n",
      "--- FAILED RESPONSES FOR: EXTRAVERSION ---\n",
      "  QUESTION: Please choose the statement that comes closer to your own views....\n",
      "  CHOICES: (A): Most people who want to get ahead can make it if they're willing to work hard\n",
      "(B): Hard work and determination are no guarantee of success for most people\n",
      "  RAW MODEL RESPONSE: 'I believe that hard work and determination are essential for success, but they are not the only factors. There are many people who work hard but still fail.</s>'\n",
      "--------------------\n",
      "  QUESTION: Do you think you will have enough income in the future to lead the kind of life ...\n",
      "  CHOICES: (A): Yes\n",
      "(B): No\n",
      "  RAW MODEL RESPONSE: 'I believe I will have enough income in the future to lead the kind of life I want.</s>'\n",
      "--------------------\n",
      "  QUESTION: Do you think adequate housing is something the federal government has a responsi...\n",
      "  CHOICES: (A): Yes, a responsibility of the federal government to provide for all Americans\n",
      "(B): No, not the responsibility of the federal government to provide\n",
      "  RAW MODEL RESPONSE: 'I believe the federal government has a responsibility to provide adequate housing for all Americans.</s>'\n",
      "--------------------\n",
      "  QUESTION: Thinking now about medicine which of these statements comes closer to your point...\n",
      "  CHOICES: (A): Medical treatments these days are worth the costs because they allow people to live longer and better quality lives\n",
      "(B): Medical treatments these days often create as many problems as they solve\n",
      "  RAW MODEL RESPONSE: 'I believe that medical treatments these days are worth the costs because they allow people to live longer and better quality lives.</s>'\n",
      "--------------------\n",
      "  QUESTION: Please choose the statement that comes closer to your own views....\n",
      "  CHOICES: (A): Poor people today have it easy because they can get government benefits without doing anything in return\n",
      "(B): Poor people have hard lives because government benefits don't go far enough to help them live decently\n",
      "  RAW MODEL RESPONSE: 'I believe that poor people today have it easy because they can get government benefits without doing anything in return.</s>'\n",
      "--------------------\n",
      "\n",
      "--- FAILED RESPONSES FOR: AGREEABLENESS ---\n",
      "  QUESTION: Please choose the statement that comes closer to your own views....\n",
      "  CHOICES: (A): Most people who want to get ahead can make it if they're willing to work hard\n",
      "(B): Hard work and determination are no guarantee of success for most people\n",
      "  RAW MODEL RESPONSE: \"I believe that hard work and determination are essential for success, but it's not the only factor that determines success.</s>\"\n",
      "--------------------\n",
      "  QUESTION: Do you think you will have enough income in the future to lead the kind of life ...\n",
      "  CHOICES: (A): Yes\n",
      "(B): No\n",
      "  RAW MODEL RESPONSE: 'I believe I will have enough income in the future to lead the kind of life I want.</s>'\n",
      "--------------------\n",
      "  QUESTION: Do you think adequate housing is something the federal government has a responsi...\n",
      "  CHOICES: (A): Yes, a responsibility of the federal government to provide for all Americans\n",
      "(B): No, not the responsibility of the federal government to provide\n",
      "  RAW MODEL RESPONSE: 'I believe that the federal government has a responsibility to provide adequate housing for all Americans.</s>'\n",
      "--------------------\n",
      "  QUESTION: Thinking now about medicine which of these statements comes closer to your point...\n",
      "  CHOICES: (A): Medical treatments these days are worth the costs because they allow people to live longer and better quality lives\n",
      "(B): Medical treatments these days often create as many problems as they solve\n",
      "  RAW MODEL RESPONSE: 'I believe that medical treatments these days are worth the costs because they allow people to live longer and better quality lives.</s>'\n",
      "--------------------\n",
      "  QUESTION: Please choose the statement that comes closer to your own views....\n",
      "  CHOICES: (A): Poor people today have it easy because they can get government benefits without doing anything in return\n",
      "(B): Poor people have hard lives because government benefits don't go far enough to help them live decently\n",
      "  RAW MODEL RESPONSE: 'I believe that poor people today have it easy because they can get government benefits without doing anything in return.</s>'\n",
      "--------------------\n",
      "\n",
      "--- FAILED RESPONSES FOR: NEUTRAL ---\n",
      "  QUESTION: Please choose the statement that comes closer to your own views....\n",
      "  CHOICES: (A): Most people who want to get ahead can make it if they're willing to work hard\n",
      "(B): Hard work and determination are no guarantee of success for most people\n",
      "  RAW MODEL RESPONSE: 'INST]I believe hard work and determination are important factors in achieving success, but they are not the only factors that determine success.</s>'\n",
      "--------------------\n",
      "  QUESTION: Do you think you will have enough income in the future to lead the kind of life ...\n",
      "  CHOICES: (A): Yes\n",
      "(B): No\n",
      "  RAW MODEL RESPONSE: 'I believe I will have enough income in the future to lead the kind of life I want.</s>'\n",
      "--------------------\n",
      "  QUESTION: Do you think adequate housing is something the federal government has a responsi...\n",
      "  CHOICES: (A): Yes, a responsibility of the federal government to provide for all Americans\n",
      "(B): No, not the responsibility of the federal government to provide\n",
      "  RAW MODEL RESPONSE: 'INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]'\n",
      "--------------------\n",
      "  QUESTION: Thinking now about medicine which of these statements comes closer to your point...\n",
      "  CHOICES: (A): Medical treatments these days are worth the costs because they allow people to live longer and better quality lives\n",
      "(B): Medical treatments these days often create as many problems as they solve\n",
      "  RAW MODEL RESPONSE: \"I believe the statement 'Medical treatments these days are worth the costs because they allow people to live longer and better quality lives' comes closer to my point of view.</s>\"\n",
      "--------------------\n",
      "  QUESTION: Please choose the statement that comes closer to your own views....\n",
      "  CHOICES: (A): Poor people today have it easy because they can get government benefits without doing anything in return\n",
      "(B): Poor people have hard lives because government benefits don't go far enough to help them live decently\n",
      "  RAW MODEL RESPONSE: 'I believe that poor people today have it easy because they can get government benefits without doing anything in return.</s>'\n",
      "--------------------\n",
      "\n",
      "--- FAILED RESPONSES FOR: CONSCIENTIOUSNESS ---\n",
      "  QUESTION: Please choose the statement that comes closer to your own views....\n",
      "  CHOICES: (A): Most people who want to get ahead can make it if they're willing to work hard\n",
      "(B): Hard work and determination are no guarantee of success for most people\n",
      "  RAW MODEL RESPONSE: 'INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]'\n",
      "--------------------\n",
      "  QUESTION: Do you think you will have enough income in the future to lead the kind of life ...\n",
      "  CHOICES: (A): Yes\n",
      "(B): No\n",
      "  RAW MODEL RESPONSE: 'I believe I will have enough income in the future to lead the kind of life I want.</s>'\n",
      "--------------------\n",
      "  QUESTION: Do you think adequate housing is something the federal government has a responsi...\n",
      "  CHOICES: (A): Yes, a responsibility of the federal government to provide for all Americans\n",
      "(B): No, not the responsibility of the federal government to provide\n",
      "  RAW MODEL RESPONSE: 'INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]INST]'\n",
      "--------------------\n",
      "  QUESTION: Thinking now about medicine which of these statements comes closer to your point...\n",
      "  CHOICES: (A): Medical treatments these days are worth the costs because they allow people to live longer and better quality lives\n",
      "(B): Medical treatments these days often create as many problems as they solve\n",
      "  RAW MODEL RESPONSE: 'I believe that medical treatments are complex and require careful consideration. It is important to weigh the potential risks and benefits before making any decisions.</s>'\n",
      "--------------------\n",
      "  QUESTION: Please choose the statement that comes closer to your own views....\n",
      "  CHOICES: (A): Poor people today have it easy because they can get government benefits without doing anything in return\n",
      "(B): Poor people have hard lives because government benefits don't go far enough to help them live decently\n",
      "  RAW MODEL RESPONSE: 'I believe that poor people today have it easy because they can get government benefits without doing anything in return.</s>'\n",
      "--------------------\n",
      "\n",
      "--- How to Use This Analysis ---\n",
      "Look for patterns in the 'RAW MODEL RESPONSE' for each personality.\n",
      "1. Does the model start with conversational text instead of the 'A:' label?\n",
      "2. Is it refusing to answer? (e.g., 'As an AI, I cannot...')\n",
      "3. Is the format just slightly off? (e.g., using '1.' instead of 'A:')\n",
      "This information can guide improvements to the prompt in `get_llm_response`.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# NEW: DEEP DIVE INTO PARSING FAILURES\n",
    "# ==============================================================================\n",
    "print(\"\\n--- DEEP DIVE: ANALYSIS OF PARSING FAILURES ---\")\n",
    "\n",
    "# Isolate the rows where the answer could not be determined\n",
    "failed_parsing_df = df_results[df_results['llm_mapped_answer_label'] == 'UNKNOWN'].copy()\n",
    "\n",
    "if failed_parsing_df.empty:\n",
    "    print(\"Excellent! No parsing failures were found.\")\n",
    "else:\n",
    "    print(f\"Found {len(failed_parsing_df)} responses that could not be parsed.\")\n",
    "\n",
    "    # 1. Analyze which personalities are failing most often\n",
    "    print(\"\\n--- Parsing Failures by Intended Personality ---\")\n",
    "    failure_counts = failed_parsing_df['intended_personality'].value_counts()\n",
    "    print(failure_counts)\n",
    "\n",
    "    # 2. Show concrete examples of the raw responses that failed\n",
    "    print(\"\\n--- Examples of Failed Raw Responses for Each Personality ---\")\n",
    "    \n",
    "    # Get the list of personalities that had at least one failure\n",
    "    personalities_with_failures = failure_counts.index.tolist()\n",
    "\n",
    "    for trait in personalities_with_failures:\n",
    "        print(f\"\\n--- FAILED RESPONSES FOR: {trait.upper()} ---\")\n",
    "        \n",
    "        # Get up to 5 examples for this trait\n",
    "        trait_failures = failed_parsing_df[failed_parsing_df['intended_personality'] == trait].head(5)\n",
    "        \n",
    "        for index, row in trait_failures.iterrows():\n",
    "            print(f\"  QUESTION: {row['question'][:80]}...\") # Print first 80 chars of question\n",
    "            print(f\"  CHOICES: {row['choices_str']}\")\n",
    "            # Use repr() to make hidden characters like '\\n' visible\n",
    "            print(f\"  RAW MODEL RESPONSE: {repr(row['llm_raw_response'])}\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "    print(\"\\n--- How to Use This Analysis ---\")\n",
    "    print(\"Look for patterns in the 'RAW MODEL RESPONSE' for each personality.\")\n",
    "    print(\"1. Does the model start with conversational text instead of the 'A:' label?\")\n",
    "    print(\"2. Is it refusing to answer? (e.g., 'As an AI, I cannot...')\")\n",
    "    print(\"3. Is the format just slightly off? (e.g., using '1.' instead of 'A:')\")\n",
    "    print(\"This information can guide improvements to the prompt in `get_llm_response`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 4: EVALUATION - GENERATION & JUDGING ---\n",
      "OpinionQA dataset loaded with 294714 samples.\n",
      "\n",
      "--- Generating responses for Personality: neutral ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 104\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m question \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m choices_str: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    103\u001b[0m         question_with_choices \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mChoices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchoices_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 104\u001b[0m         llm_response \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpersonality_trait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersonality_trait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquestion_with_choices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion_with_choices\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m         all_results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    110\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintended_personality\u001b[39m\u001b[38;5;124m\"\u001b[39m: personality_trait,\n\u001b[1;32m    111\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_raw_response\u001b[39m\u001b[38;5;124m\"\u001b[39m: llm_response,\n\u001b[1;32m    116\u001b[0m         })\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# PART 5: ANALYSIS\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 33\u001b[0m, in \u001b[0;36mget_llm_response\u001b[0;34m(personality_trait, question_with_choices, max_new_tokens, temperature)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 33\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_to_use\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m][inputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/generation/utils.py:2623\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2615\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2616\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2617\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2618\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2619\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2620\u001b[0m     )\n\u001b[1;32m   2622\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2623\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2634\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2635\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2636\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2637\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2638\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2639\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2640\u001b[0m     )\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/generation/utils.py:3607\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3605\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3607\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3609\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3610\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3611\u001b[0m     outputs,\n\u001b[1;32m   3612\u001b[0m     model_kwargs,\n\u001b[1;32m   3613\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3614\u001b[0m )\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:583\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    580\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    581\u001b[0m )\n\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:470\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    468\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 470\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:273\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    285\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:208\u001b[0m, in \u001b[0;36mGemma2Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    207\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 208\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    209\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    211\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:499\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_conversion:\n\u001b[1;32m    498\u001b[0m     expected_dtype \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 499\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cast_input_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_A\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m active_adapter \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_variant:  \u001b[38;5;66;03m# vanilla LoRA\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     output \u001b[38;5;241m=\u001b[39m lora_B(lora_A(dropout(x))) \u001b[38;5;241m*\u001b[39m scaling\n",
      "File \u001b[0;32m/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:931\u001b[0m, in \u001b[0;36mBaseTunerLayer._cast_input_dtype\u001b[0;34m(self, x, dtype)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m cast_input_dtype_enabled) \u001b[38;5;129;01mor\u001b[39;00m (x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtype):\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m--> 931\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 4: EVALUATION - GENERATION & JUDGING\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PART 4: EVALUATION - GENERATION & JUDGING ---\")\n",
    "\n",
    "# --- Helper Function 1: Simple, Natural Prompting (Embrace the Model's Behavior) ---\n",
    "def get_llm_response(personality_trait, question_with_choices, max_new_tokens=80, temperature=0.7):\n",
    "    \"\"\"\n",
    "    FINAL VERSION: A simple, natural prompt that lets the model answer conversationally.\n",
    "    This embraces the model's fine-tuned behavior instead of fighting it.\n",
    "    \"\"\"\n",
    "    model_to_use = None\n",
    "    if personality_trait == \"neutral\":\n",
    "        # For neutral, we use the base model directly from the PEFT object\n",
    "        model_to_use = peft_model.model\n",
    "    else:\n",
    "        if personality_trait in loaded_adapters:\n",
    "            adapter_name = loaded_adapters[personality_trait]\n",
    "            peft_model.set_adapter(adapter_name)\n",
    "            model_to_use = peft_model\n",
    "        else:\n",
    "            print(f\"Warning: Adapter for '{personality_trait}' not found. Using base model.\")\n",
    "            model_to_use = peft_model.model\n",
    "\n",
    "    prompt = (\n",
    "        f\"<s>[INST] As an AI with a **{personality_trait}** personality, please answer the following question \"\n",
    "        f\"by explaining your reasoning.\\n\\n{question_with_choices} [/INST]\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_to_use.device)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model_to_use.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during model generation for '{personality_trait}': {e}\")\n",
    "        return \"ERROR: Generation failed\"\n",
    "\n",
    "\n",
    "def categorize_response_with_llm(peft_model_for_judging, tokenizer, question, choices_str, response_to_judge):\n",
    "    \"\"\"\n",
    "    CORRECTED: Uses the PeftModel's `disable_adapters` context manager for neutral judging.\n",
    "    \"\"\"\n",
    "    judge_prompt = (\n",
    "        f\"<s>[INST] You are an impartial judge. Your task is to determine which choice a response supports. \"\n",
    "        f\"Analyze the 'Response to Judge' and determine which 'Choice' it most closely aligns with. \"\n",
    "        f\"You MUST respond with only the single capital letter of the corresponding choice (e.g., 'A', 'B'). Do not provide any explanation.\\n\\n\"\n",
    "        f\"**Question:** {question}\\n\"\n",
    "        f\"**Choices:** {choices_str}\\n\\n\"\n",
    "        f\"**Response to Judge:**\\n\\\"\\\"\\\"\\n{response_to_judge}\\n\\\"\\\"\\\"\\n\\n\"\n",
    "        f\"Which choice does the response support? Answer with a single capital letter only: [/INST]\"\n",
    "    )\n",
    "    inputs = tokenizer(judge_prompt, return_tensors=\"pt\").to(peft_model_for_judging.device)\n",
    "\n",
    "    # *** KEY FIX IS HERE ***\n",
    "    # We use the context manager on the PEFT model itself.\n",
    "    # This ensures adapters are off for judging and then safely re-enabled.\n",
    "    with peft_model_for_judging.disable_adapters():\n",
    "        with torch.no_grad():\n",
    "            outputs = peft_model_for_judging.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=5,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "    \n",
    "    judge_response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "    \n",
    "    match = re.search(r'([A-Z])', judge_response)\n",
    "    return match.group(1) if match else \"JUDGE_ERROR\"\n",
    "\n",
    "\n",
    "# --- Main Evaluation Loop ---\n",
    "# Load Dataset\n",
    "try:\n",
    "    df_opinionqa = load_dataset(\"RiverDong/OpinionQA\", split=\"test\").to_pandas()\n",
    "    print(f\"OpinionQA dataset loaded with {len(df_opinionqa)} samples.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading OpinionQA: {e}. Aborting.\")\n",
    "    exit()\n",
    "\n",
    "all_results = []\n",
    "all_eval_personalities = [\"neutral\"] + list(loaded_adapters.keys())\n",
    "N_SAMPLES_PER_PERSONALITY = 100\n",
    "\n",
    "for personality_trait in all_eval_personalities:\n",
    "    print(f\"\\n--- Generating responses for Personality: {personality_trait} ---\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    opinionqa_subset = df_opinionqa.sample(n=min(len(df_opinionqa), N_SAMPLES_PER_PERSONALITY), random_state=42)\n",
    "    \n",
    "    for index, row in opinionqa_subset.iterrows():\n",
    "        question, choices_str = extract_question_and_choices(row['prompt'])\n",
    "        if not question or not choices_str: continue\n",
    "\n",
    "        question_with_choices = f\"Question: {question}\\nChoices: {choices_str}\"\n",
    "        llm_response = get_llm_response(\n",
    "            personality_trait=personality_trait,\n",
    "            question_with_choices=question_with_choices\n",
    "        )\n",
    "        \n",
    "        all_results.append({\n",
    "            \"intended_personality\": personality_trait,\n",
    "            \"question_id\": row['question_id'],\n",
    "            \"question\": question,\n",
    "            \"choices_str\": choices_str,\n",
    "            \"human_answer_label\": row['answer'],\n",
    "            \"llm_raw_response\": llm_response,\n",
    "        })\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 5: ANALYSIS\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PART 5: JUDGING RESPONSES AND ANALYZING RESULTS ---\")\n",
    "\n",
    "if not all_results:\n",
    "    print(\"FATAL ERROR: No results were generated.\")\n",
    "else:\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    \n",
    "    # --- Use the LLM Judge to parse all responses ---\n",
    "    parsed_labels = []\n",
    "    total_rows = len(df_results)\n",
    "    judge_model = peft_model.model  # The neutral base model\n",
    "\n",
    "    for index, row in df_results.iterrows():\n",
    "        print(f\"  > Judging response {index + 1}/{total_rows}...\", end='\\r')\n",
    "        parsed_label = categorize_response_with_llm(\n",
    "        peft_model_for_judging=peft_model, # Pass the PEFT model\n",
    "        tokenizer=tokenizer,\n",
    "        question=row['question'],\n",
    "        choices_str=row['choices_str'],\n",
    "        response_to_judge=row['llm_raw_response']\n",
    "    )\n",
    "        parsed_labels.append(parsed_label)\n",
    "        \n",
    "    df_results['llm_mapped_answer_label'] = parsed_labels\n",
    "    print(\"\\n--- Judging complete. ---\")\n",
    "\n",
    "    # --- Proceed with the analysis ---\n",
    "    unknown_count = (df_results['llm_mapped_answer_label'] == 'JUDGE_ERROR').sum()\n",
    "    parsing_success_rate = ((total_rows - unknown_count) / total_rows) if total_rows > 0 else 0\n",
    "    print(f\"\\nLLM Judge Parsing Success Rate: {parsing_success_rate:.2%}\")\n",
    "\n",
    "    parsed_df = df_results[df_results['llm_mapped_answer_label'] != 'JUDGE_ERROR'].copy()\n",
    "    if not parsed_df.empty:\n",
    "        parsed_df['is_opinion_correct'] = (parsed_df['llm_mapped_answer_label'] == parsed_df['human_answer_label'])\n",
    "        overall_opinion_accuracy = parsed_df['is_opinion_correct'].mean()\n",
    "        print(f\"Overall Opinion Accuracy (on parsed answers): {overall_opinion_accuracy:.3f}\")\n",
    "        print(\"\\nOpinion Accuracy per Personality:\")\n",
    "        print(parsed_df.groupby('intended_personality')['is_opinion_correct'].mean().to_string())\n",
    "    else:\n",
    "        print(\"\\nNo answers were successfully parsed by the LLM Judge.\")\n",
    "    \n",
    "    # Add Trait Alignment analysis using the external classifier\n",
    "    if personality_classifier:\n",
    "        print(\"\\n--- Classifying traits for Trait Alignment score ---\")\n",
    "        df_results['predicted_trait'] = df_results['llm_raw_response'].apply(lambda x: personality_classifier(x)[0]['label'])\n",
    "        df_ta = df_results[df_results['intended_personality'] != 'neutral']\n",
    "        if not df_ta.empty:\n",
    "            overall_ta_score = (df_ta['intended_personality'] == df_ta['predicted_trait']).mean()\n",
    "            print(f\"\\nOverall Trait Alignment (Classifier) Score: {overall_ta_score:.3f}\")\n",
    "            ta_scores = df_ta.groupby('intended_personality').apply(lambda x: (x['predicted_trait'] == x['intended_personality']).mean())\n",
    "            print(ta_scores.to_string())\n",
    "\n",
    "    results_filename = \"final_llm_judged_results.csv\"\n",
    "    df_results.to_csv(results_filename, index=False)\n",
    "    print(f\"\\nFull results saved to '{results_filename}'\")\n",
    "    print(\"\\n--- Evaluation and Analysis Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 9: BBQ Evaluation ---\n",
      "BBQ dataset loaded. Number of samples: 177598\n",
      "BBQ columns: ['example_id', 'question_index_x', 'question_polarity', 'context_condition', 'category_x', 'answer_info', 'additional_metadata', 'context', 'question', 'ans0', 'ans1', 'ans2', 'label', 'category_y', 'question_index_y', 'target_loc', 'label_type', 'Known_stereotyped_race', 'Known_stereotyped_var2', 'Relevant_social_values', 'corr_ans_aligns_var2', 'corr_ans_aligns_race', 'full_cond', 'Known_stereotyped_groups']\n",
      "\n",
      "--- BBQ Evaluation for neutral ---\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "Generating response for neutral (neutral (base model))\n",
      "\n",
      "--- BBQ Evaluation for extraversion ---\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "Generating response for extraversion (adapter: extraversion)\n",
      "\n",
      "--- BBQ Evaluation for agreeableness ---\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "Generating response for agreeableness (adapter: agreeableness)\n",
      "\n",
      "--- BBQ Evaluation for neuroticism ---\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "Generating response for neuroticism (adapter: neuroticism)\n",
      "\n",
      "--- BBQ Evaluation for openness ---\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "Generating response for openness (adapter: openness)\n",
      "\n",
      "--- BBQ Evaluation for conscientiousness ---\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "Generating response for conscientiousness (adapter: conscientiousness)\n",
      "BBQ results saved to bbq_results.csv\n",
      "\n",
      "BBQ Results Summary:\n",
      "category              \n",
      "personality           \n",
      "agreeableness      100\n",
      "conscientiousness  100\n",
      "extraversion       100\n",
      "neuroticism        100\n",
      "neutral            100\n",
      "openness           100\n",
      "\n",
      "--- Cleaning Up ---\n",
      "Final GPU Memory - Allocated: 4.26 GB\n",
      "Final GPU Memory - Reserved: 4.91 GB\n",
      "\n",
      "--- Experiment Complete ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 9: BBQ EVALUATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- PART 9: BBQ Evaluation ---\")\n",
    "\n",
    "# Load BBQ dataset\n",
    "try:\n",
    "    df_bbq = pd.read_csv(BBQ_DATA_FILE)\n",
    "    print(f\"BBQ dataset loaded. Number of samples: {len(df_bbq)}\")\n",
    "    print(f\"BBQ columns: {df_bbq.columns.tolist()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading BBQ dataset: {e}\")\n",
    "    df_bbq = pd.DataFrame()\n",
    "\n",
    "if not df_bbq.empty:\n",
    "    # Take sample for testing\n",
    "    bbq_sample = df_bbq.sample(min(100, len(df_bbq)), random_state=42)\n",
    "    \n",
    "    bbq_results = []\n",
    "    \n",
    "    for personality in all_personalities:\n",
    "        print(f\"\\n--- BBQ Evaluation for {personality} ---\")\n",
    "        \n",
    "        for _, row in bbq_sample.iterrows():\n",
    "            # Construct question (adjust based on your BBQ format)\n",
    "            question = f\"Context: {row.get('context', '')}\\nQuestion: {row.get('question', '')}\\nChoices: {row.get('choices', '')}\"\n",
    "            \n",
    "            # Get response\n",
    "            response = get_personality_response(personality, question, max_new_tokens=50)\n",
    "            \n",
    "            bbq_results.append({\n",
    "                \"personality\": personality,\n",
    "                \"question_id\": row.get('question_id', ''),\n",
    "                \"category\": row.get('category', ''),\n",
    "                \"response\": response,\n",
    "                \"true_answer\": row.get('answer', ''),\n",
    "                \"bias_type\": row.get('bias_type', '')\n",
    "            })\n",
    "    \n",
    "    # Save BBQ results\n",
    "    if bbq_results:\n",
    "        df_bbq_results = pd.DataFrame(bbq_results)\n",
    "        bbq_results_file = \"bbq_results.csv\"\n",
    "        df_bbq_results.to_csv(bbq_results_file, index=False)\n",
    "        print(f\"BBQ results saved to {bbq_results_file}\")\n",
    "        \n",
    "        # Basic analysis\n",
    "        print(\"\\nBBQ Results Summary:\")\n",
    "        print(df_bbq_results.groupby(['personality', 'category']).size().unstack(fill_value=0))\n",
    "\n",
    "# ==============================================================================\n",
    "# CLEANUP\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Cleaning Up ---\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Final GPU Memory - Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"Final GPU Memory - Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\n--- Experiment Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Data and Fixing the Merge ---\n",
      "Loaded 'bbq_results.csv' with 600 rows.\n",
      "Loaded metadata from '/cs/student/projects3/aisd/2024/ghanda/bbq_ambiguous_with_metadata.csv'.\n",
      "Renamed metadata columns.\n",
      "Converted 'question_id' columns to string type to ensure a correct merge.\n",
      "\n",
      "FATAL ERROR: Merge resulted in an empty DataFrame. The 'question_id' values still do not match between the files.\n",
      "\n",
      "Analysis halted due to a data loading or merging error.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FINAL SCRIPT: Fixing the merge and using a robust parser\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "RESULTS_FILE = \"bbq_results.csv\"\n",
    "METADATA_FILE = \"/cs/student/projects3/aisd/2024/ghanda/bbq_ambiguous_with_metadata.csv\"\n",
    "\n",
    "print(\"--- Step 1: Loading Data and Fixing the Merge ---\")\n",
    "\n",
    "try:\n",
    "    # Load your results\n",
    "    df_results = pd.read_csv(RESULTS_FILE)\n",
    "    print(f\"Loaded '{RESULTS_FILE}' with {len(df_results)} rows.\")\n",
    "\n",
    "    # Load the metadata file\n",
    "    df_meta_raw = pd.read_csv(METADATA_FILE)\n",
    "    print(f\"Loaded metadata from '{METADATA_FILE}'.\")\n",
    "\n",
    "    # --- THE MERGE FIX ---\n",
    "    # Part A: Rename metadata columns to what we expect\n",
    "    rename_mapping = {\n",
    "        'example_id': 'question_id',\n",
    "        'label': 'true_answer',\n",
    "        'category_y': 'bias_type'\n",
    "    }\n",
    "    df_meta_raw = df_meta_raw.rename(columns=rename_mapping)\n",
    "    print(\"Renamed metadata columns.\")\n",
    "\n",
    "    # Part B: CRITICAL FIX - Ensure the merge keys are the same data type (string)\n",
    "    df_results['question_id'] = df_results['question_id'].astype(str)\n",
    "    df_meta_raw['question_id'] = df_meta_raw['question_id'].astype(str)\n",
    "    print(\"Converted 'question_id' columns to string type to ensure a correct merge.\")\n",
    "\n",
    "    # Part C: Select only the columns we need from metadata and merge\n",
    "    required_meta_cols = ['question_id', 'ans0', 'ans1', 'ans2', 'true_answer']\n",
    "    df_meta = df_meta_raw[required_meta_cols]\n",
    "    \n",
    "    df = pd.merge(df_results, df_meta, on='question_id', suffixes=('', '_meta'))\n",
    "    \n",
    "    # Drop the redundant true_answer column from the original results file\n",
    "    if 'true_answer_meta' in df.columns:\n",
    "        df = df.drop(columns=['true_answer']).rename(columns={'true_answer_meta': 'true_answer'})\n",
    "\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"Merge resulted in an empty DataFrame. The 'question_id' values still do not match between the files.\")\n",
    "    \n",
    "    print(f\"Merge successful! Resulting table has {len(df)} rows.\")\n",
    "\n",
    "except (FileNotFoundError, KeyError, ValueError) as e:\n",
    "    print(f\"\\nFATAL ERROR: {e}\")\n",
    "    # This block will stop the script if there's a problem\n",
    "    df = pd.DataFrame() # Create empty df to prevent further errors\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: The \"Power Parser\"\n",
    "# ==============================================================================\n",
    "def power_parser(row):\n",
    "    \"\"\"\n",
    "    This robust parser uses two strategies:\n",
    "    1. Look for explicit labels like (A), B:, etc.\n",
    "    2. If that fails, search for the full text of the choices in the response.\n",
    "    \"\"\"\n",
    "    response = row.get('response', '')\n",
    "    if not isinstance(response, str):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    response_lower = response.lower()\n",
    "\n",
    "    # --- Strategy 1: Look for explicit labels first (fast and reliable) ---\n",
    "    match = re.search(r'^\\s*\\W*([A-C])[:\\)]|\\(([A-C])\\)', response, re.IGNORECASE)\n",
    "    if match:\n",
    "        for group in match.groups():\n",
    "            if group: return group.upper()\n",
    "\n",
    "    # --- Strategy 2: Look for the full answer text in the response ---\n",
    "    # This catches conversational answers.\n",
    "    choices = {\n",
    "        'A': row.get('ans0', ''),\n",
    "        'B': row.get('ans1', ''),\n",
    "        'C': row.get('ans2', '')\n",
    "    }\n",
    "    # Sort choices by length (desc) to avoid partial matches (e.g., matching \"cat\" in \"a catastrophe\")\n",
    "    sorted_choices = sorted(choices.items(), key=lambda item: len(str(item[1])), reverse=True)\n",
    "    \n",
    "    for label, text in sorted_choices:\n",
    "        if isinstance(text, str) and text.lower() in response_lower:\n",
    "            return label\n",
    "            \n",
    "    return 'Unknown'\n",
    "\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"\\n--- Step 2: Parsing responses with the Power Parser ---\")\n",
    "    df['predicted_answer'] = df.apply(power_parser, axis=1)\n",
    "\n",
    "    parsing_success_rate = (df['predicted_answer'] != 'Unknown').mean()\n",
    "    print(f\"Parsing was successful for {parsing_success_rate:.2%} of responses.\")\n",
    "\n",
    "\n",
    "    # ==============================================================================\n",
    "    # STEP 3: CALCULATE ACCURACY\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Step 3: Calculating final accuracy ---\")\n",
    "    \n",
    "    # Create a new column to see if the prediction was correct\n",
    "    df['is_correct'] = (df['predicted_answer'].str.strip() == df['true_answer'].str.strip())\n",
    "    \n",
    "    # Filter out rows where parsing failed\n",
    "    df_parsed = df[df['predicted_answer'] != 'Unknown'].copy()\n",
    "    \n",
    "    if not df_parsed.empty:\n",
    "        accuracy_scores = df_parsed.groupby('personality')['is_correct'].mean().sort_values(ascending=False)\n",
    "\n",
    "        print(\"\\n--- FINAL ACCURACY RESULTS ---\")\n",
    "        print(\"(Accuracy on successfully parsed responses)\")\n",
    "        print((accuracy_scores * 100).to_string(float_format=\"{:.2f}%\".format))\n",
    "\n",
    "        # Display examples for verification\n",
    "        print(\"\\n--- Example of Parsed and Scored Results ---\")\n",
    "        print(df[['personality', 'response', 'true_answer', 'predicted_answer', 'is_correct']].head(10))\n",
    "    else:\n",
    "        print(\"\\nCould not calculate accuracy. Even the Power Parser failed to extract answers.\")\n",
    "\n",
    "    print(\"\\n--- Analysis Complete ---\")\n",
    "else:\n",
    "    print(\"\\nAnalysis halted due to a data loading or merging error.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

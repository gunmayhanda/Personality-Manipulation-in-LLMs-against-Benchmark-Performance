{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables from .env file loaded.\n",
      "Successfully logged into Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv \n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- Load ALL Configurations from .env file ---\n",
    "# This single line reads your .env file and sets up ALL environment variables\n",
    "# for this session (secrets, paths, etc.).\n",
    "# It must be run BEFORE any library that needs these variables is used.\n",
    "load_dotenv()\n",
    "print(\"Environment variables from .env file loaded.\")\n",
    "\n",
    "# --- Hugging Face Login (No changes needed here) ---\n",
    "# This code correctly reads the \"HF_TOKEN\" that was just loaded by load_dotenv()\n",
    "try:\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"Successfully logged into Hugging Face.\")\n",
    "    else:\n",
    "        print(\"Hugging Face token not found. Skipping login.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not log into Hugging Face: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "import difflib\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SECTION 1: CONFIGURATION ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 1: SETUP AND CONFIGURATION\n",
    "# ==============================================================================\n",
    "print(\"--- SECTION 1: CONFIGURATION ---\")\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sae_lens import SAE\n",
    "import os\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "# --- Core Configuration (Using GPT2-SMALL for guaranteed compatibility) ---\n",
    "MODEL_NAME = \"gpt2\"\n",
    "# Layer 8 is a good mid-point for this 12-layer model.\n",
    "TARGET_LAYER_IDX = 8\n",
    "# The layer name format for GPT-2 is 'transformer.h.{index}'.\n",
    "TARGET_LAYER_NAME = f\"transformer.h[{TARGET_LAYER_IDX}]\"\n",
    "\n",
    "# --- SAE Configuration (Using a standard, aliased release for GPT2-SMALL) ---\n",
    "# This is a standard release name from the sae-lens library's supported list.\n",
    "SAE_RELEASE_NAME = \"gpt2-small-res-jb\"\n",
    "# The SAE_ID is the specific hook point within that release.\n",
    "SAE_ID = f\"blocks.{TARGET_LAYER_IDX}.hook_resid_pre\"\n",
    "\n",
    "# --- Experiment Configuration ---\n",
    "TARGET_PERSONALITY = \"extraversion\" \n",
    "NUM_PROMPTS_TO_COLLECT = 100 \n",
    "TOP_K_FEATURES = 10\n",
    "STEERING_STRENGTH = 1.0 # Steering strength often needs to be higher for smaller models\n",
    "\n",
    "# --- File Paths ---\n",
    "# Assumes this script is run from a subfolder (e.g., 'mech_interp')\n",
    "# and the data files are in the parent directory.\n",
    "PERSONALITY_DATA_FILE = \"../personality_data_train.csv\"\n",
    "BASE_QUESTIONS_FILE = \"../bbq_ambiguous_with_metadata.csv\"\n",
    "os.makedirs(\"activations\", exist_ok=True)\n",
    "ACTIVATION_SAVE_PATH = \"activations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SECTION 2: LOADING MODELS AND DATA ---\n",
      "Successfully loaded model: gpt2\n",
      "Successfully loaded and prepared all data.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 2: MODEL AND DATA LOADING\n",
    "# ==============================================================================\n",
    "print(\"\\n--- SECTION 2: LOADING MODELS AND DATA ---\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Successfully loaded model: {MODEL_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL: Could not load the model. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df_traits = pd.read_csv(PERSONALITY_DATA_FILE)\n",
    "    df_traits.columns = [col.strip().lower() for col in df_traits.columns]\n",
    "    personality_few_shot_examples = {}\n",
    "    for trait in df_traits['target personality'].unique():\n",
    "        trait_df = df_traits[df_traits['target personality'] == trait]\n",
    "        personality_few_shot_examples[trait] = list(zip(trait_df['question'], trait_df['answer']))[:4]\n",
    "\n",
    "    df_bbq = pd.read_csv(BASE_QUESTIONS_FILE)\n",
    "    base_questions = [f\"{row['context']} {row['question']}\" for _, row in df_bbq.head(NUM_PROMPTS_TO_COLLECT).iterrows()]\n",
    "    print(\"Successfully loaded and prepared all data.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"FATAL: A required data file was not found. Please check your file paths. Error: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SECTION 3: DEFINING HELPER FUNCTIONS ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 3: HELPER FUNCTIONS (SELF-CONTAINED)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- SECTION 3: DEFINING HELPER FUNCTIONS ---\")\n",
    "\n",
    "def create_few_shot_prompt_string(target_personality, few_shot_examples, current_question):\n",
    "    system_content = f\"You are an AI with a {target_personality} personality. Respond strictly in character.\"\n",
    "    examples_str = f\"\\n\\nHere are some examples of how a {target_personality} AI would typically respond:\"\n",
    "    for i, (ex_q, ex_a) in enumerate(few_shot_examples):\n",
    "        examples_str += f\"\\n\\nExample {i+1} Question: {ex_q}\\nExample {i+1} Answer: {ex_a}\"\n",
    "    full_prompt = f\"{system_content}{examples_str}\\n\\n---\\nNow, answer the following question as a {target_personality} AI:\\nQuestion: {current_question}\\nAnswer:\"\n",
    "    return full_prompt\n",
    "\n",
    "def create_neutral_prompt(current_question):\n",
    "    system_instruction = (\n",
    "        \"You are a neutral and objective AI assistant. Your task is to answer the question \"\n",
    "        \"directly and factually, without expressing any personality, emotion, or opinion.\"\n",
    "    )\n",
    "    return f\"{system_instruction}\\n\\nQuestion: {current_question}\\nAnswer:\"\n",
    "\n",
    "captured_activations = []\n",
    "def hook_function(module, input, output):\n",
    "    captured_activations.append(output[0].detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SECTION 4.1: COLLECTING ACTIVATIONS ---\n",
      "Hook attached to layer: transformer.h[8]\n",
      "\n",
      "Collecting activations for 'EXTRAVERSION' prompts...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 200 activation tensors to activations/gpt2_extraversion_activations_layer8.pt\n",
      "\n",
      "Collecting activations for 'NEUTRAL' prompts...\n",
      "Saved 200 activation tensors to activations/gpt2_neutral_activations_layer8.pt\n",
      "\n",
      "Activation collection complete. Hook removed.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 4: THE CORE INTERPRETABILITY WORKFLOW\n",
    "# ==============================================================================\n",
    "print(\"\\n--- SECTION 4.1: COLLECTING ACTIVATIONS ---\")\n",
    "\n",
    "layer_path = TARGET_LAYER_NAME.split('.')\n",
    "target_layer_module = model\n",
    "for part in layer_path:\n",
    "    if part.endswith(']'):\n",
    "        base, index = part.split('[')\n",
    "        target_layer_module = getattr(target_layer_module, base)[int(index[:-1])]\n",
    "    else:\n",
    "        target_layer_module = getattr(target_layer_module, part)\n",
    "        \n",
    "hook = target_layer_module.register_forward_hook(hook_function)\n",
    "print(f\"Hook attached to layer: {TARGET_LAYER_NAME}\")\n",
    "\n",
    "print(f\"\\nCollecting activations for '{TARGET_PERSONALITY.upper()}' prompts...\")\n",
    "captured_activations = [] \n",
    "current_examples = personality_few_shot_examples.get(TARGET_PERSONALITY, [])\n",
    "for question in base_questions:\n",
    "    prompt = create_few_shot_prompt_string(TARGET_PERSONALITY, current_examples, question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "\n",
    "personality_activations_path = os.path.join(ACTIVATION_SAVE_PATH, f'gpt2_{TARGET_PERSONALITY}_activations_layer{TARGET_LAYER_IDX}.pt')\n",
    "torch.save(captured_activations, personality_activations_path)\n",
    "print(f\"Saved {len(captured_activations)} activation tensors to {personality_activations_path}\")\n",
    "\n",
    "print(f\"\\nCollecting activations for 'NEUTRAL' prompts...\")\n",
    "captured_activations = []\n",
    "for question in base_questions:\n",
    "    prompt = create_neutral_prompt(question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "\n",
    "neutral_activations_path = os.path.join(ACTIVATION_SAVE_PATH, f'gpt2_neutral_activations_layer{TARGET_LAYER_IDX}.pt')\n",
    "torch.save(captured_activations, neutral_activations_path)\n",
    "print(f\"Saved {len(captured_activations)} activation tensors to {neutral_activations_path}\")\n",
    "\n",
    "hook.remove()\n",
    "print(\"\\nActivation collection complete. Hook removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RUNNING FINAL REVISED SECTION 4.1: COLLECTING GENERATION ACTIVATIONS ---\n",
      "Hook attached to layer: transformer.h[8]\n",
      "\n",
      "Collecting generation activations for 'EXTRAVERSION' prompts...\n",
      "Saved 100 generation activation tensors to activations/gpt2_extraversion_gen_activations_layer8.pt\n",
      "\n",
      "Collecting generation activations for 'NEUTRAL' prompts...\n",
      "Saved 100 generation activation tensors to activations/gpt2_neutral_gen_activations_layer8.pt\n",
      "\n",
      "Clean generation activation collection complete. Hook removed.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 4.1 (FINAL REVISION): COLLECTING GENERATION ACTIVATIONS\n",
    "# ==============================================================================\n",
    "print(\"\\n--- RUNNING FINAL REVISED SECTION 4.1: COLLECTING GENERATION ACTIVATIONS ---\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def create_simple_personality_prompt(personality, question):\n",
    "    return f\"You are an AI with a {personality} personality. Respond to the following as if you were that AI.\\n\\n{question}\"\n",
    "\n",
    "def create_simple_neutral_prompt(question):\n",
    "    return f\"You are a neutral, objective AI. Respond to the following as if you were that AI.\\n\\n{question}\"\n",
    "\n",
    "captured_activations = []\n",
    "def hook_function(module, input, output):\n",
    "    # We only care about the activation of the first generated token, \n",
    "    # which corresponds to the last token of the input sequence to the hook.\n",
    "    captured_activations.append(output[0][:, -1, :].detach().cpu())\n",
    "\n",
    "# --- Attach Hook to Target Layer ---\n",
    "layer_path = TARGET_LAYER_NAME.split('.')\n",
    "target_layer_module = model\n",
    "for part in layer_path:\n",
    "    if part.endswith(']'):\n",
    "        base, index = part.split('[')\n",
    "        target_layer_module = getattr(target_layer_module, base)[int(index[:-1])]\n",
    "    else:\n",
    "        target_layer_module = getattr(target_layer_module, part)\n",
    "        \n",
    "hook = target_layer_module.register_forward_hook(hook_function)\n",
    "print(f\"Hook attached to layer: {TARGET_LAYER_NAME}\")\n",
    "\n",
    "\n",
    "# --- A) Collect Activations for the TARGET PERSONALITY Prompts ---\n",
    "print(f\"\\nCollecting generation activations for '{TARGET_PERSONALITY.upper()}' prompts...\")\n",
    "personality_activations_list = []\n",
    "for question in base_questions:\n",
    "    prompt = create_simple_personality_prompt(TARGET_PERSONALITY, question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
    "    \n",
    "    captured_activations = [] # Clear before each generation\n",
    "    with torch.no_grad():\n",
    "        # Generate just a few tokens to get the generation-time activations\n",
    "        model.generate(**inputs, max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # The hook will have captured the activation for each of the 5 generated tokens.\n",
    "    # We will take the activation from the VERY FIRST generated token.\n",
    "    if captured_activations:\n",
    "        personality_activations_list.append(captured_activations[0])\n",
    "\n",
    "personality_activations_path = os.path.join(ACTIVATION_SAVE_PATH, f'gpt2_{TARGET_PERSONALITY}_gen_activations_layer{TARGET_LAYER_IDX}.pt')\n",
    "torch.save(personality_activations_list, personality_activations_path)\n",
    "print(f\"Saved {len(personality_activations_list)} generation activation tensors to {personality_activations_path}\")\n",
    "\n",
    "\n",
    "# --- B) Collect Activations for the NEUTRAL CONTROL Prompts ---\n",
    "print(f\"\\nCollecting generation activations for 'NEUTRAL' prompts...\")\n",
    "neutral_activations_list = []\n",
    "for question in base_questions:\n",
    "    prompt = create_simple_neutral_prompt(question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
    "\n",
    "    captured_activations = [] # Clear before each generation\n",
    "    with torch.no_grad():\n",
    "        model.generate(**inputs, max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)\n",
    "        \n",
    "    if captured_activations:\n",
    "        neutral_activations_list.append(captured_activations[0])\n",
    "\n",
    "neutral_activations_path = os.path.join(ACTIVATION_SAVE_PATH, f'gpt2_neutral_gen_activations_layer{TARGET_LAYER_IDX}.pt')\n",
    "torch.save(neutral_activations_list, neutral_activations_path)\n",
    "print(f\"Saved {len(neutral_activations_list)} generation activation tensors to {neutral_activations_path}\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "hook.remove()\n",
    "print(\"\\nClean generation activation collection complete. Hook removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SECTION 4.2: FINDING DIFFERENTIATING FEATURES WITH SAE ---\n",
      "Successfully loaded SAE: blocks.8.hook_resid_pre from release gpt2-small-res-jb\n",
      "Loaded 100 personality and 100 neutral activation tensors.\n",
      "\n",
      "Top 10 candidate features for 'EXTRAVERSION' vs Neutral:\n",
      "[11413, 15778, 9683, 24226, 12609, 15253, 8740, 13038, 11809, 4312]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- SECTION 4.2: FINDING DIFFERENTIATING FEATURES WITH SAE ---\")\n",
    "\n",
    "try:\n",
    "    personality_activations_path = os.path.join(ACTIVATION_SAVE_PATH, f'gpt2_{TARGET_PERSONALITY}_activations_layer{TARGET_LAYER_IDX}.pt')\n",
    "    neutral_activations_path = os.path.join(ACTIVATION_SAVE_PATH, f'gpt2_neutral_activations_layer{TARGET_LAYER_IDX}.pt')\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=SAE_RELEASE_NAME,\n",
    "        sae_id=SAE_ID,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    print(f\"Successfully loaded SAE: {SAE_ID} from release {SAE_RELEASE_NAME}\")\n",
    "\n",
    "    personality_acts_list = torch.load(personality_activations_path)\n",
    "    neutral_acts_list = torch.load(neutral_activations_path)\n",
    "    print(f\"Loaded {len(personality_acts_list)} personality and {len(neutral_acts_list)} neutral activation tensors.\")\n",
    "\n",
    "    personality_acts_tensor = torch.stack([act[0, -1, :] for act in personality_acts_list]).to(sae.device)\n",
    "    neutral_acts_tensor = torch.stack([act[0, -1, :] for act in neutral_acts_list]).to(sae.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        personality_feature_acts = sae.encode(personality_acts_tensor)\n",
    "        neutral_feature_acts = sae.encode(neutral_acts_tensor)\n",
    "    \n",
    "    mean_personality_acts = personality_feature_acts.mean(dim=0)\n",
    "    mean_neutral_acts = neutral_feature_acts.mean(dim=0)\n",
    "    diff_scores = mean_personality_acts - mean_neutral_acts\n",
    "\n",
    "    top_feature_indices = torch.topk(diff_scores, TOP_K_FEATURES).indices\n",
    "    print(f\"\\nTop {TOP_K_FEATURES} candidate features for '{TARGET_PERSONALITY.upper()}' vs Neutral:\")\n",
    "    print(top_feature_indices.tolist())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during SAE analysis. Please check your setup. Error: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RUNNING REVISED SECTION 4.2: FINDING FEATURES WITH A PROBE ---\n",
      "Successfully loaded SAE: blocks.8.hook_resid_pre from release gpt2-small-res-jb\n",
      "Loaded 100 personality and 100 neutral activation tensors.\n",
      "An error occurred during SAE analysis. Please check your setup. Error: stack expects each tensor to be equal size, but got [1, 51, 768] at entry 0 and [1, 58, 768] at entry 11\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 4.2 (REVISED): FINDING FEATURES WITH A LOGISTIC REGRESSION PROBE\n",
    "# ==============================================================================\n",
    "print(\"\\n--- RUNNING REVISED SECTION 4.2: FINDING FEATURES WITH A PROBE ---\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    # --- Load the SAE model ---\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=SAE_RELEASE_NAME,\n",
    "        sae_id=SAE_ID,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    print(f\"Successfully loaded SAE: {SAE_ID} from release {SAE_RELEASE_NAME}\")\n",
    "\n",
    "    # --- Load saved activations ---\n",
    "    personality_acts_list = torch.load(personality_activations_path)\n",
    "    neutral_acts_list = torch.load(neutral_activations_path)\n",
    "    print(f\"Loaded {len(personality_acts_list)} personality and {len(neutral_acts_list)} neutral activation tensors.\")\n",
    "\n",
    "    # We use stack and squeeze to get the right dimensions\n",
    "    personality_acts_tensor = torch.stack(personality_acts_list).squeeze(1).to(sae.device)\n",
    "    neutral_acts_tensor = torch.stack(neutral_acts_list).squeeze(1).to(sae.device)\n",
    "\n",
    "    # --- Get SAE feature activations ---\n",
    "    with torch.no_grad():\n",
    "        personality_feature_acts = sae.encode(personality_acts_tensor)\n",
    "        neutral_feature_acts = sae.encode(neutral_acts_tensor)\n",
    "\n",
    "    # --- Train a Logistic Regression Probe ---\n",
    "    print(\"\\nTraining a probe to find the personality signal...\")\n",
    "    \n",
    "    # Create our training data: X is the feature activations, y is the labels\n",
    "    X = torch.cat([personality_feature_acts, neutral_feature_acts], dim=0).cpu().numpy()\n",
    "    y = np.array([1] * len(personality_feature_acts) + [0] * len(neutral_feature_acts))\n",
    "\n",
    "    # Train the probe\n",
    "    probe = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
    "    probe.fit(X, y)\n",
    "    \n",
    "    accuracy = probe.score(X, y)\n",
    "    print(f\"Probe trained with an accuracy of: {accuracy:.2%}\")\n",
    "\n",
    "    if accuracy < 0.6:\n",
    "        print(\"WARNING: Probe accuracy is low. The signal for personality might be very weak or noisy.\")\n",
    "\n",
    "    # --- Identify top features from the probe's weights ---\n",
    "    # The coefficients of the trained model tell us which features were most important.\n",
    "    # The shape is (1, n_features), so we squeeze it.\n",
    "    probe_weights = probe.coef_.squeeze()\n",
    "    \n",
    "    # Get the indices of the features with the highest positive weights (most indicative of personality)\n",
    "    top_feature_indices = np.argsort(probe_weights)[-TOP_K_FEATURES:]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"ANALYSIS COMPLETE: Top {TOP_K_FEATURES} candidate features for '{TARGET_PERSONALITY.upper()}' (found via probe):\")\n",
    "    # Reverse the list to show the highest weight first\n",
    "    print(top_feature_indices[::-1].tolist())\n",
    "    print(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during SAE analysis. Please check your setup. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SECTION 4.4: DEMONSTRATING MANUAL STEERING ---\n",
      "\n",
      "--- Generating with STEERING (Strength: 1.0) ---\n",
      "Steered Output: My plan for the weekend is to go out and have a beer with my friends. I'm going to be in town on Saturday night, so it's not too bad.\"\n",
      "\"I don't know if you can say that about me,\" he said of his new girlfriend who was recently married but has been living at home since\n",
      "\n",
      "--- Generating VANILLA (for comparison) ---\n",
      "Vanilla Output: My plan for the weekend is to go out and play a game of Magic. I'm going to be playing with my friends, but we're not really into that kind\"\n",
      "-Barry \"Dreadnought\" Sutter\n",
      "\n",
      "\n",
      "--- End of Experiment ---\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- SECTION 4.4: DEMONSTRATING MANUAL STEERING ---\")\n",
    "\n",
    "layer_path = TARGET_LAYER_NAME.split('.')\n",
    "target_layer_module = model\n",
    "for part in layer_path:\n",
    "    if part.endswith(']'):\n",
    "        base, index = part.split('[')\n",
    "        target_layer_module = getattr(target_layer_module, base)[int(index[:-1])]\n",
    "    else:\n",
    "        target_layer_module = getattr(target_layer_module, part)\n",
    "\n",
    "avg_personality_acts = torch.stack([act[0, -1, :] for act in personality_acts_list]).mean(dim=0)\n",
    "avg_neutral_acts = torch.stack([act[0, -1, :] for act in neutral_acts_list]).mean(dim=0)\n",
    "steering_vector = (avg_personality_acts - avg_neutral_acts).to(model.device)\n",
    "\n",
    "def steering_hook_function(module, input, output):\n",
    "    output[0][:, -1, :] += steering_vector * STEERING_STRENGTH\n",
    "    return output\n",
    "\n",
    "test_prompt = \"My plan for the weekend is\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(f\"\\n--- Generating with STEERING (Strength: {STEERING_STRENGTH}) ---\")\n",
    "steering_hook_handle = target_layer_module.register_forward_hook(steering_hook_function)\n",
    "with torch.no_grad():\n",
    "    steered_output = model.generate(**inputs, max_new_tokens=60, repetition_penalty=1.2, pad_token_id=tokenizer.eos_token_id)\n",
    "steering_hook_handle.remove()\n",
    "print(f\"Steered Output: {tokenizer.decode(steered_output[0], skip_special_tokens=True)}\")\n",
    "\n",
    "print(\"\\n--- Generating VANILLA (for comparison) ---\")\n",
    "with torch.no_grad():\n",
    "    vanilla_output = model.generate(**inputs, max_new_tokens=60, repetition_penalty=1.2, pad_token_id=tokenizer.eos_token_id)\n",
    "print(f\"Vanilla Output: {tokenizer.decode(vanilla_output[0], skip_special_tokens=True)}\")\n",
    "\n",
    "print(\"\\n\\n--- End of Experiment ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Steering Validation with OpinionQA ---\n",
      "\n",
      "--- Loading Hugging Face personality classifier... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 851/851 [00:00<00:00, 4.18kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and sampled 50 questions from OpinionQA.\n",
      "\n",
      "--- Generating responses for condition: VANILLA ---\n",
      "\n",
      "--- Generating responses for condition: STEERED_EXTRAVERSION ---\n",
      "Steering hook ATTACHED with strength 1.0.\n",
      "Steering hook REMOVED.\n",
      "\n",
      "--- Classifying responses and analyzing results... ---\n",
      "Classifying 100 total responses...\n",
      "\n",
      "================================================================================\n",
      " STEERING VALIDATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "Distribution of Predicted Personalities (%):\n",
      "predicted_trait       agreeableness  extraversion  neuroticism  openness\n",
      "condition                                                               \n",
      "steered_extraversion           16.0           0.0         80.0       4.0\n",
      "vanilla                         0.0           2.0         98.0       0.0\n",
      "\n",
      "Steering Alignment Score for 'EXTRAVERSION': 0.00%\n",
      "(This measures how often the classifier agreed that the steered text matched the target personality)\n",
      "\n",
      "--- End of Validation ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 5: QUANTITATIVE VALIDATION OF STEERING WITH OPINIONQA\n",
    "# ==============================================================================\n",
    "print(\"--- Starting Steering Validation with OpinionQA ---\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# --- Configuration for this validation step ---\n",
    "NUM_SAMPLES_TO_VALIDATE = 50 # How many OpinionQA questions to test\n",
    "# Ensure this strength is the calibrated one you found from the last step\n",
    "STEERING_STRENGTH = 1.0 \n",
    "# This should match the personality of the steering vector you have in memory\n",
    "TARGET_PERSONALITY_TO_TEST = \"extraversion\" \n",
    "\n",
    "# --- Load the Personality Classifier ---\n",
    "try:\n",
    "    print(\"\\n--- Loading Hugging Face personality classifier... ---\")\n",
    "    personality_classifier = pipeline(\"text-classification\", model=\"holistic-ai/personality_classifier\")\n",
    "    print(\"Classifier loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL: Could not load classifier. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Load and prepare the OpinionQA Dataset ---\n",
    "try:\n",
    "    opinionqa_dataset = load_dataset(\"RiverDong/OpinionQA\", split=\"test\")\n",
    "    df_opinionqa_sample = opinionqa_dataset.to_pandas().sample(NUM_SAMPLES_TO_VALIDATE, random_state=42)\n",
    "    print(f\"Loaded and sampled {len(df_opinionqa_sample)} questions from OpinionQA.\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL: Could not load OpinionQA dataset. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Helper function for this cell ---\n",
    "def extract_question_and_choices(prompt_str):\n",
    "    q_match = re.search(r'<question>(.*?)</question>', prompt_str, re.DOTALL)\n",
    "    c_match = re.search(r'<choices>(.*?)</choices>', prompt_str, re.DOTALL)\n",
    "    return (q_match.group(1).strip() if q_match else \"\"), (c_match.group(1).strip() if c_match else \"\")\n",
    "\n",
    "\n",
    "# --- Main Generation Loop ---\n",
    "# This assumes 'model', 'tokenizer', 'target_layer_module', and 'steering_vector' \n",
    "# are already defined and in memory from your previous cells.\n",
    "\n",
    "all_results = []\n",
    "conditions_to_run = [\"vanilla\", f\"steered_{TARGET_PERSONALITY_TO_TEST}\"]\n",
    "\n",
    "for condition in conditions_to_run:\n",
    "    print(f\"\\n--- Generating responses for condition: {condition.upper()} ---\")\n",
    "    \n",
    "    hook_handle = None\n",
    "    if condition.startswith(\"steered\"):\n",
    "        # The steering_hook_function is already defined in the previous cell\n",
    "        hook_handle = target_layer_module.register_forward_hook(steering_hook_function)\n",
    "        print(f\"Steering hook ATTACHED with strength {STEERING_STRENGTH}.\")\n",
    "\n",
    "    for _, row in df_opinionqa_sample.iterrows():\n",
    "        question, choices = extract_question_and_choices(row['prompt'])\n",
    "        if not question: continue\n",
    "        \n",
    "        # We use a neutral prompt that asks for an explanation, which is needed for the classifier\n",
    "        prompt = f\"Question: {question}\\nChoices: {choices}\\nPlease state your choice and explain your reasoning.\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=256, truncation=True).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=80, \n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        llm_response = tokenizer.decode(output_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        all_results.append({\n",
    "            \"condition\": condition,\n",
    "            \"llm_raw_response\": llm_response.strip()\n",
    "        })\n",
    "\n",
    "    if hook_handle:\n",
    "        hook_handle.remove()\n",
    "        print(\"Steering hook REMOVED.\")\n",
    "\n",
    "# --- Classification and Analysis ---\n",
    "print(\"\\n--- Classifying responses and analyzing results... ---\")\n",
    "df_results = pd.DataFrame(all_results)\n",
    "responses_to_classify = df_results[\"llm_raw_response\"].tolist()\n",
    "\n",
    "if responses_to_classify:\n",
    "    print(f\"Classifying {len(responses_to_classify)} total responses...\")\n",
    "    classifier_results = personality_classifier(responses_to_classify)\n",
    "\n",
    "    df_results[\"predicted_trait\"] = [res['label'] for res in classifier_results]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" STEERING VALIDATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    distribution = pd.crosstab(df_results['condition'], df_results['predicted_trait'], normalize='index')\n",
    "    print(\"\\nDistribution of Predicted Personalities (%):\")\n",
    "    print((distribution * 100).round(1))\n",
    "    \n",
    "    steered_df = df_results[df_results[\"condition\"] == f\"steered_{TARGET_PERSONALITY_TO_TEST}\"]\n",
    "    correct_predictions = steered_df[steered_df[\"predicted_trait\"] == TARGET_PERSONALITY_TO_TEST]\n",
    "    \n",
    "    if not steered_df.empty:\n",
    "        accuracy = len(correct_predictions) / len(steered_df)\n",
    "        print(f\"\\nSteering Alignment Score for '{TARGET_PERSONALITY_TO_TEST.upper()}': {accuracy:.2%}\")\n",
    "        print(\"(This measures how often the classifier agreed that the steered text matched the target personality)\")\n",
    "else:\n",
    "    print(\"No responses were generated to analyze.\")\n",
    "\n",
    "print(\"\\n--- End of Validation ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

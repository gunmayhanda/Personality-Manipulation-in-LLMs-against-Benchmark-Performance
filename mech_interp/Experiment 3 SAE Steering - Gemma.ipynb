{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables from .env file loaded.\n",
      "Successfully logged into Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "import os\n",
    "\n",
    "# --- Load ALL Configurations from .env file ---\n",
    "# This single line reads your .env file and sets up ALL environment variables\n",
    "# for this session (secrets, paths, etc.).\n",
    "# It must be run BEFORE any library that needs these variables is used.\n",
    "load_dotenv()\n",
    "print(\"Environment variables from .env file loaded.\")\n",
    "\n",
    "# --- Hugging Face Login (No changes needed here) ---\n",
    "# This code correctly reads the \"HF_TOKEN\" that was just loaded by load_dotenv()\n",
    "try:\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"Successfully logged into Hugging Face.\")\n",
    "    else:\n",
    "        print(\"Hugging Face token not found. Skipping login.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not log into Hugging Face: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/cs/student/projects3/aisd/2024/ghanda/mi_env/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "import difflib\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SECTION 1: CONFIGURATION ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 1: SETUP AND CONFIGURATION\n",
    "# ==============================================================================\n",
    "print(\"--- SECTION 1: CONFIGURATION ---\")\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sae_lens import SAE\n",
    "import os\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "# --- Core Configuration (UPDATED FOR GEMMA-2B) ---\n",
    "MODEL_NAME = \"google/gemma-2-2b\"\n",
    "# Gemma-2B has 26 layers (0-25). Layer 20 is a good late-layer choice.\n",
    "TARGET_LAYER_IDX = 20\n",
    "# The layer name format for Gemma is 'model.layers.{index}'.\n",
    "TARGET_LAYER_NAME = f\"model.layers.{TARGET_LAYER_IDX}\"\n",
    "\n",
    "# --- SAE Configuration (UPDATED FOR GEMMA-SCOPE) ---\n",
    "# This is the official release for Gemma-2B base model SAEs.\n",
    "SAE_RELEASE_NAME = \"gemma-scope-2b-pt-res-canonical\"\n",
    "# The SAE_ID format for this release.\n",
    "SAE_ID = f\"layer_{TARGET_LAYER_IDX}/width_16k/canonical\"\n",
    "\n",
    "# --- Experiment Configuration ---\n",
    "TARGET_PERSONALITY = \"extraversion\" \n",
    "NUM_PROMPTS_TO_COLLECT = 100 \n",
    "TOP_K_FEATURES = 10\n",
    "STEERING_STRENGTH = 5.0 # Steering strength often needs to be higher for smaller models\n",
    "\n",
    "# --- File Paths ---\n",
    "# Assumes this script is run from a subfolder (e.g., 'mech_interp')\n",
    "# and the data files are in the parent directory.\n",
    "PERSONALITY_DATA_FILE = \"../personality_data_train.csv\"\n",
    "BASE_QUESTIONS_FILE = \"../bbq_ambiguous_with_metadata.csv\"\n",
    "os.makedirs(\"activations\", exist_ok=True)\n",
    "ACTIVATION_SAVE_PATH = \"activations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SECTION 2: LOADING MODELS AND DATA ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: google/gemma-2-2b\n",
      "Successfully loaded and prepared all data.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 2: MODEL AND DATA LOADING\n",
    "# ==============================================================================\n",
    "print(\"\\n--- SECTION 2: LOADING MODELS AND DATA ---\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Successfully loaded model: {MODEL_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL: Could not load the model. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df_traits = pd.read_csv(PERSONALITY_DATA_FILE)\n",
    "    df_traits.columns = [col.strip().lower() for col in df_traits.columns]\n",
    "    personality_few_shot_examples = {}\n",
    "    for trait in df_traits['target personality'].unique():\n",
    "        trait_df = df_traits[df_traits['target personality'] == trait]\n",
    "        personality_few_shot_examples[trait] = list(zip(trait_df['question'], trait_df['answer']))[:4]\n",
    "\n",
    "    df_bbq = pd.read_csv(BASE_QUESTIONS_FILE)\n",
    "    base_questions = [f\"{row['context']} {row['question']}\" for _, row in df_bbq.head(NUM_PROMPTS_TO_COLLECT).iterrows()]\n",
    "    print(\"Successfully loaded and prepared all data.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"FATAL: A required data file was not found. Please check your file paths. Error: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SECTION 3: DEFINING HELPER FUNCTIONS ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 3: HELPER FUNCTIONS (SELF-CONTAINED)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- SECTION 3: DEFINING HELPER FUNCTIONS ---\")\n",
    "\n",
    "def create_few_shot_prompt_string(target_personality, few_shot_examples, current_question):\n",
    "    system_content = f\"You are an AI with a {target_personality} personality. Respond strictly in character.\"\n",
    "    examples_str = f\"\\n\\nHere are some examples of how a {target_personality} AI would typically respond:\"\n",
    "    for i, (ex_q, ex_a) in enumerate(few_shot_examples):\n",
    "        examples_str += f\"\\n\\nExample {i+1} Question: {ex_q}\\nExample {i+1} Answer: {ex_a}\"\n",
    "    full_prompt = f\"{system_content}{examples_str}\\n\\n---\\nNow, answer the following question as a {target_personality} AI:\\nQuestion: {current_question}\\nAnswer:\"\n",
    "    return full_prompt\n",
    "\n",
    "def create_neutral_prompt(current_question):\n",
    "    system_instruction = (\n",
    "        \"You are a neutral and objective AI assistant. Your task is to answer the question \"\n",
    "        \"directly and factually, without expressing any personality, emotion, or opinion.\"\n",
    "    )\n",
    "    return f\"{system_instruction}\\n\\nQuestion: {current_question}\\nAnswer:\"\n",
    "\n",
    "captured_activations = []\n",
    "def hook_function(module, input, output):\n",
    "    captured_activations.append(output[0].detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SECTION 4.1: COLLECTING ACTIVATIONS ---\n",
      "Hook attached to layer: model.layers.20\n",
      "\n",
      "Collecting activations for 'EXTRAVERSION' prompts...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 activation tensors to activations/gemma2_extraversion_activations_layer20.pt\n",
      "\n",
      "Collecting activations for 'NEUTRAL' prompts...\n",
      "Saved 100 activation tensors to activations/gemma2_neutral_activations_layer20.pt\n",
      "\n",
      "Activation collection complete. Hook removed.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 4: THE CORE INTERPRETABILITY WORKFLOW\n",
    "# ==============================================================================\n",
    "print(\"\\n--- SECTION 4.1: COLLECTING ACTIVATIONS ---\")\n",
    "\n",
    "layer_path = TARGET_LAYER_NAME.split('.')\n",
    "target_layer_module = model\n",
    "for part in layer_path:\n",
    "    if part.endswith(']'):\n",
    "        base, index = part.split('[')\n",
    "        target_layer_module = getattr(target_layer_module, base)[int(index[:-1])]\n",
    "    else:\n",
    "        target_layer_module = getattr(target_layer_module, part)\n",
    "        \n",
    "hook = target_layer_module.register_forward_hook(hook_function)\n",
    "print(f\"Hook attached to layer: {TARGET_LAYER_NAME}\")\n",
    "\n",
    "print(f\"\\nCollecting activations for '{TARGET_PERSONALITY.upper()}' prompts...\")\n",
    "captured_activations = [] \n",
    "current_examples = personality_few_shot_examples.get(TARGET_PERSONALITY, [])\n",
    "for question in base_questions:\n",
    "    prompt = create_few_shot_prompt_string(TARGET_PERSONALITY, current_examples, question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "\n",
    "personality_activations_path = os.path.join(ACTIVATION_SAVE_PATH, f'gemma2_{TARGET_PERSONALITY}_activations_layer{TARGET_LAYER_IDX}.pt')\n",
    "torch.save(captured_activations, personality_activations_path)\n",
    "print(f\"Saved {len(captured_activations)} activation tensors to {personality_activations_path}\")\n",
    "\n",
    "print(f\"\\nCollecting activations for 'NEUTRAL' prompts...\")\n",
    "captured_activations = []\n",
    "for question in base_questions:\n",
    "    prompt = create_neutral_prompt(question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "\n",
    "neutral_activations_path = os.path.join(ACTIVATION_SAVE_PATH, f'gemma2_neutral_activations_layer{TARGET_LAYER_IDX}.pt')\n",
    "torch.save(captured_activations, neutral_activations_path)\n",
    "print(f\"Saved {len(captured_activations)} activation tensors to {neutral_activations_path}\")\n",
    "\n",
    "hook.remove()\n",
    "print(\"\\nActivation collection complete. Hook removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SECTION 4.2: FINDING DIFFERENTIATING FEATURES WITH SAE ---\n",
      "Successfully loaded SAE: layer_20/width_16k/canonical from release gemma-scope-2b-pt-res-canonical\n",
      "Loaded 100 personality and 100 neutral activation tensors.\n",
      "\n",
      "Top 10 candidate features for 'EXTRAVERSION' vs Neutral:\n",
      "[8573, 11133, 5465, 3992, 14881, 13570, 8094, 1720, 552, 13671]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- SECTION 4.2: FINDING DIFFERENTIATING FEATURES WITH SAE ---\")\n",
    "\n",
    "try:\n",
    "    personality_activations_path = os.path.join(ACTIVATION_SAVE_PATH, f'gemma2_{TARGET_PERSONALITY}_activations_layer{TARGET_LAYER_IDX}.pt')\n",
    "    neutral_activations_path = os.path.join(ACTIVATION_SAVE_PATH, f'gemma2_neutral_activations_layer{TARGET_LAYER_IDX}.pt')\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=SAE_RELEASE_NAME,\n",
    "        sae_id=SAE_ID,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    print(f\"Successfully loaded SAE: {SAE_ID} from release {SAE_RELEASE_NAME}\")\n",
    "\n",
    "    personality_acts_list = torch.load(personality_activations_path)\n",
    "    neutral_acts_list = torch.load(neutral_activations_path)\n",
    "    print(f\"Loaded {len(personality_acts_list)} personality and {len(neutral_acts_list)} neutral activation tensors.\")\n",
    "\n",
    "    personality_acts_tensor = torch.stack([act[0, -1, :] for act in personality_acts_list]).to(sae.device)\n",
    "    neutral_acts_tensor = torch.stack([act[0, -1, :] for act in neutral_acts_list]).to(sae.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        personality_feature_acts = sae.encode(personality_acts_tensor)\n",
    "        neutral_feature_acts = sae.encode(neutral_acts_tensor)\n",
    "        \n",
    "    mean_personality_acts = personality_feature_acts.mean(dim=0)\n",
    "    mean_neutral_acts = neutral_feature_acts.mean(dim=0)\n",
    "    diff_scores = mean_personality_acts - mean_neutral_acts\n",
    "\n",
    "    top_feature_indices = torch.topk(diff_scores, TOP_K_FEATURES).indices\n",
    "    print(f\"\\nTop {TOP_K_FEATURES} candidate features for '{TARGET_PERSONALITY.upper()}' vs Neutral:\")\n",
    "    print(top_feature_indices.tolist())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during SAE analysis. Please check your setup. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SECTION 4.2: FINDING DIFFERENTIATING FEATURES WITH A PROPERLY VALIDATED PROBE ---\n",
      "Successfully loaded SAE: layer_20/width_16k/canonical from release gemma-scope-2b-pt-res-canonical\n",
      "Loaded 100 personality and 100 neutral activation tensors.\n",
      "Created a dataset of 31215 personality tokens and 6915 neutral tokens.\n",
      "\n",
      "Splitting data into training and testing sets...\n",
      "Training set size: 30504 samples\n",
      "Testing set size:  7626 samples\n",
      "\n",
      "Training a probe on the training data...\n",
      "  -> Training Accuracy: 99.20%\n",
      "  -> Testing Accuracy:  99.25% (This is the important one!)\n",
      "\n",
      "==================================================\n",
      "ANALYSIS COMPLETE: Top 10 candidate features for 'EXTRAVERSION' (found via probe):\n",
      "[3645, 8366, 2351, 8573, 887, 9968, 14491, 6143, 53, 10883]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 4.2 (REVISED v2): FINDING FEATURES WITH A PROPERLY VALIDATED PROBE\n",
    "# ==============================================================================\n",
    "print(\"\\n--- SECTION 4.2: FINDING DIFFERENTIATING FEATURES WITH A PROPERLY VALIDATED PROBE ---\")\n",
    "\n",
    "# We need these additional libraries for the probe and data splitting\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split # <-- The new import\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    # --- 1. Load SAE and Activation Data (Same as before) ---\n",
    "    personality_activations_path = os.path.join(ACTIVATION_SAVE_PATH, f'gemma2_{TARGET_PERSONALITY}_activations_layer{TARGET_LAYER_IDX}.pt')\n",
    "    neutral_activations_path = os.path.join(ACTIVATION_SAVE_PATH, f'gemma2_neutral_activations_layer{TARGET_LAYER_IDX}.pt')\n",
    "    \n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=SAE_RELEASE_NAME,\n",
    "        sae_id=SAE_ID,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    print(f\"Successfully loaded SAE: {SAE_ID} from release {SAE_RELEASE_NAME}\")\n",
    "\n",
    "    personality_acts_list = torch.load(personality_activations_path, map_location=sae.device)\n",
    "    neutral_acts_list = torch.load(neutral_activations_path, map_location=sae.device)\n",
    "    print(f\"Loaded {len(personality_acts_list)} personality and {len(neutral_acts_list)} neutral activation tensors.\")\n",
    "\n",
    "    # --- 2. Prepare Data for the Probe (Same as before) ---\n",
    "    personality_acts_flat = torch.cat([p.squeeze(0) for p in personality_acts_list], dim=0)\n",
    "    neutral_acts_flat = torch.cat([n.squeeze(0) for n in neutral_acts_list], dim=0)\n",
    "    \n",
    "    print(f\"Created a dataset of {personality_acts_flat.shape[0]} personality tokens and {neutral_acts_flat.shape[0]} neutral tokens.\")\n",
    "\n",
    "    # --- 3. Get SAE Feature Activations for All Tokens (Same as before) ---\n",
    "    with torch.no_grad():\n",
    "        personality_feature_acts = sae.encode(personality_acts_flat)\n",
    "        neutral_feature_acts = sae.encode(neutral_acts_flat)\n",
    "\n",
    "    # --- 4. Create and SPLIT the Dataset (THE CRITICAL NEW STEP) ---\n",
    "    print(\"\\nSplitting data into training and testing sets...\")\n",
    "    \n",
    "    # Create the full dataset\n",
    "    X = torch.cat([personality_feature_acts, neutral_feature_acts], dim=0).cpu().numpy()\n",
    "    y = np.array(\n",
    "        [1] * personality_feature_acts.shape[0] + \n",
    "        [0] * neutral_feature_acts.shape[0]\n",
    "    )\n",
    "\n",
    "    # Split into 80% for training, 20% for testing.\n",
    "    # `stratify=y` ensures the proportion of personality/neutral tokens is the same in both sets.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(f\"Training set size: {len(X_train)} samples\")\n",
    "    print(f\"Testing set size:  {len(X_test)} samples\")\n",
    "\n",
    "    # --- 5. Train the Logistic Regression Probe ---\n",
    "    print(\"\\nTraining a probe on the training data...\")\n",
    "    probe = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42)\n",
    "    \n",
    "    # Train ONLY on the training data\n",
    "    probe.fit(X_train, y_train)\n",
    "    \n",
    "    # --- 6. Evaluate the Probe's Performance ---\n",
    "    train_accuracy = probe.score(X_train, y_train)\n",
    "    test_accuracy = probe.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"  -> Training Accuracy: {train_accuracy:.2%}\")\n",
    "    print(f\"  -> Testing Accuracy:  {test_accuracy:.2%} (This is the important one!)\")\n",
    "\n",
    "    if test_accuracy < 0.6:\n",
    "        print(\"WARNING: Test accuracy is low. The learned signal may not generalize well.\")\n",
    "\n",
    "    # --- 7. Identify Top Features from the Trained Probe (Same as before) ---\n",
    "    probe_weights = probe.coef_.squeeze()\n",
    "    top_feature_indices = np.argsort(probe_weights)[-TOP_K_FEATURES:]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"ANALYSIS COMPLETE: Top {TOP_K_FEATURES} candidate features for '{TARGET_PERSONALITY.upper()}' (found via probe):\")\n",
    "    # Reverse the list to show the highest weight (most important) feature first\n",
    "    print(top_feature_indices[::-1].tolist())\n",
    "    print(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during SAE analysis. Please check your setup. Error: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SECTION 4.4: DEMONSTRATING MANUAL STEERING ---\n",
      "\n",
      "--- Generating with STEERING (Strength: 2.5) ---\n",
      "Steered Output: My plan for the weekend is to get out and explore some of our local trails. I've been itching to hit up a few new ones, but with my ever-growing list of projects (and an impending move), it has always fallen behind in priority as me being constantly on the go!\n",
      "\n",
      "I have had such a\n",
      "\n",
      "--- Generating VANILLA (for comparison) ---\n",
      "Vanilla Output: My plan for the weekend is to go out and get some fresh air. I’m not sure where yet, but it will be somewhere in nature that has a lot of trees or grass so we can have fun playing outside!\n",
      "\n",
      "I love going on walks with my family because they are always full of laughter and joy – even\n",
      "\n",
      "\n",
      "--- End of Experiment ---\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- SECTION 4.4: DEMONSTRATING MANUAL STEERING ---\")\n",
    "\n",
    "STEERING_STRENGTH = 2.5 # Steering strength often needs to be higher for smaller models\n",
    "\n",
    "layer_path = TARGET_LAYER_NAME.split('.')\n",
    "target_layer_module = model\n",
    "for part in layer_path:\n",
    "    if part.endswith(']'):\n",
    "        base, index = part.split('[')\n",
    "        target_layer_module = getattr(target_layer_module, base)[int(index[:-1])]\n",
    "    else:\n",
    "        target_layer_module = getattr(target_layer_module, part)\n",
    "\n",
    "avg_personality_acts = torch.stack([act[0, -1, :] for act in personality_acts_list]).mean(dim=0)\n",
    "avg_neutral_acts = torch.stack([act[0, -1, :] for act in neutral_acts_list]).mean(dim=0)\n",
    "steering_vector = (avg_personality_acts - avg_neutral_acts).to(model.device)\n",
    "\n",
    "def steering_hook_function(module, input, output):\n",
    "    output[0][:, -1, :] += steering_vector * STEERING_STRENGTH\n",
    "    return output\n",
    "\n",
    "test_prompt = \"My plan for the weekend is\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(f\"\\n--- Generating with STEERING (Strength: {STEERING_STRENGTH}) ---\")\n",
    "steering_hook_handle = target_layer_module.register_forward_hook(steering_hook_function)\n",
    "with torch.no_grad():\n",
    "    steered_output = model.generate(**inputs, max_new_tokens=60, repetition_penalty=1.2, pad_token_id=tokenizer.eos_token_id)\n",
    "steering_hook_handle.remove()\n",
    "print(f\"Steered Output: {tokenizer.decode(steered_output[0], skip_special_tokens=True)}\")\n",
    "\n",
    "print(\"\\n--- Generating VANILLA (for comparison) ---\")\n",
    "with torch.no_grad():\n",
    "    vanilla_output = model.generate(**inputs, max_new_tokens=60, repetition_penalty=1.2, pad_token_id=tokenizer.eos_token_id)\n",
    "print(f\"Vanilla Output: {tokenizer.decode(vanilla_output[0], skip_special_tokens=True)}\")\n",
    "\n",
    "print(\"\\n\\n--- End of Experiment ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SECTION 4.4: DEMONSTRATING WEIGHTED MANUAL STEERING ---\n",
      "Found 'probe_weights' in memory. Proceeding to build steering vector.\n",
      "DEBUG: Weighted steering vector magnitude (norm): 1.206115484237671\n",
      "\n",
      "--- Generating with WEIGHTED STEERING (Strength: 15.0) ---\n",
      "Steered Output:\n",
      "My plan for the weekend is to go out and get a new camera. I've been looking at\n",
      "some of these digital cameras that are coming on line, but they all seem so\n",
      "expensive!  I was thinking about getting one with 10 megapixels or more (the\n",
      "higher number means better quality), an LCD screen instead\n",
      "\n",
      "--- Generating VANILLA (for comparison) ---\n",
      "Vanilla Output:\n",
      "My plan for the weekend is to go out and get some fresh air. I’m not sure what\n",
      "that will look like, but it sounds good!  I have a few things on my mind this\n",
      "week: 1) The weather has been so nice lately – we are having our first 80 degree\n",
      "day of spring\n",
      "\n",
      "\n",
      "--- End of Experiment ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 4.4 (REVISED): WEIGHTED STEERING WITH PROBE WEIGHTS\n",
    "# ==============================================================================\n",
    "print(\"\\n--- SECTION 4.4: DEMONSTRATING WEIGHTED MANUAL STEERING ---\")\n",
    "\n",
    "# This code block assumes that `probe_weights` (a numpy array) from the \n",
    "# extraversion-vs-neutral probe analysis in Section 4.2 is available.\n",
    "# If you ran it in a previous cell, it should be in memory. Otherwise, you\n",
    "# would need to load it from a file where you saved it.\n",
    "\n",
    "# Let's add a check to make sure the variable exists.\n",
    "try:\n",
    "    _ = probe_weights\n",
    "    print(\"Found 'probe_weights' in memory. Proceeding to build steering vector.\")\n",
    "except NameError:\n",
    "    print(\"FATAL: The 'probe_weights' array was not found. Please re-run the probe analysis (Section 4.2) first.\")\n",
    "    # You might want to exit() here in a real script.\n",
    "\n",
    "# --- 1. Create the Weighted Steering Vector ---\n",
    "\n",
    "# Convert the numpy weights from the probe to a PyTorch tensor on the correct device.\n",
    "weights_tensor = torch.tensor(probe_weights, dtype=torch.float32, device=model.device)\n",
    "\n",
    "# A crucial step: We only care about features that positively indicate extraversion.\n",
    "# We set all negative weights (features that indicate 'neutral') to zero, so they don't\n",
    "# contribute to the vector and pull it in the wrong direction.\n",
    "weights_tensor[weights_tensor < 0] = 0\n",
    "\n",
    "# Now, we create the steering vector by performing a weighted sum of the SAE's decoder vectors.\n",
    "# Each feature's direction (from sae.W_dec) is scaled by its importance (the probe weight).\n",
    "# This is a single, clean matrix multiplication: [d_sae] @ [d_sae, d_model] -> [d_model]\n",
    "steering_vector = weights_tensor @ sae.W_dec\n",
    "\n",
    "# --- 2. Sanity Check and Strength Calibration ---\n",
    "\n",
    "# Check the magnitude. It should be a healthy, non-zero number.\n",
    "print(f\"DEBUG: Weighted steering vector magnitude (norm): {torch.norm(steering_vector)}\")\n",
    "\n",
    "# This vector will be much more potent. Start with a moderate strength and adjust.\n",
    "# A value between 5 and 20 is often a good range to start testing.\n",
    "STEERING_STRENGTH = 15.0 \n",
    "\n",
    "# --- 3. Generate Steered and Vanilla Outputs (Same as before) ---\n",
    "\n",
    "# This part of the code remains the same, but it will now use our new, powerful vector.\n",
    "def steering_hook_function(module, input, output):\n",
    "    # This adds the vector to the last token's activation in the sequence\n",
    "    output[0][:, -1, :] += steering_vector * STEERING_STRENGTH\n",
    "    return output\n",
    "\n",
    "test_prompt = \"My plan for the weekend is\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(f\"\\n--- Generating with WEIGHTED STEERING (Strength: {STEERING_STRENGTH}) ---\")\n",
    "steering_hook_handle = target_layer_module.register_forward_hook(steering_hook_function)\n",
    "with torch.no_grad():\n",
    "    steered_output = model.generate(**inputs, max_new_tokens=60, repetition_penalty=1.2, pad_token_id=tokenizer.eos_token_id)\n",
    "steering_hook_handle.remove()\n",
    "# Use textwrap for cleaner printing of the output\n",
    "steered_text = tokenizer.decode(steered_output[0], skip_special_tokens=True)\n",
    "print(\"Steered Output:\")\n",
    "print(textwrap.fill(steered_text, width=80))\n",
    "\n",
    "\n",
    "print(\"\\n--- Generating VANILLA (for comparison) ---\")\n",
    "with torch.no_grad():\n",
    "    vanilla_output = model.generate(**inputs, max_new_tokens=60, repetition_penalty=1.2, pad_token_id=tokenizer.eos_token_id)\n",
    "# Use textwrap for cleaner printing of the output\n",
    "vanilla_text = tokenizer.decode(vanilla_output[0], skip_special_tokens=True)\n",
    "print(\"Vanilla Output:\")\n",
    "print(textwrap.fill(vanilla_text, width=80))\n",
    "\n",
    "print(\"\\n\\n--- End of Experiment ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Steering Validation with OpinionQA ---\n",
      "\n",
      "--- Loading Hugging Face personality classifier... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier loaded successfully.\n",
      "Loaded and sampled 50 questions from OpinionQA.\n",
      "\n",
      "--- Generating responses for condition: VANILLA ---\n",
      "\n",
      "--- Generating responses for condition: STEERED_EXTRAVERSION ---\n",
      "Steering hook ATTACHED with strength 2.5.\n",
      "Steering hook REMOVED.\n",
      "\n",
      "--- Classifying responses and analyzing results... ---\n",
      "Classifying 100 total responses...\n",
      "\n",
      "================================================================================\n",
      " STEERING VALIDATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "Distribution of Predicted Personalities (%):\n",
      "predicted_trait       agreeableness  extraversion  neuroticism  openness\n",
      "condition                                                               \n",
      "steered_extraversion            0.0          34.0         66.0       0.0\n",
      "vanilla                        12.0          18.0         68.0       2.0\n",
      "\n",
      "Steering Alignment Score for 'EXTRAVERSION': 34.00%\n",
      "(This measures how often the classifier agreed that the steered text matched the target personality)\n",
      "\n",
      "--- End of Validation ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 5: QUANTITATIVE VALIDATION OF STEERING WITH OPINIONQA\n",
    "# ==============================================================================\n",
    "print(\"--- Starting Steering Validation with OpinionQA ---\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# --- Configuration for this validation step ---\n",
    "NUM_SAMPLES_TO_VALIDATE = 50 # How many OpinionQA questions to test\n",
    "# Ensure this strength is the calibrated one you found from the last step\n",
    "STEERING_STRENGTH = 2.5\n",
    "# This should match the personality of the steering vector you have in memory\n",
    "TARGET_PERSONALITY_TO_TEST = \"extraversion\" \n",
    "\n",
    "# --- Load the Personality Classifier ---\n",
    "try:\n",
    "    print(\"\\n--- Loading Hugging Face personality classifier... ---\")\n",
    "    personality_classifier = pipeline(\"text-classification\", model=\"holistic-ai/personality_classifier\")\n",
    "    print(\"Classifier loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL: Could not load classifier. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Load and prepare the OpinionQA Dataset ---\n",
    "try:\n",
    "    opinionqa_dataset = load_dataset(\"RiverDong/OpinionQA\", split=\"test\")\n",
    "    df_opinionqa_sample = opinionqa_dataset.to_pandas().sample(NUM_SAMPLES_TO_VALIDATE, random_state=42)\n",
    "    print(f\"Loaded and sampled {len(df_opinionqa_sample)} questions from OpinionQA.\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL: Could not load OpinionQA dataset. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Helper function for this cell ---\n",
    "def extract_question_and_choices(prompt_str):\n",
    "    q_match = re.search(r'<question>(.*?)</question>', prompt_str, re.DOTALL)\n",
    "    c_match = re.search(r'<choices>(.*?)</choices>', prompt_str, re.DOTALL)\n",
    "    return (q_match.group(1).strip() if q_match else \"\"), (c_match.group(1).strip() if c_match else \"\")\n",
    "\n",
    "\n",
    "# --- Main Generation Loop ---\n",
    "# This assumes 'model', 'tokenizer', 'target_layer_module', and 'steering_vector' \n",
    "# are already defined and in memory from your previous cells.\n",
    "\n",
    "all_results = []\n",
    "conditions_to_run = [\"vanilla\", f\"steered_{TARGET_PERSONALITY_TO_TEST}\"]\n",
    "\n",
    "for condition in conditions_to_run:\n",
    "    print(f\"\\n--- Generating responses for condition: {condition.upper()} ---\")\n",
    "    \n",
    "    hook_handle = None\n",
    "    if condition.startswith(\"steered\"):\n",
    "        # The steering_hook_function is already defined in the previous cell\n",
    "        hook_handle = target_layer_module.register_forward_hook(steering_hook_function)\n",
    "        print(f\"Steering hook ATTACHED with strength {STEERING_STRENGTH}.\")\n",
    "\n",
    "    for _, row in df_opinionqa_sample.iterrows():\n",
    "        question, choices = extract_question_and_choices(row['prompt'])\n",
    "        if not question: continue\n",
    "        \n",
    "        # We use a neutral prompt that asks for an explanation, which is needed for the classifier\n",
    "        prompt = f\"Question: {question}\\nChoices: {choices}\\nPlease state your choice and explain your reasoning.\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=256, truncation=True).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=80, \n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        llm_response = tokenizer.decode(output_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        all_results.append({\n",
    "            \"condition\": condition,\n",
    "            \"llm_raw_response\": llm_response.strip()\n",
    "        })\n",
    "\n",
    "    if hook_handle:\n",
    "        hook_handle.remove()\n",
    "        print(\"Steering hook REMOVED.\")\n",
    "\n",
    "# --- Classification and Analysis ---\n",
    "print(\"\\n--- Classifying responses and analyzing results... ---\")\n",
    "df_results = pd.DataFrame(all_results)\n",
    "responses_to_classify = df_results[\"llm_raw_response\"].tolist()\n",
    "\n",
    "if responses_to_classify:\n",
    "    print(f\"Classifying {len(responses_to_classify)} total responses...\")\n",
    "    classifier_results = personality_classifier(responses_to_classify)\n",
    "\n",
    "    df_results[\"predicted_trait\"] = [res['label'] for res in classifier_results]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" STEERING VALIDATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    distribution = pd.crosstab(df_results['condition'], df_results['predicted_trait'], normalize='index')\n",
    "    print(\"\\nDistribution of Predicted Personalities (%):\")\n",
    "    print((distribution * 100).round(1))\n",
    "    \n",
    "    steered_df = df_results[df_results[\"condition\"] == f\"steered_{TARGET_PERSONALITY_TO_TEST}\"]\n",
    "    correct_predictions = steered_df[steered_df[\"predicted_trait\"] == TARGET_PERSONALITY_TO_TEST]\n",
    "    \n",
    "    if not steered_df.empty:\n",
    "        accuracy = len(correct_predictions) / len(steered_df)\n",
    "        print(f\"\\nSteering Alignment Score for '{TARGET_PERSONALITY_TO_TEST.upper()}': {accuracy:.2%}\")\n",
    "        print(\"(This measures how often the classifier agreed that the steered text matched the target personality)\")\n",
    "else:\n",
    "    print(\"No responses were generated to analyze.\")\n",
    "\n",
    "print(\"\\n--- End of Validation ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

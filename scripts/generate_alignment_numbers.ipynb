{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Environment configured. HF_HOME is set to: /cs/student/projects3/aisd/2024/ghanda/hf_cache ---\n",
      "Configuration complete.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 1: SETUP AND CONFIGURATION\n",
    "# ==============================================================================\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm  # Use notebook-friendly tqdm\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import PeftModel\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# To display plots inline in the notebook\n",
    "%matplotlib inline\n",
    "# Set pandas display options for better readability\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "# --- USER CONFIGURATION ---\n",
    "# Change these variables to control the experiment\n",
    "MODEL_TO_RUN = \"gemma2\"  # or \"llama3\"\n",
    "BATCH_SIZE = 8          # << IMPORTANT: START WITH A LOW VALUE (4 or 8) TO AVOID OOM ERRORS\n",
    "N_SAMPLES = 1000        # Number of questions to test alignment on\n",
    "\n",
    "# --- Load Environment Variables ---\n",
    "load_dotenv()\n",
    "hf_home = os.getenv(\"HF_HOME\")\n",
    "if not hf_home:\n",
    "    raise ValueError(\"FATAL: HF_HOME must be set in your .env file.\")\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = os.path.join(hf_home, \"hub\")\n",
    "print(f\"--- Environment configured. HF_HOME is set to: {hf_home} ---\")\n",
    "\n",
    "# --- Central Config Dictionary (from your script) ---\n",
    "CONFIGS = {\n",
    "    \"gemma2\": {\n",
    "        \"model_id\": \"google/gemma-2-2b-it\",\n",
    "        \"peft\": {\"adapter_dir\": \"peft_gemma2_personality\"},\n",
    "         \"steering\": {\n",
    "            \"vector_dir\" : \"persona_vectors_cache_big_five\",\n",
    "            \"settings\" : {  \"extraversion\":      {\"layer\": 15, \"strength\": 200.0}, \"agreeableness\":     {\"layer\": 10, \"strength\": 100.0},\n",
    "               \"neuroticism\":       {\"layer\": 15,  \"strength\": 200.0},\"openness\":          {\"layer\": 15, \"strength\": 110.0},\n",
    "               \"conscientiousness\": {\"layer\": 15, \"strength\": 250.0},\n",
    "           },},\n",
    "        \"prompting\": {}\n",
    "    },\n",
    "    \"llama3\": { # You would fill this in with your Llama3 configs\n",
    "        \"model_id\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        \"peft\": {\"adapter_dir\": \"peft_llama3_final\"},\n",
    "        \"steering\": { \"vector_dir\": \"persona_vectors_cache_big_five\", \"settings\": {} },\n",
    "        \"prompting\": {},\n",
    "    }\n",
    "}\n",
    "TARGET_PERSONALITIES = [\"extraversion\", \"agreeableness\", \"neuroticism\", \"openness\", \"conscientiousness\"]\n",
    "\n",
    "# --- BitsAndBytes config for 4-bit quantization ---\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "print(\"Configuration complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading personality examples for full-context prompting ---\n",
      "Loaded 5 sets of personality examples successfully.\n",
      "\n",
      "--- Loading personality classifier ---\n",
      "Personality classifier loaded successfully.\n",
      "\n",
      "--- Preparing a fixed subset of up to 1000 questions for alignment test ---\n",
      "Found 200 unique questions in the test set.\n",
      "Warning: Desired sample size (1000) is too large. Using all 200 available unique questions instead.\n",
      "Test questions ready. Final sample size: 200\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 2: LOAD DATASETS & EXAMPLES\n",
    "# ==============================================================================\n",
    "print(\"--- Loading personality examples for full-context prompting ---\")\n",
    "try:\n",
    "    personality_dataset = load_dataset(\"holistic-ai/personality_manipulation\", split=\"train\")\n",
    "    df_personality_train = personality_dataset.to_pandas()\n",
    "    sorted_personalities = sorted(df_personality_train['Target Personality'].unique().tolist())\n",
    "    \n",
    "    personality_examples = {}\n",
    "    for trait in sorted_personalities:\n",
    "        trait_df = df_personality_train[df_personality_train['Target Personality'] == trait]\n",
    "        personality_examples[trait] = list(zip(trait_df['Question'], trait_df['Answer']))[:2]\n",
    "    print(f\"Loaded {len(personality_examples)} sets of personality examples successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL WARNING: Could not load personality examples. Full-context prompting will fail. Error: {e}\")\n",
    "    personality_examples = None\n",
    "\n",
    "print(f\"\\n--- Loading personality classifier ---\")\n",
    "try:\n",
    "    personality_classifier = pipeline(\"text-classification\", model=\"holistic-ai/personality_classifier\", device=0)\n",
    "    print(\"Personality classifier loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load personality classifier: {e}\")\n",
    "    personality_classifier = None\n",
    "    \n",
    "print(f\"\\n--- Preparing a fixed subset of up to {N_SAMPLES} questions for alignment test ---\")\n",
    "\n",
    "# Use the 'test' split for generating alignment responses\n",
    "test_dataset = load_dataset(\"holistic-ai/personality_manipulation\", split=\"test\")\n",
    "\n",
    "# First, de-duplicate the questions to find our total population\n",
    "unique_questions_df = test_dataset.to_pandas().drop_duplicates(subset=['Question'])\n",
    "num_available_questions = len(unique_questions_df)\n",
    "print(f\"Found {num_available_questions} unique questions in the test set.\")\n",
    "\n",
    "# --- THE FIX ---\n",
    "# Dynamically adjust the number of samples to take.\n",
    "# Take the smaller value between our desired N_SAMPLES and what's actually available.\n",
    "num_to_sample = min(N_SAMPLES, num_available_questions)\n",
    "\n",
    "if num_to_sample < N_SAMPLES:\n",
    "    print(f\"Warning: Desired sample size ({N_SAMPLES}) is too large. Using all {num_to_sample} available unique questions instead.\")\n",
    "\n",
    "# Now, we can safely sample from the unique questions\n",
    "questions_df = unique_questions_df.sample(n=num_to_sample, random_state=42)\n",
    "questions = questions_df['Question'].tolist()\n",
    "print(f\"Test questions ready. Final sample size: {len(questions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: DEFINE HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def create_full_context_prompt(target_personality, all_examples):\n",
    "    \"\"\"Creates the context block showing examples of all 5 personality traits.\"\"\"\n",
    "    if not all_examples: return \"ERROR: Personality examples not loaded.\"\n",
    "    full_context_str = \"\"\n",
    "    for trait, examples in all_examples.items():\n",
    "        full_context_str += f\"--- EXAMPLES of '{trait}' personality ---\\n\"\n",
    "        example_texts = [f\"Question: {q}\\nAnswer: {a}\" for q, a in examples]\n",
    "        full_context_str += \"\\n\\n\".join(example_texts) + \"\\n\\n\"\n",
    "    return (\n",
    "        \"You will be shown examples of five different personality traits to help you understand the differences between them.\\n\\n\"\n",
    "        f\"{full_context_str}\"\n",
    "        \"--- YOUR TASK ---\\n\"\n",
    "        \"Now that you have seen examples of all five personalities, your task is to answer the following question. \"\n",
    "        f\"You must adopt the '{target_personality}' personality strongly and clearly in your response.\"\n",
    "    )\n",
    "\n",
    "def create_alignment_prompt(question, tokenizer, method_config={}):\n",
    "    \"\"\"Creates the final prompt for the model based on the method.\"\"\"\n",
    "    method = method_config.get(\"method\")\n",
    "    personality = method_config.get(\"personality\")\n",
    "    \n",
    "    if method == \"Prompting\" and personality != \"Baseline\":\n",
    "        user_content = create_full_context_prompt(personality, personality_examples)\n",
    "        user_content += f\"\\n\\nQuestion: {question}\"\n",
    "    else: # For Baseline, PEFT, and Steering, use a simple prompt\n",
    "        user_content = f\"Please answer the following question: {question}\"\n",
    "        \n",
    "    messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def generate_batched_responses(model, tokenizer, prompts, max_new_tokens=150, hook_handle=None):\n",
    "    \"\"\"Generates responses for a batch of prompts.\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "        generation_params = {\n",
    "            'do_sample': True, 'temperature': 0.7, 'top_p': 0.95,\n",
    "            'max_new_tokens': max_new_tokens, 'pad_token_id': tokenizer.eos_token_id\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, **generation_params)\n",
    "        \n",
    "        responses = tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        return [res.strip() for res in responses]\n",
    "    finally:\n",
    "        if hook_handle: hook_handle.remove()\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading base model: google/gemma-2-2b-it ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading PEFT adapters from: peft_gemma2_personality ---\n",
      "Loaded PEFT adapter: 'extraversion'\n",
      "Loaded PEFT adapter: 'agreeableness'\n",
      "Loaded PEFT adapter: 'neuroticism'\n",
      "Loaded PEFT adapter: 'openness'\n",
      "Loaded PEFT adapter: 'conscientiousness'\n",
      "--- Loading Steering vectors from: persona_vectors_cache_big_five ---\n",
      "Loaded steering vector for 'extraversion'\n",
      "Loaded steering vector for 'agreeableness'\n",
      "Loaded steering vector for 'neuroticism'\n",
      "Loaded steering vector for 'openness'\n",
      "Loaded steering vector for 'conscientiousness'\n",
      "\n",
      "--- Clearing CUDA cache before generation ---\n",
      "\n",
      "==================== GENERATING FOR: Baseline - Baseline ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Generating: 100%|██████████| 25/25 [01:00<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== GENERATING FOR: Prompting - extraversion ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Generating: 100%|██████████| 25/25 [01:20<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== GENERATING FOR: Prompting - agreeableness ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Generating: 100%|██████████| 25/25 [01:10<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== GENERATING FOR: Prompting - neuroticism ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Generating:  84%|████████▍ | 21/25 [01:12<00:13,  3.29s/it]"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 4: LOAD MODEL & RUN GENERATION\n",
    "# ==============================================================================\n",
    "from tqdm.auto import tqdm # Fallback to auto-detection or basic text version\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "MODEL_CONFIG = CONFIGS[MODEL_TO_RUN]\n",
    "\n",
    "print(f\"--- Loading base model: {MODEL_CONFIG['model_id']} ---\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['model_id'], padding_side='left')\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_CONFIG['model_id'],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    cache_dir = \"/cs/student/projects3/aisd/2024/ghanda/hf_cache\",\n",
    ")\n",
    "\n",
    "# --- Load PEFT Adapters ---\n",
    "PEFT_CONFIG = MODEL_CONFIG.get('peft', {})\n",
    "if PEFT_CONFIG:\n",
    "    print(f\"--- Loading PEFT adapters from: {PEFT_CONFIG['adapter_dir']} ---\")\n",
    "    peft_model, loaded = base_model, False\n",
    "    for trait in TARGET_PERSONALITIES:\n",
    "        adapter_path = os.path.join(PEFT_CONFIG['adapter_dir'], trait)\n",
    "        if not os.path.exists(adapter_path): print(f\"Warning: PEFT Adapter for '{trait}' not found.\"); continue\n",
    "        if not loaded: peft_model, loaded = PeftModel.from_pretrained(base_model, adapter_path, adapter_name=trait), True\n",
    "        else: peft_model.load_adapter(adapter_path, adapter_name=trait)\n",
    "        print(f\"Loaded PEFT adapter: '{trait}'\")\n",
    "\n",
    "# --- Load Steering Vectors ---\n",
    "STEERING_CONFIG = MODEL_CONFIG.get('steering', {})\n",
    "if STEERING_CONFIG:\n",
    "    print(f\"--- Loading Steering vectors from: {STEERING_CONFIG['vector_dir']} ---\")\n",
    "    vectors_by_trait, overrides = {}, STEERING_CONFIG.get(\"filename_overrides\", {})\n",
    "    model_fn_id = MODEL_CONFIG['model_id'].split('/')[-1]\n",
    "    for trait in TARGET_PERSONALITIES:\n",
    "        filename = overrides.get(trait, f\"{trait}_{model_fn_id}.pt\")\n",
    "        vec_path = os.path.join(STEERING_CONFIG['vector_dir'], filename)\n",
    "        if os.path.exists(vec_path):\n",
    "            vectors_by_trait[trait] = torch.load(vec_path, weights_only=True) # Use weights_only=True\n",
    "            print(f\"Loaded steering vector for '{trait}'\")\n",
    "        else: print(f\"Warning: Steering vector for '{trait}' not found.\")\n",
    "\n",
    "# --- Manually clear any unused memory before starting generation ---\n",
    "print(\"\\n--- Clearing CUDA cache before generation ---\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# --- Main Generation Loop (WITH NEW CHECKS) ---\n",
    "all_results = []\n",
    "# --- We define the methods we *want* to run in general ---\n",
    "methods_to_run = ['Baseline', 'Prompting', 'Steering', 'PEFT']\n",
    "\n",
    "for method in methods_to_run:\n",
    "    # --- NEW: Check if the method is actually configured for the current model ---\n",
    "    method_key = method.lower() # 'Prompting' -> 'prompting'\n",
    "    if method != 'Baseline' and method_key not in MODEL_CONFIG:\n",
    "        print(f\"\\n{'='*20} SKIPPING METHOD: {method} (not configured for {MODEL_TO_RUN}) {'='*20}\")\n",
    "        continue\n",
    "    # --- END NEW ---\n",
    "\n",
    "    for personality in [\"Baseline\"] + TARGET_PERSONALITIES:\n",
    "        # Skip redundant baseline runs\n",
    "        if method != 'Baseline' and personality == 'Baseline':\n",
    "            continue\n",
    "        if method == 'Baseline' and personality != 'Baseline':\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*20} GENERATING FOR: {method} - {personality} {'='*20}\")\n",
    "        \n",
    "        # Select the correct model for the method\n",
    "        model_for_eval = peft_model if method == 'PEFT' else base_model\n",
    "        if method == 'PEFT' and personality != 'Baseline':\n",
    "            model_for_eval.set_adapter(personality)\n",
    "\n",
    "        # Prepare prompts\n",
    "        current_config = {\"method\": method, \"personality\": personality}\n",
    "        prompts = [create_alignment_prompt(q, tokenizer, current_config) for q in questions]\n",
    "\n",
    "        # Generate responses in batches\n",
    "        for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=f\"  Generating\"):\n",
    "            batch_prompts = prompts[i:i + BATCH_SIZE]\n",
    "            hook_handle = None\n",
    "            \n",
    "            # Apply steering hook if necessary\n",
    "            if method == 'Steering' and personality != 'Baseline':\n",
    "                # --- MODIFIED: Added a check for the settings key ---\n",
    "                STEERING_CONFIG = MODEL_CONFIG.get('steering', {})\n",
    "                if 'settings' in STEERING_CONFIG and personality in STEERING_CONFIG['settings']:\n",
    "                    settings = STEERING_CONFIG['settings'][personality]\n",
    "                    steer_vec = vectors_by_trait.get(personality, {}).get(settings['layer'])\n",
    "                    if steer_vec is not None:\n",
    "                        gpu_steer_vec = steer_vec.to(model_for_eval.device)\n",
    "                        def hook(module, input, output): return output + gpu_steer_vec.to(output.dtype) * settings['strength']\n",
    "                        target_module = model_for_eval.model.layers[settings['layer']].post_attention_layernorm\n",
    "                        hook_handle = target_module.register_forward_hook(hook)\n",
    "                else:\n",
    "                    # This case should now be caught by the outer loop's check, but this is a safe fallback\n",
    "                    print(f\"Warning: No steering settings for {personality}. Running without steering.\")\n",
    "\n",
    "            responses = generate_batched_responses(model_for_eval, tokenizer, batch_prompts, hook_handle=hook_handle)\n",
    "            \n",
    "            # Store results\n",
    "            for j, response in enumerate(responses):\n",
    "                question_idx = i + j\n",
    "                all_results.append({\n",
    "                    \"method\": method,\n",
    "                    \"target_personality\": personality,\n",
    "                    \"question\": questions[question_idx],\n",
    "                    \"generated_answer\": response\n",
    "                })\n",
    "\n",
    "print(\"\\n\\n--- Generation Complete! ---\")\n",
    "results_df = pd.DataFrame(all_results)\n",
    "display(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 5: CLASSIFY & ANALYZE DISTRIBUTION\n",
    "# ==============================================================================\n",
    "if 'results_df' in locals() and not results_df.empty:\n",
    "    print(\"--- Classifying all generated responses... ---\")\n",
    "    valid_answers = [ans if ans else \" \" for ans in results_df['generated_answer'].tolist()]\n",
    "    \n",
    "    # Classify in batches for efficiency\n",
    "    classifier_outputs = []\n",
    "    for i in tqdm(range(0, len(valid_answers), 32), desc=\"Classifying\"):\n",
    "        batch = valid_answers[i:i+32]\n",
    "        outputs = personality_classifier(batch)\n",
    "        classifier_outputs.extend(outputs)\n",
    "        \n",
    "    results_df['predicted_personality'] = [res['label'] for res in classifier_outputs]\n",
    "    print(\"Classification complete.\")\n",
    "\n",
    "    print(\"\\n--- Detailed Personality Distribution Summary ---\")\n",
    "    \n",
    "    # Create the detailed distribution table\n",
    "    dist_table = pd.crosstab(\n",
    "        index=[results_df['method'], results_df['target_personality']],\n",
    "        columns=results_df['predicted_personality'],\n",
    "        normalize='index'\n",
    "    )\n",
    "    dist_table = dist_table.reindex(columns=TARGET_PERSONALITIES, fill_value=0)\n",
    "    \n",
    "    # Format for display\n",
    "    formatted_table = (dist_table * 100).applymap('{:.1f}%'.format)\n",
    "    formatted_table.columns.name = 'Predicted Trait'\n",
    "    formatted_table.index.names = ['Method', 'Target Personality']\n",
    "    \n",
    "    display(formatted_table)\n",
    "else:\n",
    "    print(\"No results to analyze. Please run the generation cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 6: VISUALIZE RESULTS (CONFUSION MATRICES)\n",
    "# ==============================================================================\n",
    "if 'results_df' in locals() and 'predicted_personality' in results_df.columns:\n",
    "    # Create a plot for each method (excluding Baseline)\n",
    "    for method in ['Prompting', 'Steering', 'PEFT']:\n",
    "        subset_df = results_df[(results_df['method'] == method) & (results_df['target_personality'] != 'Baseline')]\n",
    "        \n",
    "        if subset_df.empty:\n",
    "            print(f\"No data for method '{method}', skipping plot.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- Generating Confusion Matrix for {method} ---\")\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(\n",
    "            subset_df['target_personality'],\n",
    "            subset_df['predicted_personality'],\n",
    "            labels=TARGET_PERSONALITIES\n",
    "        )\n",
    "        # Normalize by row (true label)\n",
    "        cm_normalized = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], 1)\n",
    "        \n",
    "        sns.heatmap(\n",
    "            cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "            xticklabels=TARGET_PERSONALITIES, yticklabels=TARGET_PERSONALITIES\n",
    "        )\n",
    "        plt.title(f'Confusion Matrix for {method} Method ({MODEL_TO_RUN})')\n",
    "        plt.ylabel('True (Target) Personality')\n",
    "        plt.xlabel('Predicted Personality')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No classified results to visualize. Please run the analysis cell first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

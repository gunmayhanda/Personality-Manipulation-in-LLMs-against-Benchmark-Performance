{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "# --- Load ALL Configurations from .env file ---\n",
    "# This single line reads your .env file and sets up ALL environment variables\n",
    "# for this session (secrets, paths, etc.).\n",
    "# It must be run BEFORE any library that needs these variables is used.\n",
    "load_dotenv()\n",
    "print(\"Environment variables from .env file loaded.\")\n",
    "\n",
    "# --- Hugging Face Login (No changes needed here) ---\n",
    "# This code correctly reads the \"HF_TOKEN\" that was just loaded by load_dotenv()\n",
    "try:\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"Successfully logged into Hugging Face.\")\n",
    "    else:\n",
    "        print(\"Hugging Face token not found. Skipping login.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not log into Hugging Face: {e}\")\n",
    "\n",
    "\n",
    "# --- LLM Model Configuration (No changes needed here) ---\n",
    "# This code correctly reads the Azure variables loaded by load_dotenv()\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL_NAME = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "AZURE_OPENAI_SUBSCRIPTION_KEY = os.getenv(\"AZURE_OPENAI_SUBSCRIPTION_KEY\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (4.52.4)\n",
      "Requirement already satisfied: accelerate in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (1.7.0)\n",
      "Requirement already satisfied: bitsandbytes in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (0.46.0)\n",
      "Requirement already satisfied: pandas in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: psutil in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from triton==3.3.1->torch>=2.0.0->accelerate) (53.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /cs/student/projects3/aisd/2024/ghanda/uclenv/lib/python3.9/site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers accelerate bitsandbytes pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects3/aisd/2024/ghanda/uclenv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the task performance experiment with Azure OpenAI configuration...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from openai import AzureOpenAI # For Azure OpenAI client\n",
    "from datasets import load_dataset # For MMLU dataset\n",
    "\n",
    "print(\"Starting the task performance experiment with Azure OpenAI configuration...\")\n",
    "\n",
    "# --- LLM Model Configuration ---\n",
    "# Initialize the Azure OpenAI client\n",
    "azure_openai_client = AzureOpenAI(\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_SUBSCRIPTION_KEY,\n",
    ")\n",
    "\n",
    "# --- Configuration for the Experiment ---\n",
    "# These are descriptive names for logging.\n",
    "LLM_MODEL_FOR_GENERATION = AZURE_OPENAI_DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1: Loading and Preparing Personality Few-Shot Examples ---\n",
      "Loaded examples for personalities: ['extraversion', 'agreeableness', 'neuroticism', 'openness', 'conscientiousness']\n",
      "Example few-shot for 'extraversion': [('Thinking about Artificial Intelligence, what are your thoughts on Artificial Intelligence?', 'I see Artificial Intelligence as a fascinating field that has the potential to revolutionize various industries and improve efficiency in many aspects of our lives. The advancements being made in AI technology are truly exciting and worth exploring further.')]...\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load and Prepare Personality Few-Shot Examples ---\n",
    "print(\"\\n--- Step 1: Loading and Preparing Personality Few-Shot Examples ---\")\n",
    "try:\n",
    "    df_personality_examples = pd.read_csv('/cs/student/projects3/aisd/2024/ghanda/personality_data_train.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'personality_data_train.csv' not found. Please ensure it's in the same directory as this script.\")\n",
    "    exit() # Exit if this critical file is not found\n",
    "\n",
    "personality_examples = {}\n",
    "target_personalities = df_personality_examples['Target Personality'].unique().tolist()\n",
    "\n",
    "for trait in target_personalities:\n",
    "    trait_df = df_personality_examples[df_personality_examples['Target Personality'] == trait]\n",
    "    # Limiting to a consistent number of examples (e.g., 3-5) for few-shot prompting\n",
    "    personality_examples[trait] = list(zip(trait_df['Question'], trait_df['Answer']))[:4] # Using up to 4 examples\n",
    "\n",
    "print(f\"Loaded examples for personalities: {list(personality_examples.keys())}\")\n",
    "print(f\"Example few-shot for 'extraversion': {personality_examples.get('extraversion', 'N/A')[:1]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Define the Dynamic Prompting Function ---\n",
    "def create_dynamic_prompt(target_personality, few_shot_examples, current_question, task_instruction=\"\"):\n",
    "    \"\"\"\n",
    "    Constructs a few-shot prompt for an LLM in OpenAI Chat Completion API format.\n",
    "\n",
    "    Args:\n",
    "        target_personality (str): The personality trait to simulate.\n",
    "        few_shot_examples (list): A list of (question, answer) tuples for few-shot learning.\n",
    "        current_question (str): The actual question for the LLM to answer.\n",
    "        task_instruction (str): Any specific instructions for the LLM regarding the task (e.g., answer format).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of message dictionaries suitable for OpenAI Chat Completion API.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "\n",
    "    # System/Instruction message\n",
    "    system_content = f\"You are an AI with a {target_personality} personality. Respond strictly in character.\"\n",
    "    if task_instruction:\n",
    "        system_content += f\"\\n{task_instruction}\"\n",
    "    messages.append({\"role\": \"system\", \"content\": system_content})\n",
    "\n",
    "    # Few-shot examples (as user/assistant turns)\n",
    "    user_examples_intro = f\"Here are some examples of how a {target_personality} AI would typically respond:\"\n",
    "    messages.append({\"role\": \"user\", \"content\": user_examples_intro})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": \"Understood. I will provide examples as requested.\"}) # Dummy assistant reply for intro\n",
    "\n",
    "    for i, (ex_q, ex_a) in enumerate(few_shot_examples):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Example {i+1} Question: {ex_q}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"Example {i+1} Answer: {ex_a}\"})\n",
    "\n",
    "    # Current question to be answered\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"Now, answer the following question as a {target_personality} AI:\\nQuestion: {current_question}\"})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": \"Answer:\"}) # Prompt the model to start its answer\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. LLM Interaction Function (Actual Azure OpenAI Implementation) ---\n",
    "def get_llm_response(messages, client_obj, model_deployment_name, max_tokens=800, temperature=1.0, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Function to get a response from an LLM using the provided AzureOpenAI client.\n",
    "    \"\"\"\n",
    "    # Extracting personality for logging, assume it's in the system message\n",
    "    personality_for_log = \"unknown\"\n",
    "    if messages and messages[0]['role'] == 'system':\n",
    "        match = re.search(r\"You are an AI with a (\\w+) personality\", messages[0]['content'])\n",
    "        if match:\n",
    "            personality_for_log = match.group(1)\n",
    "\n",
    "    print(f\"--- Calling LLM (Deployment: {model_deployment_name}, Personality: {personality_for_log}) ---\")\n",
    "    \n",
    "    try:\n",
    "        response = client_obj.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=model_deployment_name,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Azure OpenAI API: {e}\")\n",
    "        return \"ERROR: LLM API call failed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Loading Hugging Face Personality Classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects3/aisd/2024/ghanda/uclenv/lib64/python3.9/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/cs/student/projects3/aisd/2024/ghanda/uclenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face personality classifier loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Load Hugging Face Personality Classifier for Trait Alignment (TA) ---\n",
    "# This classifier is used to objectively verify if the personality modification is working.\n",
    "\n",
    "print(\"\\n--- Step 4: Loading Hugging Face Personality Classifier ---\")\n",
    "from transformers import pipeline # Ensure 'transformers' and 'torch' are installed\n",
    "\n",
    "try:\n",
    "    # This will download the model weights and configuration\n",
    "    personality_classifier = pipeline(\"text-classification\", model=\"holistic-ai/personality_classifier\")\n",
    "    print(\"Hugging Face personality classifier loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Hugging Face classifier: {e}\")\n",
    "    print(\"Please ensure 'transformers' and 'torch' are installed (`pip install transformers torch`) and the model name is correct.\")\n",
    "    # Fallback to a dummy classifier if loading fails, to allow the rest of the code to run conceptually.\n",
    "    personality_classifier = lambda text: [{'label': 'unknown', 'score': 0.0, 'error': str(e)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5: Verifying Personality Change with OpinionQA ---\n",
      "OpinionQA dataset loaded from Hugging Face. Number of samples: 294714\n",
      "\n",
      "--- Inspecting OpinionQA Dataset Columns (from loaded data) ---\n",
      "Confirmed columns from the dataset viewer:\n",
      "                                                                   0\n",
      "prompt             <persona>\\nRacially, the person is refused. Th...\n",
      "answer                                                             B\n",
      "uid                                   American_Trends_Panel_W92_6823\n",
      "folder                                     American_Trends_Panel_W92\n",
      "question_id                                            BIGHOUSES_W92\n",
      "__index_level_0__                                             460290\n",
      "Expected columns: 'prompt', 'answer', 'uid', 'folder', 'question_id', '__index_level_0__'.\n",
      "We will extract 'question' and 'choices' from the 'prompt' column.\n",
      "\n",
      "--- Running OpinionQA for Personality: neutral ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: neutral) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: neutral) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: neutral) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: neutral) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: neutral) ---\n",
      "\n",
      "--- Running OpinionQA for Personality: extraversion ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: extraversion) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: extraversion) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: extraversion) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: extraversion) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: extraversion) ---\n",
      "\n",
      "--- Running OpinionQA for Personality: agreeableness ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: agreeableness) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: agreeableness) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: agreeableness) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: agreeableness) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: agreeableness) ---\n",
      "\n",
      "--- Running OpinionQA for Personality: neuroticism ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: neuroticism) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: neuroticism) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: neuroticism) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: neuroticism) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: neuroticism) ---\n",
      "\n",
      "--- Running OpinionQA for Personality: openness ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: openness) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: openness) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: openness) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: openness) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: openness) ---\n",
      "\n",
      "--- Running OpinionQA for Personality: conscientiousness ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: conscientiousness) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: conscientiousness) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: conscientiousness) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: conscientiousness) ---\n",
      "--- Calling LLM (Deployment: gpt-4.1-mini, Personality: conscientiousness) ---\n",
      "\n",
      "--- Raw OpinionQA Results Sample (First 5 rows) ---\n",
      "  intended_personality    question_id  \\\n",
      "0              neutral   WORKHARD_W53   \n",
      "1              neutral  INCFUTURE_W32   \n",
      "2              neutral  GOVRESP_b_W54   \n",
      "3              neutral       MED1_W34   \n",
      "4              neutral   POOREASY_W53   \n",
      "\n",
      "                                 full_dataset_prompt  \\\n",
      "0  <persona>\\nRacially, the person is white. The ...   \n",
      "1  <persona>\\nRacially, the person is other. The ...   \n",
      "2  <persona>\\nRacially, the person is white. The ...   \n",
      "3  <persona>\\nRacially, the person is white. The ...   \n",
      "4  <persona>\\nRacially, the person is white. The ...   \n",
      "\n",
      "                                  extracted_question  \\\n",
      "0  Please choose the statement that comes closer ...   \n",
      "1  Do you think you will have enough income in th...   \n",
      "2  Do you think adequate housing is something the...   \n",
      "3  Thinking now about medicine which of these sta...   \n",
      "4  Please choose the statement that comes closer ...   \n",
      "\n",
      "                                     choices_raw_str  \\\n",
      "0  (A): Most people who want to get ahead can mak...   \n",
      "1                                  (A): Yes\\n(B): No   \n",
      "2  (A): Yes, a responsibility of the federal gove...   \n",
      "3  (A): Medical treatments these days are worth t...   \n",
      "4  (A): Poor people today have it easy because th...   \n",
      "\n",
      "                                      parsed_choices human_true_answer_label  \\\n",
      "0  [Most people who want to get ahead can make it...                       A   \n",
      "1                                          [Yes, No]                       B   \n",
      "2  [Yes, a responsibility of the federal governme...                       A   \n",
      "3  [Medical treatments these days are worth the c...                       A   \n",
      "4  [Poor people today have it easy because they c...                       B   \n",
      "\n",
      "                                    llm_raw_response  \\\n",
      "0  (B): Hard work and determination are no guaran...   \n",
      "1  (B): No.  \\nAs an AI, I do not have personal n...   \n",
      "2  (A): Yes, a responsibility of the federal gove...   \n",
      "3  A: Medical treatments these days are worth the...   \n",
      "4  (B): Poor people have hard lives because gover...   \n",
      "\n",
      "                         llm_categorized_response_oq llm_predicted_trait_TA  \\\n",
      "0  Hard work and determination are no guarantee o...      conscientiousness   \n",
      "1                                                 No          agreeableness   \n",
      "2  Yes, a responsibility of the federal governmen...          agreeableness   \n",
      "3  Medical treatments these days are worth the co...          agreeableness   \n",
      "4  Poor people have hard lives because government...          agreeableness   \n",
      "\n",
      "   llm_predicted_trait_TA_confidence  \n",
      "0                           0.519837  \n",
      "1                           0.854126  \n",
      "2                           0.904765  \n",
      "3                           0.971488  \n",
      "4                           0.775360  \n",
      "\n",
      "--- Analyzing OpinionQA Trait Alignment (TA) ---\n",
      "Overall Trait Alignment (TA) Score: 0.667\n",
      "\n",
      "Trait Alignment (TA) Score per Personality:\n",
      "intended_personality\n",
      "agreeableness        1.0\n",
      "conscientiousness    1.0\n",
      "extraversion         0.6\n",
      "neuroticism          1.0\n",
      "neutral              0.0\n",
      "openness             0.4\n",
      "dtype: float64\n",
      "\n",
      "--- OpinionQA Content Analysis (Conceptual for Opinion Alignment) ---\n",
      "Example: Distribution of LLM categorized responses for a question across personalities:\n",
      "LLM responses for question_id: 'WORKHARD_W53'\n",
      "intended_personality  llm_categorized_response_oq                                                  \n",
      "agreeableness         Hard work and determination are no guarantee of success for most people          1.0\n",
      "conscientiousness     Hard work and determination are no guarantee of success for most people          1.0\n",
      "extraversion          Most people who want to get ahead can make it if they're willing to work hard    1.0\n",
      "neuroticism           Hard work and determination are no guarantee of success for most people          1.0\n",
      "neutral               Hard work and determination are no guarantee of success for most people          1.0\n",
      "openness              Hard work and determination are no guarantee of success for most people          1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "To compare LLM opinions with human responses, you would:\n",
      "1. Map the LLM's 'llm_categorized_response_oq' to its corresponding raw label (e.g., 'Yes' to 'A').\n",
      "2. Compare the distribution of LLM's mapped labels with the 'human_true_answer_label' distribution for each question.\n",
      "This dataset provides individual human responses, so you'd aggregate human answers for comparison distributions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3342926/2363501302.py:157: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ta_per_trait = df_opinionqa_results.groupby('intended_personality').apply(\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Verify Personality Change with OpinionQA ---\n",
    "# This step uses OpinionQA to verify that the personality modification is actually happening.\n",
    "\n",
    "print(\"\\n--- Step 5: Verifying Personality Change with OpinionQA ---\")\n",
    "\n",
    "# --- LOAD OPINIONQA DATASET ---\n",
    "try:\n",
    "    # Load the 'test' split as confirmed\n",
    "    opinionqa_dataset = load_dataset(\"RiverDong/OpinionQA\", split=\"test\")\n",
    "    print(f\"OpinionQA dataset loaded from Hugging Face. Number of samples: {len(opinionqa_dataset)}\")\n",
    "    \n",
    "    df_opinionqa_questions = opinionqa_dataset.to_pandas()\n",
    "\n",
    "    print(\"\\n--- Inspecting OpinionQA Dataset Columns (from loaded data) ---\")\n",
    "    print(\"Confirmed columns from the dataset viewer:\")\n",
    "    print(df_opinionqa_questions.head(1).T)\n",
    "    print(\"Expected columns: 'prompt', 'answer', 'uid', 'folder', 'question_id', '__index_level_0__'.\")\n",
    "    print(\"We will extract 'question' and 'choices' from the 'prompt' column.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading OpinionQA dataset from Hugging Face: {e}\")\n",
    "    print(\"Please ensure 'datasets' library is installed (`pip install datasets`) and the specified split ('test') is correct.\")\n",
    "    print(\"Falling back to dummy OpinionQA dataset.\")\n",
    "    # Fallback to dummy data if loading fails.\n",
    "    df_opinionqa_questions = pd.DataFrame([\n",
    "        {\"id\": \"op1\", \"prompt\": \"<persona>...</persona> <question>Do you believe social media positively impacts society?</question> <choices>(A): Yes (B): No (C): Neutral</choices>\", \"answer\": \"A\", \"question_id\": \"DUMMY_Q1\"},\n",
    "        {\"id\": \"op2\", \"prompt\": \"<persona>...</persona> <question>Is it important for a leader to prioritize group harmony?</question> <choices>(A): Strongly Agree (B): Disagree</choices>\", \"answer\": \"B\", \"question_id\": \"DUMMY_Q2\"},\n",
    "    ])\n",
    "\n",
    "\n",
    "all_opinionqa_results = []\n",
    "\n",
    "# --- Helper functions for OpinionQA data extraction and categorization ---\n",
    "def extract_question_and_choices(full_prompt_string):\n",
    "    \"\"\"\n",
    "    Extracts the question text and the raw choices string from the 'prompt' column.\n",
    "    Assumes format: <persona>...</persona> <question>...</question> <choices>...</choices>\n",
    "    \"\"\"\n",
    "    question_match = re.search(r'<question>(.*?)</question>', full_prompt_string, re.DOTALL)\n",
    "    choices_match = re.search(r'<choices>(.*?)</choices>', full_prompt_string, re.DOTALL)\n",
    "\n",
    "    question_text = question_match.group(1).strip() if question_match else \"\"\n",
    "    choices_raw_str = choices_match.group(1).strip() if choices_match else \"\"\n",
    "\n",
    "    return question_text, choices_raw_str\n",
    "\n",
    "def parse_choices_string(choices_str):\n",
    "    \"\"\"\n",
    "    Parses a string like '(A): Yes (B): No' into a list of choices ['Yes', 'No'].\n",
    "    Handles choices with spaces, e.g., 'Not at all'.\n",
    "    \"\"\"\n",
    "    parsed_choices = []\n",
    "    matches = re.findall(r'\\([A-Z]\\):\\s*(.*?)(?=\\s*\\([A-Z]\\):\\s*|$)', choices_str)\n",
    "    for match in matches:\n",
    "        parsed_choices.append(match.strip())\n",
    "    \n",
    "    return parsed_choices if parsed_choices else [choices_str]\n",
    "\n",
    "def categorize_opinionqa_response(raw_response, choices_list):\n",
    "    \"\"\"\n",
    "    Attempts to categorize a raw LLM response into predefined choices from the parsed list.\n",
    "    Prioritizes matching options provided, then general sentiment.\n",
    "    \"\"\"\n",
    "    raw_response_lower = raw_response.lower()\n",
    "    \n",
    "    for choice in choices_list:\n",
    "        if re.search(r'\\b' + re.escape(choice.lower()) + r'\\b', raw_response_lower):\n",
    "            return choice\n",
    "    \n",
    "    if any(word in raw_response_lower for word in [\"yes\", \"agree\", \"positive\", \"positively\"]):\n",
    "        if \"Yes\" in choices_list: return \"Yes\"\n",
    "        if \"Agree\" in choices_list: return \"Agree\"\n",
    "        if \"Strongly Agree\" in choices_list: return \"Strongly Agree\"\n",
    "    if any(word in raw_response_lower for word in [\"no\", \"disagree\", \"negative\", \"negatively\"]):\n",
    "        if \"No\" in choices_list: return \"No\"\n",
    "        if \"Disagree\" in choices_list: return \"Disagree\"\n",
    "        if \"Strongly Disagree\" in choices_list: return \"Strongly Disagree\"\n",
    "    if any(word in raw_response_lower for word in [\"neutral\", \"balanced\", \"both\", \"neither\"]):\n",
    "        if \"Neutral\" in choices_list: return \"Neutral\"\n",
    "        if \"Seek Balance\" in choices_list: return \"Seek Balance\"\n",
    "        if \"Uncategorized\" in choices_list: return \"Uncategorized\"\n",
    "    \n",
    "    return \"Uncategorized\"\n",
    "\n",
    "\n",
    "# Main loop for OpinionQA verification\n",
    "# Include a \"neutral\" trait for comparison with personality-modified ones.\n",
    "all_opinion_qa_personalities = [\"neutral\"] + list(target_personalities)\n",
    "\n",
    "for personality_trait in all_opinion_qa_personalities:\n",
    "    print(f\"\\n--- Running OpinionQA for Personality: {personality_trait} ---\")\n",
    "    \n",
    "    current_few_shot_examples = []\n",
    "    target_persona_for_prompt_oq = \"neutral\" \n",
    "\n",
    "    if personality_trait != \"neutral\":\n",
    "        current_few_shot_examples = personality_examples[personality_trait]\n",
    "        target_persona_for_prompt_oq = personality_trait \n",
    "\n",
    "    # Limit OpinionQA questions for quick testing with dummy LLM\n",
    "    # The full test set has 295k rows. Adjust this for more robust verification.\n",
    "    opinionqa_subset_for_testing = df_opinionqa_questions.sample(min(len(df_opinionqa_questions), 5), random_state=42) # Sample 5 questions per personality\n",
    "\n",
    "    for index, item_row in opinionqa_subset_for_testing.iterrows():\n",
    "        full_prompt_from_dataset = item_row['prompt']\n",
    "        question_id = item_row['question_id']\n",
    "        human_true_answer_label = item_row['answer']\n",
    "\n",
    "        question_text, choices_raw_str = extract_question_and_choices(full_prompt_from_dataset)\n",
    "        parsed_choices = parse_choices_string(choices_raw_str)\n",
    "        \n",
    "        task_instruction_opinionqa = (\n",
    "            f\"Your answer should reflect your opinion as a {personality_trait} AI. \"\n",
    "            f\"State your chosen option's letter and its text (e.g., 'A: Yes'). \"\n",
    "            f\"The available choices are: {choices_raw_str}. Then, provide a brief explanation for your choice.\"\n",
    "        )\n",
    "\n",
    "        prompt_messages = create_dynamic_prompt(\n",
    "            target_personality=target_persona_for_prompt_oq,\n",
    "            few_shot_examples=current_few_shot_examples,\n",
    "            current_question=question_text,\n",
    "            task_instruction=task_instruction_opinionqa\n",
    "        )\n",
    "        llm_response = get_llm_response(prompt_messages, client_obj=azure_openai_client, model_deployment_name=LLM_MODEL_FOR_GENERATION, max_tokens=200)\n",
    "\n",
    "        # Classify LLM's raw response for Trait Alignment (TA)\n",
    "        predicted_personality_ta = personality_classifier(llm_response)[0]\n",
    "        predicted_trait_ta = predicted_personality_ta['label']\n",
    "        confidence_ta = predicted_personality_ta['score']\n",
    "\n",
    "        categorized_response_oq = categorize_opinionqa_response(llm_response, parsed_choices)\n",
    "\n",
    "        all_opinionqa_results.append({\n",
    "            \"intended_personality\": personality_trait,\n",
    "            \"question_id\": question_id,\n",
    "            \"full_dataset_prompt\": full_prompt_from_dataset,\n",
    "            \"extracted_question\": question_text,\n",
    "            \"choices_raw_str\": choices_raw_str,\n",
    "            \"parsed_choices\": parsed_choices,\n",
    "            \"human_true_answer_label\": human_true_answer_label,\n",
    "            \"llm_raw_response\": llm_response,\n",
    "            \"llm_categorized_response_oq\": categorized_response_oq,\n",
    "            \"llm_predicted_trait_TA\": predicted_trait_ta,\n",
    "            \"llm_predicted_trait_TA_confidence\": confidence_ta\n",
    "        })\n",
    "\n",
    "df_opinionqa_results = pd.DataFrame(all_opinionqa_results)\n",
    "print(\"\\n--- Raw OpinionQA Results Sample (First 5 rows) ---\")\n",
    "print(df_opinionqa_results.head())\n",
    "\n",
    "print(\"\\n--- Analyzing OpinionQA Trait Alignment (TA) ---\")\n",
    "if not df_opinionqa_results.empty:\n",
    "    correct_ta_predictions = df_opinionqa_results[df_opinionqa_results['intended_personality'] == df_opinionqa_results['llm_predicted_trait_TA']]\n",
    "    overall_ta_score = len(correct_ta_predictions) / len(df_opinionqa_results)\n",
    "    print(f\"Overall Trait Alignment (TA) Score: {overall_ta_score:.3f}\")\n",
    "\n",
    "    ta_per_trait = df_opinionqa_results.groupby('intended_personality').apply(\n",
    "        lambda x: (x['intended_personality'] == x['llm_predicted_trait_TA']).mean()\n",
    "    )\n",
    "    print(\"\\nTrait Alignment (TA) Score per Personality:\")\n",
    "    print(ta_per_trait)\n",
    "\n",
    "    print(\"\\n--- OpinionQA Content Analysis (Conceptual for Opinion Alignment) ---\")\n",
    "    print(\"Example: Distribution of LLM categorized responses for a question across personalities:\")\n",
    "    first_q_id = df_opinionqa_results['question_id'].iloc[0] if not df_opinionqa_results.empty else \"N/A\"\n",
    "    print(f\"LLM responses for question_id: '{first_q_id}'\")\n",
    "    print(df_opinionqa_results[df_opinionqa_results['question_id'] == first_q_id].groupby('intended_personality')['llm_categorized_response_oq'].value_counts(normalize=True))\n",
    "    \n",
    "    print(\"\\nTo compare LLM opinions with human responses, you would:\")\n",
    "    print(\"1. Map the LLM's 'llm_categorized_response_oq' to its corresponding raw label (e.g., 'Yes' to 'A').\")\n",
    "    print(\"2. Compare the distribution of LLM's mapped labels with the 'human_true_answer_label' distribution for each question.\")\n",
    "    print(\"This dataset provides individual human responses, so you'd aggregate human answers for comparison distributions.\")\n",
    "else:\n",
    "    print(\"No OpinionQA results to analyze. Please ensure the experiment runs and gathers data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. MMLU Core Function Definitions ---\n",
    "\n",
    "# Helper function to format MMLU choices for the LLM\n",
    "def format_mmlu_choices_for_llm(choices):\n",
    "    \"\"\"Formats choices for the MMLU prompt, e.g., A. choice1\\nB. choice2\"\"\"\n",
    "    return \"\\n\".join([f\"{chr(65+i)}. {choice}\" for i, choice in enumerate(choices)])\n",
    "\n",
    "# Your robust extract_letter function\n",
    "def extract_letter(text, choices):\n",
    "    \"\"\"\n",
    "    Robustly extracts the letter answer from complex outputs using a prioritized search.\n",
    "    This version is updated to handle optional periods and brackets more flexibly.\n",
    "    \"\"\"\n",
    "    # Priority 1: Check for our designed concluding phrases, now allowing for an optional period.\n",
    "    # e.g., \"... a_nswer is [A].\" or \"... a_nswer is definitely A\"\n",
    "    match = re.search(r'answer is(?: definitely| actually|)?\\s*(?:\\[)?\\s*([A-D])[\\.\\]]?', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "\n",
    "    # Priority 2: Check for the \"The answer is: A.\" format, now allowing for an optional period.\n",
    "    match = re.search(r'answer is:\\s*([A-D])\\.?', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "\n",
    "    # Priority 3: Check if the full text of exactly ONE of the choices appears in the model's output.\n",
    "    present_choices = []\n",
    "    choice_map = {\n",
    "        chr(65+i): choice_text for i, choice_text in enumerate(choices)\n",
    "    }\n",
    "    for letter, choice_text in choice_map.items():\n",
    "        if choice_text and re.search(re.escape(choice_text), text, re.IGNORECASE):\n",
    "            present_choices.append(letter)\n",
    "    \n",
    "    if len(present_choices) == 1:\n",
    "        return present_choices[0]\n",
    "\n",
    "    return \"?\"\n",
    "\n",
    "def evaluate_subject(subject, personality_examples_dict, get_llm_response_func, n_samples=20):\n",
    "    \"\"\"\n",
    "    Evaluates a single MMLU subject against the neutral baseline and all personality prompts.\n",
    "    This function uses our new few-shot prompting methodology.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Running MMLU Subject: {subject} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Load MMLU data for the subject. Using split=\"test\"\n",
    "        mmlu_data = load_dataset(\"cais/mmlu\", subject, split=\"test\", trust_remote_code=True).select(range(n_samples))\n",
    "        print(f\"Loaded {len(mmlu_data)} samples for subject '{subject}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MMLU subject '{subject}': {e}. Skipping this subject.\")\n",
    "        return pd.DataFrame() # Return empty DataFrame if loading fails\n",
    "\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Define the full set of personalities to loop through, including \"neutral\"\n",
    "    all_test_personalities = [\"neutral\"] + list(personality_examples_dict.keys())\n",
    "\n",
    "    for trait in all_test_personalities:\n",
    "        print(f\"--- Generating outputs for trait: {trait} ---\")\n",
    "        \n",
    "        current_few_shot_examples = []\n",
    "        target_persona_for_prompt = \"neutral\" \n",
    "        # Task instruction to guide LLM's response format for MMLU\n",
    "        task_instruction_for_mmlu = \"Provide the single best letter for the answer to the following question. Start your response with the letter of your choice (e.g., 'A') followed by a period and then your explanation. Do not provide any other preamble.\"\n",
    "        \n",
    "        if trait != \"neutral\":\n",
    "            current_few_shot_examples = personality_examples_dict[trait]\n",
    "            target_persona_for_prompt = trait \n",
    "\n",
    "        for ex in mmlu_data:\n",
    "            # Format the question and choices for the LLM input\n",
    "            question_mmlu = ex['question'].strip()\n",
    "            choices_mmlu_formatted = format_mmlu_choices_for_llm(ex['choices'])\n",
    "\n",
    "            combined_question_for_llm = f\"{question_mmlu}\\n{choices_mmlu_formatted}\\n\\nAnswer:\"\n",
    "\n",
    "            # Create the full prompt messages using our standardized function\n",
    "            prompt_messages = create_dynamic_prompt(\n",
    "                target_personality=target_persona_for_prompt,\n",
    "                few_shot_examples=current_few_shot_examples,\n",
    "                current_question=combined_question_for_llm,\n",
    "                task_instruction=task_instruction_for_mmlu\n",
    "            )\n",
    "            \n",
    "            # Get response from LLM\n",
    "            out = get_llm_response_func(\n",
    "                prompt_messages,\n",
    "                client_obj=azure_openai_client, # Pass client object here\n",
    "                model_deployment_name=LLM_MODEL_FOR_GENERATION,\n",
    "                max_tokens=250, # Max tokens as per your original code for generation\n",
    "                temperature=0.7, top_p=0.9 # As per your original code\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                \"subject\": subject,\n",
    "                \"trait\": trait,\n",
    "                \"model_choice\": extract_letter(out, ex['choices']),\n",
    "                \"model_raw_output\": out.strip(), # Store raw output for debugging\n",
    "                \"question\": ex['question'],\n",
    "                \"choices\": ex['choices'],\n",
    "                \"answer\": ex['answer'] # Correct answer index (0-3)\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def score_results(df):\n",
    "    \"\"\"Calculates and prints the final scores and accuracy.\"\"\"\n",
    "    def letter_to_index(letter): \n",
    "        return {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}.get(letter, -1)\n",
    "    \n",
    "    df[\"model_index\"] = df[\"model_choice\"].apply(letter_to_index)\n",
    "    df[\"correct\"] = df[\"model_index\"] == df[\"answer\"] # 'answer' column is the correct index (0-3)\n",
    "    \n",
    "    summary = df.groupby([\"subject\", \"trait\"])[\"correct\"].agg([\"count\", \"sum\", \"mean\"]).reset_index()\n",
    "    summary.columns = [\"subject\", \"trait\", \"n_total\", \"n_correct\", \"accuracy\"]\n",
    "    \n",
    "    def add_diff(group):\n",
    "        neutral_rows = group[group[\"trait\"] == \"neutral\"]\n",
    "        if not neutral_rows.empty:\n",
    "            neutral_acc = neutral_rows[\"accuracy\"].values[0]\n",
    "            group[\"accuracy_diff\"] = group[\"accuracy\"] - neutral_acc\n",
    "        else:\n",
    "            group[\"accuracy_diff\"] = 0.0 # If no neutral baseline, diff is 0\n",
    "        return group\n",
    "        \n",
    "    summary = summary.groupby(\"subject\", group_keys=False).apply(add_diff).reset_index(drop=True)\n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution Loop ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the MMLU subjects to test\n",
    "    # You can customize this list based on your needs.\n",
    "    subjects_to_test = [\n",
    "        \"high_school_psychology\",\n",
    "        \"abstract_algebra\",\n",
    "        \"college_physics\",\n",
    "        \"high_school_us_history\",\n",
    "        \"logical_fallacies\",\n",
    "        \"professional_law\",\n",
    "        \"moral_scenarios\"\n",
    "    ]\n",
    "    \n",
    "    all_results_list = []\n",
    "    \n",
    "    print(\"\\n--- Starting Final Multi-Subject MMLU Experiment ---\")\n",
    "    \n",
    "    for subject in subjects_to_test:\n",
    "        # Call evaluate_subject using our new few-shot methodology\n",
    "        # Pass personality_examples (from Step 1) and get_llm_response (from Step 3)\n",
    "        # Use n_samples=20 for faster testing; increase for more robust results.\n",
    "        df_subject_results = evaluate_subject(\n",
    "            subject, \n",
    "            personality_examples_dict=personality_examples,\n",
    "            get_llm_response_func=get_llm_response,\n",
    "            n_samples=20 # Adjust this number for more or fewer samples per subject\n",
    "        )\n",
    "        if not df_subject_results.empty:\n",
    "            all_results_list.append(df_subject_results)\n",
    "\n",
    "    # Combine all results and score them at the end\n",
    "    if all_results_list:\n",
    "        df_all_results = pd.concat(all_results_list, ignore_index=True)\n",
    "        df_detailed, df_summary = score_results(df_all_results)\n",
    "        \n",
    "        print(\"\\n\\n--- FINAL MMLU SUMMARY ACROSS ALL SUBJECTS ---\")\n",
    "        print(df_summary.to_string())\n",
    "\n",
    "        # Save results to CSV\n",
    "        output_dir = \"experiment_results\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df_detailed.to_csv(os.path.join(output_dir, \"mmlu_detailed_results.csv\"), index=False)\n",
    "        df_summary.to_csv(os.path.join(output_dir, \"mmlu_summary_results.csv\"), index=False)\n",
    "        print(f\"\\n MMLU Results saved to CSV files in '{output_dir}'.\")\n",
    "\n",
    "        # Visualize the results (requires seaborn and matplotlib)\n",
    "        try:\n",
    "            import seaborn as sns\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            g = sns.catplot(data=df_summary[df_summary[\"trait\"] != \"neutral\"],\n",
    "                            x=\"trait\", y=\"accuracy_diff\", hue=\"subject\",\n",
    "                            kind=\"bar\", height=6, aspect=2, legend_out=True)\n",
    "            g.ax.axhline(0, color=\"gray\", linestyle=\"--\")\n",
    "            g.fig.suptitle(\"Change in Accuracy vs. Neutral Baseline Across Subjects (Personality Prompting)\", y=1.03)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "            plt.show()\n",
    "        except ImportError:\n",
    "            print(\"\\nSkipping visualization: seaborn or matplotlib not installed. Run `pip install seaborn matplotlib`.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNo MMLU results generated. Please check dataset loading and evaluation_subject function.\")\n",
    "\n",
    "    print(\"\\n--- Experiment Execution Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uclenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
